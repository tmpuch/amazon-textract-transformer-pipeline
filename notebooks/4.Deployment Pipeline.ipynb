{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6df488b2-c156-431d-80d6-95409cec7b1c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Deployment Pipeline\n",
    "\n",
    "> *This notebook works well with the `Data Science 3.0 (Python 3)` kernel on SageMaker Studio*\n",
    "\n",
    "Use this notebook to process Legal Invoices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaf75a5-f133-4d85-b780-c42a256ef620",
   "metadata": {},
   "source": [
    "---\n",
    "## Environment setup \n",
    "\n",
    "### SageMaker notebook permissions\n",
    "\n",
    "▶️ In the [AWS IAM Console](https://console.aws.amazon.com/iamv2/home#/roles), check that you've attached the deployed OCR pipeline stack's **data science policy** to your SageMaker Execution Role, before continuing. You can find your deployed OCRPipeline stack in the [AWS CloudFormation Console](https://console.aws.amazon.com/cloudformation/home), and the Data Science Policy name is one of the Stack outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e799284-0075-4343-8e99-94a9cdc357a4",
   "metadata": {},
   "source": [
    "### Notebook libraries and configurations\n",
    "\n",
    "This notebook will require some additional libraries that aren't available by default in the SageMaker Studio Data Science kernel. Run the cell below to install the extra dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9cdc9fb-377f-44dc-bb8c-51401c4a07d0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: amazon-textract-response-parser in /opt/conda/lib/python3.7/site-packages (0.1.40)\n",
      "Requirement already satisfied: sagemaker-studio-image-build in /opt/conda/lib/python3.7/site-packages (0.6.0)\n",
      "Requirement already satisfied: sagemaker<3,>=2.87 in /opt/conda/lib/python3.7/site-packages (2.145.0)\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.7/site-packages (from amazon-textract-response-parser) (1.26.111)\n",
      "Requirement already satisfied: marshmallow==3.14.1 in /opt/conda/lib/python3.7/site-packages (from amazon-textract-response-parser) (3.14.1)\n",
      "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker<3,>=2.87) (4.13.0)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker<3,>=2.87) (1.0.1)\n",
      "Requirement already satisfied: attrs<23,>=20.3.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker<3,>=2.87) (22.2.0)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.7/site-packages (from sagemaker<3,>=2.87) (3.2.0)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.7/site-packages (from sagemaker<3,>=2.87) (3.2.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from sagemaker<3,>=2.87) (1.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker<3,>=2.87) (20.1)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker<3,>=2.87) (3.20.3)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.7/site-packages (from sagemaker<3,>=2.87) (0.2.0)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /opt/conda/lib/python3.7/site-packages (from sagemaker<3,>=2.87) (0.1.5)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.7/site-packages (from sagemaker<3,>=2.87) (0.7.5)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.7/site-packages (from sagemaker<3,>=2.87) (0.3.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker<3,>=2.87) (1.21.6)\n",
      "Requirement already satisfied: PyYAML==5.4.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker<3,>=2.87) (5.4.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from boto3->amazon-textract-response-parser) (0.6.0)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.111 in /opt/conda/lib/python3.7/site-packages (from boto3->amazon-textract-response-parser) (1.29.111)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3->amazon-textract-response-parser) (1.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker<3,>=2.87) (3.15.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker<3,>=2.87) (4.5.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->sagemaker<3,>=2.87) (1.14.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->sagemaker<3,>=2.87) (2.4.6)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema->sagemaker<3,>=2.87) (0.15.7)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from jsonschema->sagemaker<3,>=2.87) (59.3.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->sagemaker<3,>=2.87) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->sagemaker<3,>=2.87) (2.8.2)\n",
      "Requirement already satisfied: dill>=0.3.6 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker<3,>=2.87) (0.3.6)\n",
      "Requirement already satisfied: multiprocess>=0.70.14 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker<3,>=2.87) (0.70.14)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker<3,>=2.87) (1.7.6.6)\n",
      "Requirement already satisfied: pox>=0.3.2 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker<3,>=2.87) (0.3.2)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.7/site-packages (from schema->sagemaker<3,>=2.87) (0.6.0.post1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore<1.30.0,>=1.29.111->boto3->amazon-textract-response-parser) (1.26.15)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "--2023-05-26 22:30:04--  https://nodejs.org/dist/v16.18.0/node-v16.18.0-linux-x64.tar.xz\n",
      "Resolving nodejs.org (nodejs.org)... 104.20.23.46, 104.20.22.46, 2606:4700:10::6814:172e, ...\n",
      "Connecting to nodejs.org (nodejs.org)|104.20.23.46|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 22496468 (21M) [application/x-xz]\n",
      "Saving to: ‘STDOUT’\n",
      "\n",
      "-                   100%[===================>]  21.45M  7.35MB/s    in 2.9s    \n",
      "\n",
      "2023-05-26 22:30:07 (7.35 MB/s) - written to stdout [22496468/22496468]\n",
      "\n",
      "NodeJS v16.18.0 installed!\n"
     ]
    }
   ],
   "source": [
    "# Install Python libraries:\n",
    "!pip install amazon-textract-response-parser \\\n",
    "    sagemaker-studio-image-build \\\n",
    "    \"sagemaker>=2.87,<3\"\n",
    "\n",
    "# Install NodeJS:\n",
    "NODE_VER = \"v16.18.0\"\n",
    "NODE_DISTRO = \"linux-x64\"\n",
    "!mkdir -p /usr/local/lib/nodejs\n",
    "!wget -c https://nodejs.org/dist/{NODE_VER}/node-{NODE_VER}-{NODE_DISTRO}.tar.xz -O - | tar -xJ -C /usr/local/lib/nodejs\n",
    "NODE_BIN_DIR = f\"/usr/local/lib/nodejs/node-{NODE_VER}-{NODE_DISTRO}/bin\"\n",
    "ONPATH_BIN_DIR = \"/usr/local/bin\"\n",
    "!ln -fs {NODE_BIN_DIR}/node {ONPATH_BIN_DIR}/node && \\\n",
    "    ln -fs {NODE_BIN_DIR}/npm {ONPATH_BIN_DIR}/npm && \\\n",
    "    ln -fs {NODE_BIN_DIR}/npx {ONPATH_BIN_DIR}/npx && \\\n",
    "    echo \"NodeJS {NODE_VER} installed!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2e0741-21ec-438e-b782-e1728b301106",
   "metadata": {},
   "source": [
    "With the extra libraries installed, you're ready to load them into the kernel and initialise clients for the various AWS services we'll be calling from the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d433651b-ff28-4540-8c8d-ba355000f120",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Python Built-Ins:\n",
    "from datetime import datetime\n",
    "import json\n",
    "from logging import getLogger\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import time\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3  # AWS SDK for Python\n",
    "from IPython import display  # To display rich content in notebook\n",
    "import pandas as pd  # For tabular data analysis\n",
    "import sagemaker  # High-level SDK for SageMaker\n",
    "from tqdm.notebook import tqdm  # Progress bars\n",
    "\n",
    "# Local Dependencies:\n",
    "#import util\n",
    "\n",
    "# AWS service clients:\n",
    "s3 = boto3.resource(\"s3\")\n",
    "smclient = boto3.client(\"sagemaker\")\n",
    "ssm = boto3.client(\"ssm\")\n",
    "\n",
    "logger = getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d122ef40-e2fe-4ee9-9567-e1a252ae47a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/amazon-textract-transformer-pipeline1/notebooks\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef18fc5e-f69a-4867-88b1-1e3ca4cad549",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/root/amazon-textract-transformer-pipeline1/notebooks', '/opt/conda/lib/python37.zip', '/opt/conda/lib/python3.7', '/opt/conda/lib/python3.7/lib-dynload', '', '/opt/conda/lib/python3.7/site-packages', '/opt/conda/lib/python3.7/site-packages/IPython/extensions', '/root/.ipython', '/root/BT-Labs/LayoutLM/version2/notebooks/util']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "#sys.path.append('/root/BT-Labs/LayoutLM/version2/notebooks/util')\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06bccd6d-9e6e-48cb-8202-c49acdfb8735",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Local Dependencies:\n",
    "import util"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772da7c5-e69d-4e2c-bf81-97bd47e6dc71",
   "metadata": {},
   "source": [
    "This notebook will work with data sandboxes in Amazon S3, and connect to a deployed document processing pipeline solution. Below, we configure S3 data folders and read deployed pipeline parameter configuration from [AWS Systems Manager Parameter Store (AWS SSM)](https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cad19ece-bf6a-41e0-8941-d4ed0aa75e59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket Name: sagemaker-us-east-1-015943506230\n",
      "Bucket Prefix: DynamicTableParser/\n",
      "account_id:  015943506230\n",
      "region:  us-east-1\n",
      "\n",
      "training_job_name:  ws-xlm-cfpb-hf-2023-05-26-11-15-49-226\n"
     ]
    }
   ],
   "source": [
    "# Use our own bucket loaded with training images 2023-05-23\n",
    "#bucket_name = 'bt-digital-bt-labs-internal-200'\n",
    "#bucket_prefix = \"DynamicTableParser/4_TrainingData/\"\n",
    "\n",
    "bucket_name = sagemaker.Session().default_bucket()\n",
    "bucket_prefix = \"DynamicTableParser/\"\n",
    "account_id = sagemaker.Session().account_id()\n",
    "region = os.environ[\"AWS_REGION\"]\n",
    "\n",
    "print (\"Bucket Name: \" + bucket_name)\n",
    "print (\"Bucket Prefix: \" + bucket_prefix)\n",
    "print (\"account_id: \" , account_id)\n",
    "print (\"region: \", region)\n",
    "print()\n",
    "\n",
    "# TO DO:  2023-05-27  TMP \n",
    "# we need a way to \"remember\" or identify the model we want to use.\n",
    "# this code is OK but the lastest traning job is not always the model we want to run here\n",
    "#training_job_name = estimator.latest_training_job.describe()[\"TrainingJobName\"]\n",
    "#  it seem we need a way to track the last training job FOR THIS APPLICATION, WORKFLOW, and DECISON\n",
    "# we are working on.   We need a DB of keys that link the logical \n",
    "# APPLICATION, WORKFLOW, and DECISON to the \"approved and ready to use\" model we want to \n",
    "# use in PRODUCTION   2023-05-27  TMP \n",
    "training_job_name=\"ws-xlm-cfpb-hf-2023-05-26-11-15-49-226\"   # hardcoding the name of the training job for now\n",
    "print (\"training_job_name: \" , training_job_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d2a9655-b6a7-4a75-ab2f-99f2833ce4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working in bucket s3://sagemaker-us-east-1-015943506230/DynamicTableParser/\n",
      "\n",
      "2023-05-27 12:24:44,419 project [INFO] Working in project 'ocr-transformers-demo'\n",
      "<util.project.ProjectSession(\n",
      "  project_id=ocr-transformers-demo,\n",
      "  a2i_review_flow_arn_param=/ocr-transformers-demo/config/HumanReviewFlowArn,\n",
      "  entity_config_param=/ocr-transformers-demo/config/EntityConfiguration,\n",
      "  sagemaker_endpoint_name_param=/ocr-transformers-demo/config/SageMakerEndpointName,\n",
      "  thumbnail_endpoint_name_param=/ocr-transformers-demo/config/ThumbnailEndpointName,\n",
      "  a2i_execution_role_arn=arn:aws:iam::015943506230:role/OCRPipelineDemo-ProcessingPipelineReviewStepProces-M6UD1K31HQ0Q,\n",
      "  pipeline_input_bucket_name=ocrpipelinedemo-pipelineinputbucket350ea1ae-1ffox60d5xk2z,\n",
      "  model_callback_topic_arn=arn:aws:sns:us-east-1:015943506230:OCRPipelineDemo-ProcessingPipelineEnrichmentStepNLPEnrichmentModelSageMakerAsyncNLPEnrichmentModelE62625A1-WxGfSPiKRll3,\n",
      "  model_results_bucket=ocrpipelinedemo-processingpipelineenrichedresults-bt6ehtmayta3,\n",
      "  pipeline_sfn_arn=arn:aws:states:us-east-1:015943506230:stateMachine:ProcessingPipelinePipelineStateMachineC698BCB6-OfF7h2R7eWfO,\n",
      "  plain_textract_sfn_arn=arn:aws:states:us-east-1:015943506230:stateMachine:ProcessingPipelineOCRStepTextractStepTextractStateMachineFA5B3847-JnTeNju1dIbg,\n",
      "  preproc_image_uri=015943506230.dkr.ecr.us-east-1.amazonaws.com/sm-ocr-preprocs:pytorch-1.10-inf-cpu,\n",
      "  pipeline_reviews_bucket_name=ocrpipelinedemo-processingpipelinehumanreviewsbuc-v3nqj65r0rt9,\n",
      "  sm_image_build_role=OCRPipelineDemo-AnnotationInfraSMImageBuildRole8DB-93NPCEWTHQPR,\n",
      "  thumbnails_callback_topic_arn=arn:aws:sns:us-east-1:015943506230:OCRPipelineDemo-ProcessingPipelineThumbnailStepSageMakerAsyncThumbnailStepD663EB88-T83wFi71Nvmf\n",
      ") at 0x7f507fa86990>\n"
     ]
    }
   ],
   "source": [
    "# S3 data locations:\n",
    "# 2023-05-24\n",
    "#bucket_name = sagemaker.Session().default_bucket()\n",
    "#bucket_prefix = \"textract-transformers-wshp/\"\n",
    "\n",
    "raw_s3uri = f\"s3://{bucket_name}/{bucket_prefix}data/raw-inference\"\n",
    "imgs_s3uri = f\"s3://{bucket_name}/{bucket_prefix}data/imgs-clean\"\n",
    "textract_s3uri = f\"s3://{bucket_name}/{bucket_prefix}data/textracted\"\n",
    "thumbs_s3uri = f\"s3://{bucket_name}/{bucket_prefix}data/thumbnails\"\n",
    "annotations_base_s3uri = f\"s3://{bucket_name}/{bucket_prefix}data/annotations\"\n",
    "print(f\"Working in bucket s3://{bucket_name}/{bucket_prefix}\\n\")\n",
    "\n",
    "try:\n",
    "    config = util.project.init(\"ocr-transformers-demo\")\n",
    "    print(config)\n",
    "except Exception as e:\n",
    "    try:\n",
    "        print(f\"Your SageMaker execution role is: {sagemaker.get_execution_role()}\")\n",
    "    except Exception:\n",
    "        print(\"Couldn't look up your SageMaker execution role\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d2d509-523b-49cc-b259-38abb8c0ace1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15a12198-3140-458f-a6b9-4cda09304a12",
   "metadata": {},
   "source": [
    "## Part 1 - OCR the input PDFs\n",
    "\n",
    "Use Textracks for the OCR conversions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b692c1d3-1d86-4768-8ad0-9b22c193b5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping s3://sagemaker-us-east-1-015943506230/DynamicTableParser/data/raw-inference/\n",
      "\n",
      "Found 9 valid files for OCR\n"
     ]
    }
   ],
   "source": [
    "# build manifest.jsonl of raw input  files in the INBOX  \n",
    "\n",
    "raw_bucket_name, raw_prefix = util.s3.s3uri_to_bucket_and_key(raw_s3uri)\n",
    "\n",
    "valid_file_types = {\"jpeg\", \"jpg\", \"pdf\", \"png\", \"tif\", \"tiff\"}\n",
    "\n",
    "n_files = 0\n",
    "with open(\"data/raw-all.manifest.jsonl\", \"w\") as f:\n",
    "    # sorted() guarantees output order for reproducible sampling later:\n",
    "    for obj in sorted(\n",
    "        s3.Bucket(raw_bucket_name).objects.filter(Prefix=raw_prefix + \"/\"),\n",
    "        key=lambda obj: obj.key,\n",
    "    ):\n",
    "        # Filter out any files you know shouldn't be counted:\n",
    "        file_ext = obj.key.rpartition(\".\")[2].lower()\n",
    "        if \"/.\" in obj.key or file_ext not in valid_file_types:\n",
    "            print(f\"Skipping s3://{obj.bucket_name}/{obj.key}\")\n",
    "            continue\n",
    "\n",
    "        # Save\n",
    "        item = {\"raw-ref\": f\"s3://{obj.bucket_name}/{obj.key}\"}\n",
    "        f.write(json.dumps(item)+\"\\n\")\n",
    "        n_files += 1\n",
    "\n",
    "print(f\"\\nFound {n_files} valid files for OCR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05db57ce-6aab-4680-b85e-7d3584cc2e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df8ea974b4345d391accb533346a574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Textracting PDFs...', max=9.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56c251d51be8419abc1df8310e0e484e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Starting jobs...', max=9.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "CPU times: user 14.5 s, sys: 16.6 ms, total: 14.5 s\n",
      "Wall time: 1min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "textract_results = util.ocr.call_textract(\n",
    "    textract_sfn_arn=config.plain_textract_sfn_arn,\n",
    "    # Can instead use raw-all.manifest.jsonl to process whole dataset (see cost note above):\n",
    "    input_manifest=\"data/raw-.manifest.jsonl\",\n",
    "    manifest_raw_field=\"raw-ref\",\n",
    "    manifest_out_field=\"textract-ref\",\n",
    "    # Map subpaths of {input_base} to subpaths of {output_base}:\n",
    "    output_base_s3uri=textract_s3uri,\n",
    "    input_base_s3uri=raw_s3uri,\n",
    "    # Note that turning on additional features can have significant impact on API costs:\n",
    "    features=[\"FORMS\", \"TABLES\"],\n",
    "    skip_existing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd078636-be97-4ac6-8323-6c9ebb5276b9",
   "metadata": {},
   "source": [
    "Once the extraction is done, write (only successful items) to a manifest file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "730e7602-2a39-44be-9d6a-4b7a2a0c2689",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 of 9 docs processed successfully\n"
     ]
    }
   ],
   "source": [
    "n_success = 0\n",
    "n_fail = 0\n",
    "with open(\"data/textracted-all.manifest.jsonl\", \"w\") as fout:\n",
    "    for ix, item in enumerate(textract_results):\n",
    "        if isinstance(item[\"textract-ref\"], str):\n",
    "            fout.write(json.dumps(item) + \"\\n\")\n",
    "            n_success += 1\n",
    "        else:\n",
    "            if n_fail == 0:\n",
    "                print ()\n",
    "                print (item)\n",
    "                print ()\n",
    "                logger.error(\"First failure at index %s:\\n%s\", ix, item[\"textract-ref\"])\n",
    "                print ()\n",
    "            n_fail += 1\n",
    "\n",
    "print(f\"{n_success} of {n_success + n_fail} docs processed successfully\")\n",
    "if n_fail > 0:\n",
    "    raise ValueError(\n",
    "        \"Are you sure you want to continue? Consider re-trying to process the failed docs\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130873b8-ff91-493c-a6c6-ff2768d89786",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Part 2 - Extract clean input images (batch)\n",
    "\n",
    "To annotate our documents with SageMaker Ground Truth image task UIs, we need **individual page images**, stripped of EXIF rotation metadata (because, at the time of writing, SMGT ignores this rotation for annotation consistency) and converted to compatible formats (since some formats like TIFF are not supported by most browsers).\n",
    "\n",
    "For large corpora, this process of splitting PDFs and rotating and converting images may require significant resources - but is easy to parallelize.\n",
    "\n",
    "Therefore instead of pre-processing the raw documents here in the notebook, this is a good use case for a scalable [SageMaker Processing Job](https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.html).\n",
    "\n",
    "The job uses a **custom container image**, since the PDF reading tools we use aren't installed by default in pre-built SageMaker containers and aren't `pip install`able. However, the image has already been built and deployed to [Amazon Elastic Container Registry (ECR)](https://aws.amazon.com/ecr/) by the CDK stack (see `preproc_image` in [/pipeline/\\_\\_init\\_\\_.py](../pipeline/__init__.py)). All we need to do here is look it up from the stack parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909aa14f-d0fe-47ed-9fbf-90233023d502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de79e673-4cb9-430c-a5ea-ed964722c477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pre-built custom container image:\n",
      "015943506230.dkr.ecr.us-east-1.amazonaws.com/sm-ocr-preprocs:pytorch-1.10-inf-cpu\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import FrameworkProcessor, ProcessingInput, ProcessingOutput\n",
    "\n",
    "ecr_image_uri = config.preproc_image_uri\n",
    "print(f\"Using pre-built custom container image:\\n{ecr_image_uri}\")\n",
    "#015943506230.dkr.ecr.us-east-1.amazonaws.com/sm-ocr-preprocs:pytorch-1.10-inf-cpu\n",
    "\n",
    "# Output S3 locations:\n",
    "imgs_s3uri = f\"s3://{bucket_name}/{bucket_prefix}data/imgs-clean\"\n",
    "thumbs_s3uri = f\"s3://{bucket_name}/{bucket_prefix}data/thumbnails\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca9a449-d09d-43ef-9363-044ed4bf0997",
   "metadata": {},
   "source": [
    "> **Note:** The 'Non-augmented' manifest files used below for job data loading are still JSON-based, but a different format from the JSON-**Lines** manifests we use in most other places of this sample. You can find guidance on the [S3DataSource API doc](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_S3DataSource.html) for manifests as used here, and separate information in the [Ground Truth documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-input-data-input-manifest.html) on the \"augmented\" JSON-Lines manifests used elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79d46ffa-ae84-407b-9c33-a4124f574a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected whole corpus -- OPTION 1\n"
     ]
    }
   ],
   "source": [
    "#### OPTION 1: For processing the whole raw_s3uri prefix:\n",
    "\n",
    "preproc_inputs = [\n",
    "     ProcessingInput(\n",
    "         destination=\"/opt/ml/processing/input/raw\",  # Expected input location, per our script\n",
    "         input_name=\"raw\",\n",
    "         s3_data_distribution_type=\"ShardedByS3Key\",  # Distribute between instances, if multiple\n",
    "         source=raw_s3uri,  # S3 prefix for full raw document collection\n",
    "     ),\n",
    " ]\n",
    "print(\"Selected whole corpus -- OPTION 1\")\n",
    "#### END OPTION 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9293e16-39b8-4c70-918a-b73bbf87ad0b",
   "metadata": {},
   "source": [
    "The cell below will **run the processing job** and show logs from the job as it progresses. You can also check up on the status and history of jobs in the [Processing page of the Amazon SageMaker Console](https://console.aws.amazon.com/sagemaker/home?#/processing-jobs).\n",
    "\n",
    "> ⏰ **Note:** In our tests, it took (including job start-up overheads) about 8 minutes to process the 120-document sample with 2x `ml.c5.2xlarge` instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e5d3223-5fa9-47b4-bbe7-3594bffd108a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........................\u001b[34m2023-05-27 13:17:28,113 [preproc] INFO Parsed job args: Namespace(input='/opt/ml/processing/input/raw', n_workers=8, output='/opt/ml/processing/output/imgs-clean', thumbnails='/opt/ml/processing/output/thumbnails')\u001b[0m\n",
      "\u001b[34m2023-05-27 13:17:28,113 [preproc] INFO Additional thumbnail output is ENABLED\u001b[0m\n",
      "\u001b[34m2023-05-27 13:17:28,113 [preproc] INFO Reading raw files from /opt/ml/processing/input/raw\u001b[0m\n",
      "\u001b[34m2023-05-27 13:17:28,113 [preproc] INFO Processing 5 files across 8 processes\u001b[0m\n",
      "\u001b[35m2023-05-27 13:17:28,164 [preproc] INFO Parsed job args: Namespace(input='/opt/ml/processing/input/raw', n_workers=8, output='/opt/ml/processing/output/imgs-clean', thumbnails='/opt/ml/processing/output/thumbnails')\u001b[0m\n",
      "\u001b[35m2023-05-27 13:17:28,164 [preproc] INFO Additional thumbnail output is ENABLED\u001b[0m\n",
      "\u001b[35m2023-05-27 13:17:28,164 [preproc] INFO Reading raw files from /opt/ml/processing/input/raw\u001b[0m\n",
      "\u001b[35m2023-05-27 13:17:28,165 [preproc] INFO Processing 4 files across 8 processes\u001b[0m\n",
      "\u001b[34m2023-05-27 13:17:36,562 [preproc] INFO Processed doc 1 of 5\u001b[0m\n",
      "\u001b[35m2023-05-27 13:17:46,589 [preproc] INFO Processed doc 1 of 4\u001b[0m\n",
      "\u001b[34m2023-05-27 13:17:47,239 [preproc] INFO Processed doc 2 of 5\u001b[0m\n",
      "\u001b[35m2023-05-27 13:17:48,815 [preproc] INFO Processed doc 2 of 4\u001b[0m\n",
      "\u001b[35m2023-05-27 13:17:50,721 [preproc] INFO Processed doc 3 of 4\u001b[0m\n",
      "\u001b[34m2023-05-27 13:17:56,798 [preproc] INFO Processed doc 3 of 5\u001b[0m\n",
      "\u001b[34m2023-05-27 13:18:08,258 [preproc] INFO Processed doc 4 of 5\u001b[0m\n",
      "\u001b[34m2023-05-27 13:18:12,817 [preproc] INFO Processed doc 5 of 5\u001b[0m\n",
      "\u001b[34m2023-05-27 13:18:12,820 [preproc] INFO All done!\u001b[0m\n",
      "\u001b[35m2023-05-27 13:19:09,376 [preproc] INFO Processed doc 4 of 4\u001b[0m\n",
      "\u001b[35m2023-05-27 13:19:09,380 [preproc] INFO All done!\u001b[0m\n",
      "\n",
      "CPU times: user 1.04 s, sys: 73 ms, total: 1.11 s\n",
      "Wall time: 6min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "processor = FrameworkProcessor(\n",
    "    estimator_cls=util.preproc.DummyFramework,\n",
    "    image_uri=ecr_image_uri,\n",
    "    framework_version=\"\",  # Not needed as image URI already provided\n",
    "    base_job_name=\"ocr-img-dataclean\",\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    instance_count=2,\n",
    "    instance_type=\"ml.c5.2xlarge\",\n",
    "    volume_size_in_gb=15,\n",
    ")\n",
    "\n",
    "processor.run(\n",
    "    code=\"preproc.py\",  # PDF splitting / image conversion script\n",
    "    source_dir=\"preproc\",\n",
    "    inputs=preproc_inputs[:],  # Either whole corpus or sample, as above\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            destination=imgs_s3uri,\n",
    "            output_name=\"imgs-clean\",\n",
    "            s3_upload_mode=\"Continuous\",\n",
    "            source=\"/opt/ml/processing/output/imgs-clean\",  # Hi-res images for labelling\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            destination=thumbs_s3uri,\n",
    "            output_name=\"thumbnails\",\n",
    "            s3_upload_mode=\"Continuous\",\n",
    "            source=\"/opt/ml/processing/output/thumbnails\",  # Low-res images for model inputs\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72aae968-bb15-468c-95de-eaad1d4fe1ed",
   "metadata": {},
   "source": [
    "Once the images have been extracted, we'll also **optionally** download them locally to the notebook for use in visualizations later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f0dfede-76cf-46f0-8541-b51d5292f8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading cleaned images from s3://sagemaker-us-east-1-015943506230/DynamicTableParser/data/imgs-clean...\n",
      "Downloading thumbnail images from s3://sagemaker-us-east-1-015943506230/DynamicTableParser/data/thumbnails...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(f\"Downloading cleaned images from {imgs_s3uri}...\")\n",
    "!aws s3 sync --quiet {imgs_s3uri} data/imgs-clean\n",
    "print(f\"Downloading thumbnail images from {thumbs_s3uri}...\")\n",
    "!aws s3 sync --quiet {thumbs_s3uri} data/imgs-thumb\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ff4733-b09a-488d-9d5a-d00fb3f96f8f",
   "metadata": {},
   "source": [
    "You'll see that this job also generates uniformly resized \"thumbnail\" images per page when the second (optional) `thumbnails` output is specified. These aren't important for the human annotation process, but will be used later for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c931b1-f5f2-44c6-a30c-28d2b10426be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22c3b97c-d0f4-4195-907b-bd44b3fdb380",
   "metadata": {},
   "source": [
    "## Part 3 - Build Manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4139de03-aa9b-4591-a3df-514e10fe04d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "630453b4-4eb6-49ef-b994-6e27a79b14db",
   "metadata": {},
   "source": [
    "# enhance the textracks json that we will use for training data; we need\n",
    "# to add our Ground Truth annatations \n",
    "with open(\"data/textracted-BT-enhanced.manifest.jsonl\", \"w\") as updated_json:\n",
    "    with open(\"data/textracted-all.manifest.jsonl\", \"r\") as f:\n",
    "        for line in f:\n",
    "            texttracts_json = json.loads(line)\n",
    "            bucket, key, filename = split_s3_path( texttracts_json['raw-ref'])\n",
    "            #print (bucket)\n",
    "            #print (key)\n",
    "            #print (filename)\n",
    "\n",
    "            # get the annotation json that matches the texttrack source file name\n",
    "            annotation_json = annotation_manafest_dict[filename]\n",
    "\n",
    "            # add the attributres form the annotation json to the text track json \n",
    "            texttracts_json['source-ref'] = annotation_json['source-ref']\n",
    "            texttracts_json['label'] = annotation_json['label'] \n",
    "            #print (texttracts_json)\n",
    "            #print ()\n",
    "            \n",
    "            json.dump(texttracts_json, updated_json)\n",
    "            updated_json.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "62bb00be-d742-462d-aa4e-fe8c44a4430c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f206c85f575420999bc87929cdffd2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Building data manifest...', max=9.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-27 13:25:17,368 trp [INFO] INFO: Detected K/V where key does not have content. Excluding key from output. \n",
      "Field\n",
      "==========\n",
      "Key: \n",
      "Value: TBC_Insurance0014262 - {'BlockType': 'KEY_VALUE_SET', 'Confidence': 17.874217987060547, 'Geometry': {'BoundingBox': {'Width': 0.03270886838436127, 'Height': 0.010794537141919136, 'Left': 0.7914299964904785, 'Top': 0.9858063459396362}, 'Polygon': [{'X': 0.7914299964904785, 'Y': 0.9858063459396362}, {'X': 0.8241263628005981, 'Y': 0.9858697652816772}, {'X': 0.824138879776001, 'Y': 0.996600866317749}, {'X': 0.7914429903030396, 'Y': 0.9965370297431946}]}, 'Id': '288ba7d5-4b9d-492e-b3d0-d9bda975cdab', 'Relationships': [{'Type': 'VALUE', 'Ids': ['a5e2cf9e-f91c-4af0-977a-54ac221a5a5a']}], 'EntityTypes': ['KEY'], 'Page': 1}\n",
      "2023-05-27 13:25:17,717 trp [INFO] INFO: Detected K/V where key does not have content. Excluding key from output. \n",
      "Field\n",
      "==========\n",
      "Key: \n",
      "Value: TBC_Insurance0014278 - {'BlockType': 'KEY_VALUE_SET', 'Confidence': 16.114154815673828, 'Geometry': {'BoundingBox': {'Width': 0.03545636311173439, 'Height': 0.012167410925030708, 'Left': 0.7911565899848938, 'Top': 0.9856572151184082}, 'Polygon': [{'X': 0.7911622524261475, 'Y': 0.9856572151184082}, {'X': 0.8266129493713379, 'Y': 0.9856683611869812}, {'X': 0.8266074061393738, 'Y': 0.9978246092796326}, {'X': 0.7911565899848938, 'Y': 0.99781334400177}]}, 'Id': '97f032be-e86f-49b8-ac9f-8750315a792c', 'Relationships': [{'Type': 'VALUE', 'Ids': ['4ce83178-d277-4d04-89af-fbf511cf2106']}], 'EntityTypes': ['KEY'], 'Page': 17}\n",
      "2023-05-27 13:25:17,987 trp [INFO] INFO: Detected K/V where key does not have content. Excluding key from output. \n",
      "Field\n",
      "==========\n",
      "Key: \n",
      "Value: TBC_Insurance0014285 - {'BlockType': 'KEY_VALUE_SET', 'Confidence': 27.603351593017578, 'Geometry': {'BoundingBox': {'Width': 0.031984053552150726, 'Height': 0.011204457841813564, 'Left': 0.7920429706573486, 'Top': 0.9857417345046997}, 'Polygon': [{'X': 0.792046070098877, 'Y': 0.9857639670372009}, {'X': 0.8240270614624023, 'Y': 0.9857417345046997}, {'X': 0.8240230679512024, 'Y': 0.996924638748169}, {'X': 0.7920429706573486, 'Y': 0.9969462156295776}]}, 'Id': '5978417a-37ae-4ab9-9e93-ba57b3c4b2fc', 'Relationships': [{'Type': 'VALUE', 'Ids': ['6872da5b-0233-47a5-b9cf-5ddea64fbcf1']}], 'EntityTypes': ['KEY'], 'Page': 24}\n",
      "2023-05-27 13:25:20,757 trp [INFO] INFO: Detected K/V where key does not have content. Excluding key from output. \n",
      "Field\n",
      "==========\n",
      "Key: \n",
      "Value: TBC_Insurance0006995 - {'BlockType': 'KEY_VALUE_SET', 'Confidence': 18.522563934326172, 'Geometry': {'BoundingBox': {'Width': 0.03557416796684265, 'Height': 0.01193789578974247, 'Left': 0.7914878726005554, 'Top': 0.9858344197273254}, 'Polygon': [{'X': 0.7914878726005554, 'Y': 0.9858344197273254}, {'X': 0.8270540237426758, 'Y': 0.9858777523040771}, {'X': 0.8270620703697205, 'Y': 0.9977723360061646}, {'X': 0.791495680809021, 'Y': 0.9977288842201233}]}, 'Id': 'b2967012-a703-4084-9099-cb595c9ce767', 'Relationships': [{'Type': 'VALUE', 'Ids': ['d6741594-e356-438b-9ff3-6538f99aa1a1']}], 'EntityTypes': ['KEY'], 'Page': 3}\n",
      "2023-05-27 13:25:20,863 trp [INFO] INFO: Detected K/V where key does not have content. Excluding key from output. \n",
      "Field\n",
      "==========\n",
      "Key: \n",
      "Value: TBC_Insurance0007018 - {'BlockType': 'KEY_VALUE_SET', 'Confidence': 20.592897415161133, 'Geometry': {'BoundingBox': {'Width': 0.03223476558923721, 'Height': 0.011470894329249859, 'Left': 0.7921846508979797, 'Top': 0.9857816100120544}, 'Polygon': [{'X': 0.7921846508979797, 'Y': 0.9857816100120544}, {'X': 0.8244092464447021, 'Y': 0.985893964767456}, {'X': 0.824419379234314, 'Y': 0.9972525238990784}, {'X': 0.7921950221061707, 'Y': 0.9971398115158081}]}, 'Id': '81546898-e893-451b-a368-536e91f0b251', 'Relationships': [{'Type': 'VALUE', 'Ids': ['1afbbec6-5608-4069-a403-7eda902824bc']}], 'EntityTypes': ['KEY'], 'Page': 26}\n",
      "2023-05-27 13:25:21,429 trp [INFO] INFO: Detected K/V where key does not have content. Excluding key from output. \n",
      "Field\n",
      "==========\n",
      "Key: \n",
      "Value: TBC_Insurance0008030 - {'BlockType': 'KEY_VALUE_SET', 'Confidence': 23.84349822998047, 'Geometry': {'BoundingBox': {'Width': 0.034870393574237823, 'Height': 0.012060705572366714, 'Left': 0.7914726734161377, 'Top': 0.9857383966445923}, 'Polygon': [{'X': 0.7914726734161377, 'Y': 0.9857383966445923}, {'X': 0.8263395428657532, 'Y': 0.9857434034347534}, {'X': 0.8263430595397949, 'Y': 0.9977990984916687}, {'X': 0.7914761304855347, 'Y': 0.997793972492218}]}, 'Id': '92fdaa1f-0900-4cf7-86e9-8df614722d99', 'Relationships': [{'Type': 'VALUE', 'Ids': ['f0a78506-2d1c-4bed-a0cf-e82a6ae889ce']}], 'EntityTypes': ['KEY'], 'Page': 13}\n",
      "2023-05-27 13:25:21,740 trp [INFO] INFO: Detected K/V where key does not have content. Excluding key from output. \n",
      "Field\n",
      "==========\n",
      "Key: \n",
      "Value: TBC_Insurance0008509 - {'BlockType': 'KEY_VALUE_SET', 'Confidence': 18.622737884521484, 'Geometry': {'BoundingBox': {'Width': 0.032592158764600754, 'Height': 0.011013458482921124, 'Left': 0.7916899919509888, 'Top': 0.9857010245323181}, 'Polygon': [{'X': 0.7916899919509888, 'Y': 0.9857010245323181}, {'X': 0.8242701292037964, 'Y': 0.9857907891273499}, {'X': 0.824282169342041, 'Y': 0.9967144727706909}, {'X': 0.7917022705078125, 'Y': 0.9966243505477905}]}, 'Id': 'a697dbe0-cecd-451f-803d-d2e5119f80ad', 'Relationships': [{'Type': 'VALUE', 'Ids': ['61ebf42c-9ddc-4c96-b984-e7183b9c5b42']}], 'EntityTypes': ['KEY'], 'Page': 1}\n",
      "2023-05-27 13:25:21,979 trp [INFO] INFO: Detected K/V where key does not have content. Excluding key from output. \n",
      "Field\n",
      "==========\n",
      "Key: \n",
      "Value: TBC_Insurance0008526 - {'BlockType': 'KEY_VALUE_SET', 'Confidence': 21.815946578979492, 'Geometry': {'BoundingBox': {'Width': 0.03366415575146675, 'Height': 0.011586620472371578, 'Left': 0.7917801141738892, 'Top': 0.9858340620994568}, 'Polygon': [{'X': 0.7917801141738892, 'Y': 0.9858340620994568}, {'X': 0.8254309892654419, 'Y': 0.985877513885498}, {'X': 0.8254442811012268, 'Y': 0.9974206686019897}, {'X': 0.791793167591095, 'Y': 0.9973770976066589}]}, 'Id': '78f221bf-306c-489b-850c-692e49849130', 'Relationships': [{'Type': 'VALUE', 'Ids': ['8822cf3d-c2c1-42b5-96b8-da43c0a1281b']}], 'EntityTypes': ['KEY'], 'Page': 18}\n",
      "2023-05-27 13:25:22,591 trp [INFO] INFO: Detected K/V where key does not have content. Excluding key from output. \n",
      "Field\n",
      "==========\n",
      "Key: \n",
      "Value: TBC_Insurance0042999 - {'BlockType': 'KEY_VALUE_SET', 'Confidence': 24.52048110961914, 'Geometry': {'BoundingBox': {'Width': 0.033517975360155106, 'Height': 0.011332298628985882, 'Left': 0.7915172576904297, 'Top': 0.9858618378639221}, 'Polygon': [{'X': 0.7915328741073608, 'Y': 0.9858618378639221}, {'X': 0.8250352740287781, 'Y': 0.9858854413032532}, {'X': 0.8250197768211365, 'Y': 0.9971941709518433}, {'X': 0.7915172576904297, 'Y': 0.9971705079078674}]}, 'Id': '5ae19dca-ed88-45a5-8fdf-43fc5d76c447', 'Relationships': [{'Type': 'VALUE', 'Ids': ['ac664d34-38fa-48a8-aaad-2fc48396483a']}], 'EntityTypes': ['KEY'], 'Page': 2}\n",
      "2023-05-27 13:25:22,615 trp [INFO] INFO: Detected K/V where key does not have content. Excluding key from output. \n",
      "Field\n",
      "==========\n",
      "Key: \n",
      "Value: TBCInsurance0043005 - {'BlockType': 'KEY_VALUE_SET', 'Confidence': 24.388561248779297, 'Geometry': {'BoundingBox': {'Width': 0.033245764672756195, 'Height': 0.011788430623710155, 'Left': 0.7924989461898804, 'Top': 0.9856023192405701}, 'Polygon': [{'X': 0.7924989461898804, 'Y': 0.9856323003768921}, {'X': 0.8257333636283875, 'Y': 0.9856023192405701}, {'X': 0.8257446885108948, 'Y': 0.9973609447479248}, {'X': 0.7925100922584534, 'Y': 0.9973907470703125}]}, 'Id': '4fa831a5-4499-4a06-bc64-93d5ace6e2d7', 'Relationships': [{'Type': 'VALUE', 'Ids': ['43351c40-aa38-42c3-b86b-e88f819497ac']}], 'EntityTypes': ['KEY'], 'Page': 8}\n",
      "2023-05-27 13:25:22,622 trp [INFO] INFO: Detected K/V where key does not have content. Excluding key from output. \n",
      "Field\n",
      "==========\n",
      "Key: \n",
      "Value: TBC Insurance0043006 - {'BlockType': 'KEY_VALUE_SET', 'Confidence': 17.472732543945312, 'Geometry': {'BoundingBox': {'Width': 0.03549792245030403, 'Height': 0.012128147296607494, 'Left': 0.7910500168800354, 'Top': 0.9856321215629578}, 'Polygon': [{'X': 0.7910500168800354, 'Y': 0.9856321215629578}, {'X': 0.8265472054481506, 'Y': 0.9856519103050232}, {'X': 0.8265479803085327, 'Y': 0.9977602958679199}, {'X': 0.7910506725311279, 'Y': 0.9977405667304993}]}, 'Id': '26733f6c-b9b4-40e0-b01e-ea9d6d8e2636', 'Relationships': [{'Type': 'VALUE', 'Ids': ['beb93330-7cb2-451c-9dc2-b91d0787432c']}], 'EntityTypes': ['KEY'], 'Page': 9}\n",
      "2023-05-27 13:25:22,624 trp [INFO] INFO: Detected K/V where key does not have content. Excluding key from output. \n",
      "Field\n",
      "==========\n",
      "Key: \n",
      "Value: TBC_Insurance0043007 - {'BlockType': 'KEY_VALUE_SET', 'Confidence': 19.64068603515625, 'Geometry': {'BoundingBox': {'Width': 0.032617196440696716, 'Height': 0.011317089200019836, 'Left': 0.7918883562088013, 'Top': 0.9857679009437561}, 'Polygon': [{'X': 0.7918883562088013, 'Y': 0.9857679009437561}, {'X': 0.8244918584823608, 'Y': 0.9858375191688538}, {'X': 0.8245055675506592, 'Y': 0.9970849752426147}, {'X': 0.7919019460678101, 'Y': 0.9970149993896484}]}, 'Id': '91eff347-5df8-4e0c-9041-d40c0a9d0c35', 'Relationships': [{'Type': 'VALUE', 'Ids': ['b5240a18-1373-4fea-b45b-53b4e1973434']}], 'EntityTypes': ['KEY'], 'Page': 10}\n",
      "2023-05-27 13:25:22,628 trp [INFO] INFO: Detected K/V where key does not have content. Excluding key from output. \n",
      "Field\n",
      "==========\n",
      "Key: \n",
      "Value: TBC Insurance0043008 - {'BlockType': 'KEY_VALUE_SET', 'Confidence': 16.02690887451172, 'Geometry': {'BoundingBox': {'Width': 0.033507414162158966, 'Height': 0.011600017547607422, 'Left': 0.7920908331871033, 'Top': 0.9856042861938477}, 'Polygon': [{'X': 0.7920976281166077, 'Y': 0.9856129288673401}, {'X': 0.8255982398986816, 'Y': 0.9856042861938477}, {'X': 0.8255914449691772, 'Y': 0.9971956014633179}, {'X': 0.7920908331871033, 'Y': 0.9972043037414551}]}, 'Id': '6ec77f70-3695-4715-80ec-174b1000b358', 'Relationships': [{'Type': 'VALUE', 'Ids': ['a4170c53-3bc0-4772-9e95-e560482ac870']}], 'EntityTypes': ['KEY'], 'Page': 11}\n",
      "2023-05-27 13:25:22,994 trp [INFO] INFO: Detected K/V where key does not have content. Excluding key from output. \n",
      "Field\n",
      "==========\n",
      "Key: \n",
      "Value: TBC_Insurance0043093 - {'BlockType': 'KEY_VALUE_SET', 'Confidence': 17.52212142944336, 'Geometry': {'BoundingBox': {'Width': 0.03333907201886177, 'Height': 0.011124618351459503, 'Left': 0.7909933924674988, 'Top': 0.9857488870620728}, 'Polygon': [{'X': 0.7909933924674988, 'Y': 0.9857607483863831}, {'X': 0.8243012428283691, 'Y': 0.9857488870620728}, {'X': 0.8243324160575867, 'Y': 0.9968616366386414}, {'X': 0.7910247445106506, 'Y': 0.9968734979629517}]}, 'Id': 'e5460dc5-45d8-4836-86ae-bdffcb9235e4', 'Relationships': [{'Type': 'VALUE', 'Ids': ['acd30c42-f1d4-4e25-bb89-674ad9c7e753']}], 'EntityTypes': ['KEY'], 'Page': 1}\n",
      "2023-05-27 13:25:22,999 trp [INFO] INFO: Detected K/V where key does not have content. Excluding key from output. \n",
      "Field\n",
      "==========\n",
      "Key: \n",
      "Value: TBC_Insurance0043094 - {'BlockType': 'KEY_VALUE_SET', 'Confidence': 21.902664184570312, 'Geometry': {'BoundingBox': {'Width': 0.03394513204693794, 'Height': 0.011659679003059864, 'Left': 0.7917917966842651, 'Top': 0.985664427280426}, 'Polygon': [{'X': 0.7917917966842651, 'Y': 0.985664427280426}, {'X': 0.8257354497909546, 'Y': 0.9856882095336914}, {'X': 0.825736939907074, 'Y': 0.9973241090774536}, {'X': 0.7917932868003845, 'Y': 0.9973002672195435}]}, 'Id': '380519c9-b81b-42ad-8ee6-92d06c9a28c1', 'Relationships': [{'Type': 'VALUE', 'Ids': ['5229934f-5f8d-4b83-8718-3e332bac7513']}], 'EntityTypes': ['KEY'], 'Page': 2}\n",
      "2023-05-27 13:25:23,185 trp [INFO] INFO: Detected K/V where key does not have content. Excluding key from output. \n",
      "Field\n",
      "==========\n",
      "Key: \n",
      "Value: TBC_Insurance0043105 - {'BlockType': 'KEY_VALUE_SET', 'Confidence': 19.57543182373047, 'Geometry': {'BoundingBox': {'Width': 0.034173522144556046, 'Height': 0.011721649207174778, 'Left': 0.7926260232925415, 'Top': 0.9857170581817627}, 'Polygon': [{'X': 0.7926260232925415, 'Y': 0.9857170581817627}, {'X': 0.8267934918403625, 'Y': 0.9857196807861328}, {'X': 0.8267995119094849, 'Y': 0.9974387288093567}, {'X': 0.7926316857337952, 'Y': 0.9974361062049866}]}, 'Id': '39651346-66c9-423a-9146-40162c77329b', 'Relationships': [{'Type': 'VALUE', 'Ids': ['1b1eeb8e-fe09-451b-8379-8a8af2d82d38']}], 'EntityTypes': ['KEY'], 'Page': 13}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "warnings = util.preproc.collate_data_manifest(\n",
    "    # Output file:\n",
    "    \"data/pages-all-inference.manifest.jsonl\",\n",
    "    # Input manifest:\n",
    "    #input_manifest=\"data/textracted-all.manifest2.jsonl\",\n",
    "    input_manifest=\"data/textracted-all.manifest.jsonl\",\n",
    "    # s3://... base URI used to try and map 'textract-ref's to cleaned images:\n",
    "    textract_s3_prefix=textract_s3uri,\n",
    "    # The s3://... base URI under which page images are stored:\n",
    "    imgs_s3_prefix=imgs_s3uri,\n",
    "    # Optional s3://... base URI also used to try and map 'raw-ref's to images if present:\n",
    "    raw_s3_prefix=raw_s3uri,\n",
    "    # Other output manifest settings:\n",
    "    by=\"page\",\n",
    "    no_content=\"omit\",\n",
    ")\n",
    "\n",
    "if len(warnings):\n",
    "    raise ValueError(\n",
    "        \"Manifest usable but incomplete - %s docs failed. Please see `warnings` for details\"\n",
    "        % len(warnings)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cee97d-6ad1-4b11-b34a-ddbb33128f25",
   "metadata": {},
   "source": [
    "## Part 4 - Easy \"one-click\" deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2a9bfc70-d87e-4239-aba6-12820251a2dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure async endpoint settings for the pipeline stack:\n",
    "async_inference_config = sagemaker.async_inference.AsyncInferenceConfig(\n",
    "    output_path=f\"s3://{config.model_results_bucket}\",\n",
    "    max_concurrent_invocations_per_instance=2,  # (Can tune this for performance)\n",
    "    notification_config={\n",
    "        \"SuccessTopic\": config.model_callback_topic_arn,\n",
    "        \"ErrorTopic\": config.model_callback_topic_arn,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Extra environment variables to enable large payloads in async\n",
    "async_extra_env_vars = {\n",
    "    \"MMS_DEFAULT_RESPONSE_TIMEOUT\": str(60*3),  # 3min instead of default (maybe 60sec?)\n",
    "    \"MMS_MAX_REQUEST_SIZE\": str(100*1024*1024),  # 100MiB instead of default ~6.2MiB\n",
    "    \"MMS_MAX_RESPONSE_SIZE\": str(100*1024*1024),  # 100MiB instead of default ~6.2MiB\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "11fded6a-f33f-45b2-9802-41c1521fd68d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ws-xlm-cfpb-hf-2023-05-26-11-15-49-226\n",
      "\n",
      "2023-05-26 11:41:47 Starting - Preparing the instances for training\n",
      "2023-05-26 11:41:47 Downloading - Downloading input data\n",
      "2023-05-26 11:41:47 Training - Training image download completed. Training in progress.\n",
      "2023-05-26 11:41:47 Uploading - Uploading generated training model\n",
      "2023-05-26 11:41:47 Completed - Training job completed\n"
     ]
    }
   ],
   "source": [
    "# If needed, you can attach to a previous training job by name like this:\n",
    "print (training_job_name)\n",
    "estimator = HuggingFaceEstimator.attach(training_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4e71150d-465b-4100-b67e-426895db9a8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target inference image: 015943506230.dkr.ecr.us-east-1.amazonaws.com/sm-ocr-inference:hf-4.26-pt-gpu\n"
     ]
    }
   ],
   "source": [
    "# Configurations:\n",
    "hf_version = \"4.17\"\n",
    "py_version = \"py38\"\n",
    "pt_version = \"1.10\"\n",
    "train_repo_name = \"sm-ocr-training\"\n",
    "train_repo_tag = \"hf-4.26-pt-gpu\"  # (Base HF version is overridden in Dockerfile)\n",
    "inf_repo_name = \"sm-ocr-inference\"\n",
    "inf_repo_tag = train_repo_tag\n",
    "\n",
    "# Combine together into the final URIs:\n",
    "inf_image_uri = f\"{account_id}.dkr.ecr.{region}.amazonaws.com/{inf_repo_name}:{inf_repo_tag}\"\n",
    "print(f\"Target inference image: {inf_image_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "815d4375-a839-4ca8-830c-b2b7e0bf02cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(\n",
    "    # Avoid us accidentally deploying the same model twice by setting name per training job:\n",
    "    endpoint_name=training_job_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g4dn.xlarge\",  # Or try ml.m5.2xlarge\n",
    "    image_uri=inf_image_uri,\n",
    "\n",
    "    serializer=sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer=sagemaker.deserializers.JSONDeserializer(),\n",
    "\n",
    "    env={\n",
    "        \"PYTHONUNBUFFERED\": \"1\",  # TODO: Disable once debugging is done\n",
    "        \"MMS_MAX_REQUEST_SIZE\": str(100*1024*1024),  # Accept large payloads (docs)\n",
    "        \"MMS_MAX_RESPONSE_SIZE\": str(100*1024*1024),  # Allow large responses\n",
    "    },\n",
    "\n",
    "    # Deploy in Asynchronous mode, to support large req/res payloads:\n",
    "    async_inference_config=sagemaker.async_inference.AsyncInferenceConfig(\n",
    "        output_path=f\"s3://{config.model_results_bucket}\",\n",
    "        max_concurrent_invocations_per_instance=2,\n",
    "        notification_config={\n",
    "            \"SuccessTopic\": config.model_callback_topic_arn,\n",
    "            \"ErrorTopic\": config.model_callback_topic_arn,\n",
    "        },\n",
    "    ),\n",
    ")\n",
    "print (\"Async Endpoint \", training_job_name , \" is InService.\")\n",
    "# TO DO: it would be cool to actually get the status of the endpoint from AWS and also display the creation date\n",
    "# ARN and last updated date "
   ]
  },
  {
   "cell_type": "raw",
   "id": "125e6089-3a48-427c-8535-b2d7338002fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    " predictor = sagemaker.predictor_async.AsyncPredictor(\n",
    "     sagemaker.Predictor(\n",
    "         endpoint_name,\n",
    "         serializer=sagemaker.serializers.JSONSerializer(),\n",
    "         deserializer=sagemaker.deserializers.JSONDeserializer(),\n",
    "     ),\n",
    "    name=endpoint_name,\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "924daffe-3b89-4a76-9b31-e06a15151ad6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date\n",
      "Person\n",
      "Description\n",
      "Hours\n",
      "Rate\n",
      "Total\n"
     ]
    }
   ],
   "source": [
    "# Read the field lables we setup in the Ground Truth Job from a configuration file\n",
    "# that was created previously. \n",
    "with open(\"data/field-config.json\", \"r\") as f:\n",
    "     data = json.load(f)\n",
    "        \n",
    "entity_classes = [f['Name'] for f in data]\n",
    "\n",
    "# And print out a simple list:\n",
    "print(\"\\n\".join(entity_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4b3719-5f10-4f06-a9d8-eea383183394",
   "metadata": {},
   "source": [
    "## Part 5 - Inference \n",
    "For each page in our manifest we will send the Textracks .json output and thumbnail image to the model.  The model will then tag each WORD in the .json response with any identifed classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f99df9ea-0a5c-4057-ba97-6ab1881f13ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"data/pages-all-inference.manifest.jsonl\", \"r\") as fman:\n",
    "    inference_examples = [json.loads(line) for line in filter(lambda l: l, fman)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bc349cbe-dbc8-480e-8620-4aea0c45913e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5a27afd78f2496ab2c35c9d31e5ebca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Example:', max=181), Output()), _dom_classes=('widget-in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(ix)>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this cell is only for testing.  If can be set to a RAW (not executable cell once you have \n",
    "# confidence the model is working as intented.\n",
    "\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import trp\n",
    "\n",
    "# Enabling thumbnails can significantly increase inference time here, but can improve results for\n",
    "# models that consume image features (like LayoutLMv2, XLM):\n",
    "include_thumbnails = False\n",
    "\n",
    "def predict_from_manifest_item(\n",
    "    item,\n",
    "    predictor,\n",
    "    imgs_s3key_prefix=imgs_s3uri[len(\"s3://\"):].partition(\"/\")[2],\n",
    "    raw_s3uri_prefix=raw_s3uri,\n",
    "    textract_s3key_prefix=textract_s3uri[len(\"s3://\"):].partition(\"/\")[2],\n",
    "    imgs_local_prefix=\"data/imgs-clean\",\n",
    "    textract_local_prefix=\"data/textracted\",\n",
    "    draw=True,\n",
    "):\n",
    "    paths = util.viz.local_paths_from_manifest_item(\n",
    "        item,\n",
    "        imgs_s3key_prefix,\n",
    "        textract_s3key_prefix=textract_s3key_prefix,\n",
    "        imgs_local_prefix=imgs_local_prefix,\n",
    "        textract_local_prefix=textract_local_prefix,\n",
    "    )\n",
    "\n",
    "    if include_thumbnails:\n",
    "        doc_textract_s3key = item[\"textract-ref\"][len(\"s3://\"):].partition(\"/\")[2]\n",
    "        doc_raw_s3uri = raw_s3uri_prefix + doc_textract_s3key[len(textract_s3key_prefix):].rpartition(\"/\")[0]\n",
    "        print(f\"Fetching thumbnails for {doc_raw_s3uri}\")\n",
    "        thumbs_async = preproc_predictor.predict_async(input_path=doc_raw_s3uri)\n",
    "        thumbs_bucket, _, thumbs_key = thumbs_async.output_path[len(\"s3://\"):].partition(\"/\")\n",
    "        # Wait for the request to complete:\n",
    "        thumbs_async.get_result(sagemaker.async_inference.WaiterConfig())\n",
    "        req_extras = {\"S3Thumbnails\": {\"Bucket\": thumbs_bucket, \"Key\": thumbs_key}}\n",
    "        print(\"Got thumbnails result\")\n",
    "    else:\n",
    "        req_extras = {}\n",
    "\n",
    "    result_json = predictor.predict({\n",
    "        \"S3Input\": {\"S3Uri\": item[\"textract-ref\"]},\n",
    "        \"TargetPageNum\": item[\"page-num\"],\n",
    "        \"TargetPageOnly\": True,\n",
    "        **req_extras,\n",
    "    })\n",
    "\n",
    "    if \"Warnings\" in result_json:\n",
    "        for warning in result_json[\"Warnings\"]:\n",
    "            logger.warning(warning)\n",
    "    result_trp = trp.Document(result_json)\n",
    "\n",
    "    if draw:\n",
    "        util.viz.draw_smgt_annotated_page(\n",
    "            paths[\"image\"],\n",
    "            entity_classes,\n",
    "            annotations=[],\n",
    "            textract_result=result_trp,\n",
    "            # Note that page_num should be item[\"page-num\"] if we requested the full set of pages\n",
    "            # from the model above:\n",
    "            page_num=1,\n",
    "        )\n",
    "    return result_trp\n",
    "\n",
    "\n",
    "widgets.interact(\n",
    "    lambda ix: predict_from_manifest_item(inference_examples[ix], predictor),\n",
    "    ix=widgets.IntSlider(\n",
    "        min=0,\n",
    "        max=len(inference_examples) - 1,\n",
    "        step=1,\n",
    "        value=0,\n",
    "        description=\"Example:\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "f8f366f1-f790-45c1-8370-5efb64443a06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-27 23:41:48,465 root [WARNING] SageMaker model's preprocessor (<class 'transformers.models.layoutxlm.processing_layoutxlm.LayoutXLMProcessor'>) expects page images (as .S3Thumbnails.{Bucket, Key} numpy array pointer in the request) but none were given. Generating default blank images - accuracy may be degraded.\n"
     ]
    }
   ],
   "source": [
    "# run a prediction on page a a time\n",
    "\n",
    "i = 1  # hardcoding to 1 for testing; need to read and prcess eveyr row in inference_examples\n",
    "\n",
    "res = predict_from_manifest_item(\n",
    "    inference_examples[i],\n",
    "    predictor,\n",
    "    draw=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "528257c5-87f1-4ec3-b1a2-c1651b411d84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_s3_path(s3_path):\n",
    "    path_parts=s3_path.replace(\"s3://\",\"\").split(\"/\")\n",
    "    bucket=path_parts.pop(0)\n",
    "    key=\"/\".join(path_parts [:-1])\n",
    "    filename = path_parts[-1]\n",
    "    return bucket, key, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "8d85236a-db9f-439c-8bd5-085358ab45a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " def build_json_for_dataframe(entity_classes, text, confidence, label, label_confidence , top, left, textType,\n",
    "                             filename, pagenumber,df):\n",
    "  \"\"\"\n",
    "  Builds a JSON document from a DataFrame. The JSON document is then passed to Pandas to build a new DataFrame with the appended rows.\n",
    "\n",
    "  \"\"\"\n",
    "  bucket, key, filename = split_s3_path(filename)\n",
    "\n",
    "  # Create a new row with the `person`, `date`, and `rate` values.\n",
    "  new_row = {\"text\": text, \n",
    "             \"text_confidence\": confidence, \n",
    "             \"label\": label, \n",
    "             \"label_confidence\": label_confidence, \n",
    "             \"top\": top,\n",
    "             \"left\": left,          \n",
    "             \"top\": top,     \n",
    "             \"textType\" : textType,\n",
    "             \"file_name\" : filename,\n",
    "             \"page_number\" :  str(int(pagenumber)),\n",
    "             \"bucket\" : bucket,\n",
    "             \"key\" : key\n",
    "            }\n",
    "\n",
    "  # Append the new row to the DataFrame.\n",
    "  new_df = df.append(new_row, ignore_index=True)\n",
    "\n",
    "  # Return the new DataFrame.\n",
    "  return new_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "24adafbe-9354-4031-bf8c-fbb5ee811abb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Date', 'Person', 'Description', 'Hours', 'Rate', 'Total']\n"
     ]
    }
   ],
   "source": [
    "print (entity_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "67b17ec7-930a-44fe-9e0a-ec2b0ca50b8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "for page in res.pages:\n",
    "    for line in page.lines:\n",
    "        for word in line.words:\n",
    "            label = \"\"\n",
    "            label_confidence = \"0.00\"\n",
    "            \n",
    "            try:\n",
    "                if ( int (word._block[\"PredictedClass\"]) < int(len(entity_classes)) ):\n",
    "                    label = (entity_classes[word._block[\"PredictedClass\"]])\n",
    "                else:\n",
    "                    label =  \"None\"\n",
    "                    label_confidence = \"0.00\"\n",
    "            except: \n",
    "                label =  \"None\"\n",
    "                label_confidence = \"0.00\"       \n",
    "            \n",
    "            new_df = build_json_for_dataframe(entity_classes,\n",
    "                                              word.text, \n",
    "                                              word.confidence,\n",
    "                                              label, \n",
    "                                              label_confidence,\n",
    "                                              word.geometry.boundingBox.top,\n",
    "                                              word.geometry.boundingBox.left,\n",
    "                                              word.textType,\n",
    "                                              inference_examples[i]['raw-ref'],\n",
    "                                              inference_examples[i]['page-num'],\n",
    "                                              df )\n",
    "            df = new_df\n",
    "\n",
    "print (len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "37348eb3-169e-4bcd-8a34-8af29ce25cea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_confidence</th>\n",
       "      <th>label</th>\n",
       "      <th>label_confidence</th>\n",
       "      <th>top</th>\n",
       "      <th>left</th>\n",
       "      <th>textType</th>\n",
       "      <th>file_name</th>\n",
       "      <th>page_number</th>\n",
       "      <th>bucket</th>\n",
       "      <th>key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sher</td>\n",
       "      <td>99.961830</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.081093</td>\n",
       "      <td>0.322362</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Garner</td>\n",
       "      <td>99.975700</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.081046</td>\n",
       "      <td>0.395771</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cahill</td>\n",
       "      <td>99.927826</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.080641</td>\n",
       "      <td>0.498814</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Richter</td>\n",
       "      <td>99.849930</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.080691</td>\n",
       "      <td>0.588421</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Klein</td>\n",
       "      <td>99.951691</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.105747</td>\n",
       "      <td>0.349694</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&amp;</td>\n",
       "      <td>99.851669</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.105804</td>\n",
       "      <td>0.431569</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Hilbert,</td>\n",
       "      <td>99.933090</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.105484</td>\n",
       "      <td>0.462552</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>L.L.C.</td>\n",
       "      <td>95.647316</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.105493</td>\n",
       "      <td>0.576021</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Twenty-Eighth</td>\n",
       "      <td>99.147263</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.127919</td>\n",
       "      <td>0.406729</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Floor</td>\n",
       "      <td>99.935326</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.128136</td>\n",
       "      <td>0.551785</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>909</td>\n",
       "      <td>99.907326</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.144890</td>\n",
       "      <td>0.417617</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Poydras</td>\n",
       "      <td>99.796577</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.144887</td>\n",
       "      <td>0.456229</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Street</td>\n",
       "      <td>99.922928</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.144844</td>\n",
       "      <td>0.537171</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>New</td>\n",
       "      <td>99.959145</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.161452</td>\n",
       "      <td>0.333840</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Orleans,</td>\n",
       "      <td>99.677292</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.161367</td>\n",
       "      <td>0.381535</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Louisiana</td>\n",
       "      <td>99.686935</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.161231</td>\n",
       "      <td>0.466709</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>70112-4046</td>\n",
       "      <td>99.530350</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.160994</td>\n",
       "      <td>0.569757</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Telephone:</td>\n",
       "      <td>99.829002</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.177611</td>\n",
       "      <td>0.386017</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>504-299-2100</td>\n",
       "      <td>99.749275</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.177317</td>\n",
       "      <td>0.497576</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Fax</td>\n",
       "      <td>99.835480</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.194055</td>\n",
       "      <td>0.420600</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>504-299-2300</td>\n",
       "      <td>99.800110</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.193913</td>\n",
       "      <td>0.462596</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Federal</td>\n",
       "      <td>99.976990</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.210646</td>\n",
       "      <td>0.371488</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Tax</td>\n",
       "      <td>99.980713</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.210731</td>\n",
       "      <td>0.451021</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ID:</td>\n",
       "      <td>99.926025</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.210756</td>\n",
       "      <td>0.491743</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>72-1435271</td>\n",
       "      <td>94.785866</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.210460</td>\n",
       "      <td>0.531780</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>TEXAS</td>\n",
       "      <td>99.863289</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.267262</td>\n",
       "      <td>0.091820</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>BRINE</td>\n",
       "      <td>99.930008</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.267296</td>\n",
       "      <td>0.147831</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>COMPANY,</td>\n",
       "      <td>92.261787</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.266983</td>\n",
       "      <td>0.200096</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LLC</td>\n",
       "      <td>99.896843</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.267225</td>\n",
       "      <td>0.288229</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Date:</td>\n",
       "      <td>99.726585</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.266890</td>\n",
       "      <td>0.601095</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>October</td>\n",
       "      <td>99.945686</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.266731</td>\n",
       "      <td>0.642796</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>23,</td>\n",
       "      <td>99.457779</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.266918</td>\n",
       "      <td>0.702045</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2017</td>\n",
       "      <td>99.917793</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.266758</td>\n",
       "      <td>0.728666</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>VICE</td>\n",
       "      <td>99.910660</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.281121</td>\n",
       "      <td>0.091234</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>PRESIDENT</td>\n",
       "      <td>99.882462</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.281087</td>\n",
       "      <td>0.133379</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>AND</td>\n",
       "      <td>99.966446</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.281226</td>\n",
       "      <td>0.226461</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>GENERAL</td>\n",
       "      <td>99.895355</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.281111</td>\n",
       "      <td>0.264703</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>COUNSEL</td>\n",
       "      <td>99.914757</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.280979</td>\n",
       "      <td>0.344195</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>TEXAS</td>\n",
       "      <td>99.890228</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.294953</td>\n",
       "      <td>0.091797</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>BRINE</td>\n",
       "      <td>99.931412</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.294989</td>\n",
       "      <td>0.147742</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>COMPANY,</td>\n",
       "      <td>91.865379</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.294822</td>\n",
       "      <td>0.200073</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>LLC</td>\n",
       "      <td>99.945694</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.294998</td>\n",
       "      <td>0.288353</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Invoice</td>\n",
       "      <td>99.892815</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.294361</td>\n",
       "      <td>0.601107</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>No.:</td>\n",
       "      <td>98.696999</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.294412</td>\n",
       "      <td>0.654852</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>118116</td>\n",
       "      <td>99.887848</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.294255</td>\n",
       "      <td>0.688320</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>4800</td>\n",
       "      <td>99.962929</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.308851</td>\n",
       "      <td>0.091625</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>SAN</td>\n",
       "      <td>99.784508</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.308662</td>\n",
       "      <td>0.131476</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>FELIPE</td>\n",
       "      <td>99.641594</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.308544</td>\n",
       "      <td>0.167797</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>HOUSTON,</td>\n",
       "      <td>94.404434</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.322157</td>\n",
       "      <td>0.091840</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>TX</td>\n",
       "      <td>99.930099</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.322108</td>\n",
       "      <td>0.178505</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>77056</td>\n",
       "      <td>99.935173</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.322273</td>\n",
       "      <td>0.207664</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>RE:</td>\n",
       "      <td>99.389565</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.349938</td>\n",
       "      <td>0.146348</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>TEXAS</td>\n",
       "      <td>99.876282</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.349656</td>\n",
       "      <td>0.191127</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>BRINE/ACADIAN</td>\n",
       "      <td>98.882202</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.349738</td>\n",
       "      <td>0.246734</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>PIPELINE</td>\n",
       "      <td>99.830940</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.349694</td>\n",
       "      <td>0.374131</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>-</td>\n",
       "      <td>89.924141</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.354226</td>\n",
       "      <td>0.448521</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>PONTCHARTRAIN</td>\n",
       "      <td>98.709641</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.349306</td>\n",
       "      <td>0.459273</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>NATURAL</td>\n",
       "      <td>99.956985</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.349244</td>\n",
       "      <td>0.598257</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>GAS</td>\n",
       "      <td>99.789009</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.349263</td>\n",
       "      <td>0.675775</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>SYSTEM,</td>\n",
       "      <td>93.764107</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.363356</td>\n",
       "      <td>0.191092</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>K/D/S</td>\n",
       "      <td>99.697189</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.363349</td>\n",
       "      <td>0.268682</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>PROMIX</td>\n",
       "      <td>99.564003</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.363259</td>\n",
       "      <td>0.314449</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>L.L.C.AND</td>\n",
       "      <td>98.851494</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.363107</td>\n",
       "      <td>0.380664</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>ACADIAN</td>\n",
       "      <td>99.771828</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.363223</td>\n",
       "      <td>0.458901</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>GAS</td>\n",
       "      <td>99.693413</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.377245</td>\n",
       "      <td>0.191204</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>PIPELINE</td>\n",
       "      <td>99.827972</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.377167</td>\n",
       "      <td>0.229052</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>SYSTEM</td>\n",
       "      <td>99.977921</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.377053</td>\n",
       "      <td>0.303518</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Our</td>\n",
       "      <td>99.970718</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.391055</td>\n",
       "      <td>0.191128</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Reference:</td>\n",
       "      <td>99.870209</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.390977</td>\n",
       "      <td>0.222088</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>21116.0004</td>\n",
       "      <td>97.485184</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.390990</td>\n",
       "      <td>0.349398</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>For</td>\n",
       "      <td>99.961098</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.432306</td>\n",
       "      <td>0.137429</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Professional</td>\n",
       "      <td>99.664963</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.432162</td>\n",
       "      <td>0.165658</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Services</td>\n",
       "      <td>99.940186</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.432024</td>\n",
       "      <td>0.256520</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Rendered</td>\n",
       "      <td>99.961899</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.432074</td>\n",
       "      <td>0.320779</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Thru</td>\n",
       "      <td>99.956367</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.432048</td>\n",
       "      <td>0.393372</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>09/30/17:</td>\n",
       "      <td>98.430046</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.431866</td>\n",
       "      <td>0.430546</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>SERVICES</td>\n",
       "      <td>99.844299</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.459171</td>\n",
       "      <td>0.468820</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>SUMMARY</td>\n",
       "      <td>99.894470</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.459024</td>\n",
       "      <td>0.552993</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Timekeeper</td>\n",
       "      <td>99.686707</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.492501</td>\n",
       "      <td>0.198521</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Hours</td>\n",
       "      <td>99.949669</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.492341</td>\n",
       "      <td>0.425371</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Rate</td>\n",
       "      <td>99.961571</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.492260</td>\n",
       "      <td>0.618386</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Total</td>\n",
       "      <td>99.960831</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.491954</td>\n",
       "      <td>0.829357</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Leopold</td>\n",
       "      <td>99.811234</td>\n",
       "      <td>Person</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.506260</td>\n",
       "      <td>0.198972</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Z.</td>\n",
       "      <td>89.453529</td>\n",
       "      <td>Person</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.506455</td>\n",
       "      <td>0.258759</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Sher</td>\n",
       "      <td>99.561241</td>\n",
       "      <td>Person</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.506242</td>\n",
       "      <td>0.277839</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0,25</td>\n",
       "      <td>72.842339</td>\n",
       "      <td>Hours</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.506321</td>\n",
       "      <td>0.439881</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>hrs</td>\n",
       "      <td>99.896034</td>\n",
       "      <td>Hours</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.506105</td>\n",
       "      <td>0.484812</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>at</td>\n",
       "      <td>99.933914</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.506487</td>\n",
       "      <td>0.510529</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>$390.00</td>\n",
       "      <td>99.969902</td>\n",
       "      <td>Rate</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.505501</td>\n",
       "      <td>0.594950</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>/hr</td>\n",
       "      <td>99.588875</td>\n",
       "      <td>Rate</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.505763</td>\n",
       "      <td>0.670121</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>$97.50</td>\n",
       "      <td>99.928932</td>\n",
       "      <td>Total</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.505397</td>\n",
       "      <td>0.818871</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Kevin</td>\n",
       "      <td>99.862579</td>\n",
       "      <td>Person</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.522661</td>\n",
       "      <td>0.199034</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>McGlone</td>\n",
       "      <td>99.764442</td>\n",
       "      <td>Person</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.522692</td>\n",
       "      <td>0.242957</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>5.00</td>\n",
       "      <td>98.811661</td>\n",
       "      <td>Hours</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.522809</td>\n",
       "      <td>0.440294</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>hrs</td>\n",
       "      <td>99.918274</td>\n",
       "      <td>Hours</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.522653</td>\n",
       "      <td>0.484874</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>at</td>\n",
       "      <td>99.951401</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.523084</td>\n",
       "      <td>0.510345</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>$335.00</td>\n",
       "      <td>99.981598</td>\n",
       "      <td>Rate</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.521938</td>\n",
       "      <td>0.594973</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>/hr</td>\n",
       "      <td>99.734154</td>\n",
       "      <td>Hours</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.522287</td>\n",
       "      <td>0.670151</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>$1,675.00</td>\n",
       "      <td>99.920998</td>\n",
       "      <td>Total</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.521828</td>\n",
       "      <td>0.797077</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Jeffrey</td>\n",
       "      <td>99.727699</td>\n",
       "      <td>Person</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.539058</td>\n",
       "      <td>0.198878</td>\n",
       "      <td>PRINTED</td>\n",
       "      <td>Sher Garner Inv. 118116 (14262).pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>sagemaker-us-east-1-015943506230</td>\n",
       "      <td>DynamicTableParser/data/raw-inference</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             text  text_confidence   label label_confidence       top  \\\n",
       "0            Sher        99.961830    None             0.00  0.081093   \n",
       "1          Garner        99.975700    None             0.00  0.081046   \n",
       "2          Cahill        99.927826    None             0.00  0.080641   \n",
       "3         Richter        99.849930    None             0.00  0.080691   \n",
       "4           Klein        99.951691    None             0.00  0.105747   \n",
       "5               &        99.851669    None             0.00  0.105804   \n",
       "6        Hilbert,        99.933090    None             0.00  0.105484   \n",
       "7          L.L.C.        95.647316    None             0.00  0.105493   \n",
       "8   Twenty-Eighth        99.147263    None             0.00  0.127919   \n",
       "9           Floor        99.935326    None             0.00  0.128136   \n",
       "10            909        99.907326    None             0.00  0.144890   \n",
       "11        Poydras        99.796577    None             0.00  0.144887   \n",
       "12         Street        99.922928    None             0.00  0.144844   \n",
       "13            New        99.959145    None             0.00  0.161452   \n",
       "14       Orleans,        99.677292    None             0.00  0.161367   \n",
       "15      Louisiana        99.686935    None             0.00  0.161231   \n",
       "16     70112-4046        99.530350    None             0.00  0.160994   \n",
       "17     Telephone:        99.829002    None             0.00  0.177611   \n",
       "18   504-299-2100        99.749275    None             0.00  0.177317   \n",
       "19            Fax        99.835480    None             0.00  0.194055   \n",
       "20   504-299-2300        99.800110    None             0.00  0.193913   \n",
       "21        Federal        99.976990    None             0.00  0.210646   \n",
       "22            Tax        99.980713    None             0.00  0.210731   \n",
       "23            ID:        99.926025    None             0.00  0.210756   \n",
       "24     72-1435271        94.785866    None             0.00  0.210460   \n",
       "25          TEXAS        99.863289    None             0.00  0.267262   \n",
       "26          BRINE        99.930008    None             0.00  0.267296   \n",
       "27       COMPANY,        92.261787    None             0.00  0.266983   \n",
       "28            LLC        99.896843    None             0.00  0.267225   \n",
       "29          Date:        99.726585    None             0.00  0.266890   \n",
       "30        October        99.945686    None             0.00  0.266731   \n",
       "31            23,        99.457779    None             0.00  0.266918   \n",
       "32           2017        99.917793    None             0.00  0.266758   \n",
       "33           VICE        99.910660    None             0.00  0.281121   \n",
       "34      PRESIDENT        99.882462    None             0.00  0.281087   \n",
       "35            AND        99.966446    None             0.00  0.281226   \n",
       "36        GENERAL        99.895355    None             0.00  0.281111   \n",
       "37        COUNSEL        99.914757    None             0.00  0.280979   \n",
       "38          TEXAS        99.890228    None             0.00  0.294953   \n",
       "39          BRINE        99.931412    None             0.00  0.294989   \n",
       "40       COMPANY,        91.865379    None             0.00  0.294822   \n",
       "41            LLC        99.945694    None             0.00  0.294998   \n",
       "42        Invoice        99.892815    None             0.00  0.294361   \n",
       "43           No.:        98.696999    None             0.00  0.294412   \n",
       "44         118116        99.887848    None             0.00  0.294255   \n",
       "45           4800        99.962929    None             0.00  0.308851   \n",
       "46            SAN        99.784508    None             0.00  0.308662   \n",
       "47         FELIPE        99.641594    None             0.00  0.308544   \n",
       "48       HOUSTON,        94.404434    None             0.00  0.322157   \n",
       "49             TX        99.930099    None             0.00  0.322108   \n",
       "50          77056        99.935173    None             0.00  0.322273   \n",
       "51            RE:        99.389565    None             0.00  0.349938   \n",
       "52          TEXAS        99.876282    None             0.00  0.349656   \n",
       "53  BRINE/ACADIAN        98.882202    None             0.00  0.349738   \n",
       "54       PIPELINE        99.830940    None             0.00  0.349694   \n",
       "55              -        89.924141    None             0.00  0.354226   \n",
       "56  PONTCHARTRAIN        98.709641    None             0.00  0.349306   \n",
       "57        NATURAL        99.956985    None             0.00  0.349244   \n",
       "58            GAS        99.789009    None             0.00  0.349263   \n",
       "59        SYSTEM,        93.764107    None             0.00  0.363356   \n",
       "60          K/D/S        99.697189    None             0.00  0.363349   \n",
       "61         PROMIX        99.564003    None             0.00  0.363259   \n",
       "62      L.L.C.AND        98.851494    None             0.00  0.363107   \n",
       "63        ACADIAN        99.771828    None             0.00  0.363223   \n",
       "64            GAS        99.693413    None             0.00  0.377245   \n",
       "65       PIPELINE        99.827972    None             0.00  0.377167   \n",
       "66         SYSTEM        99.977921    None             0.00  0.377053   \n",
       "67            Our        99.970718    None             0.00  0.391055   \n",
       "68     Reference:        99.870209    None             0.00  0.390977   \n",
       "69     21116.0004        97.485184    None             0.00  0.390990   \n",
       "70            For        99.961098    None             0.00  0.432306   \n",
       "71   Professional        99.664963    None             0.00  0.432162   \n",
       "72       Services        99.940186    None             0.00  0.432024   \n",
       "73       Rendered        99.961899    None             0.00  0.432074   \n",
       "74           Thru        99.956367    None             0.00  0.432048   \n",
       "75      09/30/17:        98.430046    None             0.00  0.431866   \n",
       "76       SERVICES        99.844299    None             0.00  0.459171   \n",
       "77        SUMMARY        99.894470    None             0.00  0.459024   \n",
       "78     Timekeeper        99.686707    None             0.00  0.492501   \n",
       "79          Hours        99.949669    None             0.00  0.492341   \n",
       "80           Rate        99.961571    None             0.00  0.492260   \n",
       "81          Total        99.960831    None             0.00  0.491954   \n",
       "82        Leopold        99.811234  Person             0.00  0.506260   \n",
       "83             Z.        89.453529  Person             0.00  0.506455   \n",
       "84           Sher        99.561241  Person             0.00  0.506242   \n",
       "85           0,25        72.842339   Hours             0.00  0.506321   \n",
       "86            hrs        99.896034   Hours             0.00  0.506105   \n",
       "87             at        99.933914    None             0.00  0.506487   \n",
       "88        $390.00        99.969902    Rate             0.00  0.505501   \n",
       "89            /hr        99.588875    Rate             0.00  0.505763   \n",
       "90         $97.50        99.928932   Total             0.00  0.505397   \n",
       "91          Kevin        99.862579  Person             0.00  0.522661   \n",
       "92        McGlone        99.764442  Person             0.00  0.522692   \n",
       "93           5.00        98.811661   Hours             0.00  0.522809   \n",
       "94            hrs        99.918274   Hours             0.00  0.522653   \n",
       "95             at        99.951401    None             0.00  0.523084   \n",
       "96        $335.00        99.981598    Rate             0.00  0.521938   \n",
       "97            /hr        99.734154   Hours             0.00  0.522287   \n",
       "98      $1,675.00        99.920998   Total             0.00  0.521828   \n",
       "99        Jeffrey        99.727699  Person             0.00  0.539058   \n",
       "\n",
       "        left textType                            file_name page_number  \\\n",
       "0   0.322362  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "1   0.395771  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "2   0.498814  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "3   0.588421  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "4   0.349694  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "5   0.431569  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "6   0.462552  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "7   0.576021  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "8   0.406729  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "9   0.551785  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "10  0.417617  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "11  0.456229  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "12  0.537171  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "13  0.333840  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "14  0.381535  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "15  0.466709  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "16  0.569757  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "17  0.386017  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "18  0.497576  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "19  0.420600  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "20  0.462596  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "21  0.371488  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "22  0.451021  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "23  0.491743  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "24  0.531780  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "25  0.091820  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "26  0.147831  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "27  0.200096  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "28  0.288229  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "29  0.601095  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "30  0.642796  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "31  0.702045  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "32  0.728666  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "33  0.091234  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "34  0.133379  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "35  0.226461  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "36  0.264703  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "37  0.344195  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "38  0.091797  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "39  0.147742  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "40  0.200073  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "41  0.288353  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "42  0.601107  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "43  0.654852  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "44  0.688320  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "45  0.091625  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "46  0.131476  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "47  0.167797  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "48  0.091840  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "49  0.178505  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "50  0.207664  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "51  0.146348  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "52  0.191127  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "53  0.246734  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "54  0.374131  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "55  0.448521  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "56  0.459273  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "57  0.598257  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "58  0.675775  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "59  0.191092  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "60  0.268682  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "61  0.314449  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "62  0.380664  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "63  0.458901  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "64  0.191204  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "65  0.229052  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "66  0.303518  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "67  0.191128  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "68  0.222088  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "69  0.349398  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "70  0.137429  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "71  0.165658  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "72  0.256520  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "73  0.320779  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "74  0.393372  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "75  0.430546  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "76  0.468820  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "77  0.552993  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "78  0.198521  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "79  0.425371  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "80  0.618386  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "81  0.829357  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "82  0.198972  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "83  0.258759  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "84  0.277839  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "85  0.439881  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "86  0.484812  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "87  0.510529  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "88  0.594950  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "89  0.670121  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "90  0.818871  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "91  0.199034  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "92  0.242957  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "93  0.440294  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "94  0.484874  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "95  0.510345  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "96  0.594973  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "97  0.670151  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "98  0.797077  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "99  0.198878  PRINTED  Sher Garner Inv. 118116 (14262).pdf           2   \n",
       "\n",
       "                              bucket                                    key  \n",
       "0   sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "1   sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "2   sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "3   sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "4   sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "5   sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "6   sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "7   sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "8   sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "9   sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "10  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "11  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "12  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "13  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "14  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "15  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "16  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "17  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "18  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "19  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "20  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "21  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "22  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "23  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "24  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "25  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "26  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "27  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "28  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "29  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "30  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "31  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "32  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "33  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "34  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "35  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "36  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "37  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "38  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "39  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "40  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "41  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "42  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "43  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "44  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "45  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "46  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "47  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "48  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "49  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "50  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "51  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "52  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "53  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "54  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "55  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "56  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "57  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "58  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "59  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "60  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "61  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "62  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "63  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "64  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "65  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "66  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "67  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "68  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "69  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "70  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "71  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "72  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "73  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "74  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "75  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "76  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "77  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "78  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "79  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "80  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "81  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "82  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "83  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "84  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "85  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "86  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "87  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "88  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "89  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "90  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "91  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "92  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "93  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "94  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "95  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "96  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "97  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "98  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  \n",
       "99  sagemaker-us-east-1-015943506230  DynamicTableParser/data/raw-inference  "
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('max_rows', None)\n",
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f61f10-62fc-452c-bd12-4b0679c4f9de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fd65b9e4-1bde-4df5-8f16-3e9c8b19da1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# shut down end point after use so we don't incur needless charges. \n",
    "predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e369dccd-44f5-41ef-9156-9bda29303f82",
   "metadata": {},
   "source": [
    "## Part 6 - Parse inference response to a rows and columns (tabulear)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03af0a41-99c0-42d0-b443-261b3c78b710",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 7 - Quality Control \n",
    "Extract control totals at the invoice level and compare the detail amounts of hours and total amount to the control totals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e91da9-1509-4a90-95e1-aaaafc78f1da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e827f9f-e04a-4499-97eb-603542fc433e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aa457c-1723-43df-a97f-cd44fbac69ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5074314-74c2-4936-801e-b161f7ec480c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9835769b-c255-4226-978b-8258891eb1e8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Define the challenge\n",
    "\n",
    "So we have our sample documents - what information would we like to extract from them?\n",
    "\n",
    "As an example, we'll consider a market data aggregation use case: Collecting information like interest rates, fees, provider and product names, and some other more challenging examples like minimum payment descriptions and locally-applicable terms. The cell below defines the list of entities for the use-case, with some tips on how to annotate them that you'll also be able to see in the data labelling UI later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b51fe7d-d415-4535-b05c-891826a3b997",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from util.postproc.config import FieldConfiguration\n",
    "\n",
    "# For config API details, you can see the docs in the source file or run:\n",
    "# help(FieldConfiguration)\n",
    "\n",
    "fields = [\n",
    "    # (To prevent human error, enter class_id=0 each time and update programmatically below)\n",
    "    FieldConfiguration(0, \"Date\", optional=True, select=\"confidence\",\n",
    "        annotation_guidance=(\n",
    "            \"<p>This is the date that the task was performed by a person.  This should be a date field in mm/dd/yyyy, mm/dd, or yyyy-mm-dd, or dd-mm-yyyy</p>\"\n",
    "        ),\n",
    "    ),\n",
    "    FieldConfiguration(0, \"Person\", optional=True, select=\"confidence\",\n",
    "        annotation_guidance=(\n",
    "            \"<p>This is the person doing the work. It can have a column label such as Employee, Name, Timekeeper, Initals, etc.</p>\"\n",
    "        ),\n",
    "    ),\n",
    "    FieldConfiguration(0, \"Description\", optional=True, select=\"confidence\",\n",
    "        annotation_guidance=(\n",
    "            \"<p>It is a phrase or sentence that describes the task that was peformed by the person.</p>\"\n",
    "        ),\n",
    "    ),\n",
    "    FieldConfiguration(0, \"Hours\", optional=True, select=\"confidence\",\n",
    "        annotation_guidance=(\n",
    "            \"<p>This is a numeric or decimal value that represents the number of hours and minutes a person worked on the task.</p>\"\n",
    "        ),\n",
    "    ),\n",
    "    FieldConfiguration(0, \"Rate\", optional=True, select=\"confidence\",\n",
    "        annotation_guidance=(\n",
    "            \"<p>This is a numeric or decimal value that represents the amount the person charges per hour.</p>\"\n",
    "        ),\n",
    "    ),\n",
    "    FieldConfiguration(0, \"Total\", optional=True, select=\"confidence\",\n",
    "        annotation_guidance=(\n",
    "            \"<p>This is a numeric or deciaml amount that describes the total cost of Rate * Hours for a task.  It is optional.  This is not to be confused with a grand total. </p>\"\n",
    "        ),\n",
    "    ),\n",
    "    \n",
    "]\n",
    "for ix, cfg in enumerate(fields):\n",
    "    cfg.class_id = ix\n",
    "\n",
    "# Save the configuration to file:\n",
    "with open(\"data/field-config.json\", \"w\") as f:\n",
    "    f.write(json.dumps(\n",
    "        [cfg.to_dict() for cfg in fields],\n",
    "        indent=2,\n",
    "    ))\n",
    "\n",
    "# And print out a simple list:\n",
    "entity_classes = [f.name for f in fields]\n",
    "print(\"\\n\".join(entity_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f19c38-3c31-4a52-b9a8-497265f5cb30",
   "metadata": {},
   "source": [
    "---\n",
    "## Filter a sample corpus\n",
    "\n",
    "For a quick example model, there's no need for us to process or annotate all ~2,500 documents in the original corpus. Here, we'll select a random subset - but ensuring those present in the pre-prepared annotation data are kept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468130b2-5d40-4125-9654-eaf42586a12d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Crawl source annotated Textract URIs from the job manifests:\n",
    "annotated_textract_s3uris = util.ocr.list_preannotated_textract_uris(\n",
    "    ann_jobs_folder=\"data/annotations/BT_annotations\",\n",
    "    exclude_job_names=[\"LICENSE\"],\n",
    ")\n",
    "print (annotated_textract_s3uris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0675a4ec-324a-4030-927d-2e18ec5c16c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define how to check for matches:\n",
    "def textract_uri_matches_doc_uri(tex_uri, doc_uri) -> bool:\n",
    "    \"\"\"Customize this function if needed for your use case's data layout\"\"\"\n",
    "    # With our sample, Textract URIs will look like:\n",
    "    # some/prefix/data/textracted/subfolders/file.pdf/consolidated.json\n",
    "    tex_s3key = tex_uri[len(\"s3://\"):].partition(\"/\")[2]\n",
    "    # With our sample, Raw URIs will look like:\n",
    "    # some/prefix/data/raw/subfolders/file.pdf\n",
    "    doc_s3key = doc_uri[len(\"s3://\"):].partition(\"/\")[2]\n",
    "\n",
    "    # Given the expectations above:\n",
    "    tex_rel_filepath = tex_s3key.partition(\"data/textracted/\")[2].rpartition(\"/\")[0]\n",
    "    doc_rel_filepath = doc_s3key.partition(\"data/raw/\")[2]\n",
    "    return doc_rel_filepath == tex_rel_filepath\n",
    "\n",
    "# Build the list of docs for which some annotations exist (prioritising debug over speed here):\n",
    "annotated_doc_s3uris = set()\n",
    "for uri in annotated_textract_s3uris:\n",
    "    matching_doc_s3uris = [\n",
    "        doc_s3uri\n",
    "        for doc_s3uri in raw_doc_s3uris\n",
    "        if textract_uri_matches_doc_uri(uri, doc_s3uri)\n",
    "    ]\n",
    "    n_matches = len(matching_doc_s3uris)\n",
    "    if n_matches == 0:\n",
    "        raise ValueError(\n",
    "            \"Couldn't find matching document in dataset for annotated Textract URI: %s\"\n",
    "            % (uri,)\n",
    "        )\n",
    "    if n_matches > 1:\n",
    "        logger.warning(\n",
    "            \"Textract URI matched %s document URIs: Matching criterion may be too loose.\\n%s\\n%s\",\n",
    "            n_matches,\n",
    "            uri,\n",
    "            matching_doc_s3uris,\n",
    "        )\n",
    "    annotated_doc_s3uris.update(matching_doc_s3uris)\n",
    "\n",
    "# This sorted list of required document S3 URIs is the main result you need to get to here:\n",
    "annotated_doc_s3uris = sorted(annotated_doc_s3uris)\n",
    "print(f\"Found {len(annotated_doc_s3uris)} docs with pre-existing annotations\")\n",
    "print(\"For example:\")\n",
    "print(\"\\n\".join(annotated_doc_s3uris[:5] + [\"...\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f984f6-beab-420e-a9a1-503ae4e0894f",
   "metadata": {},
   "source": [
    "Both Amazon Textract and the multi-lingual entity recognition model we'll use later should be capable of processing Spanish, but you may want to exclude the small number of Spanish-language docs in the corpus if you're not able to confidently read and annotate them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d8d03f-7834-4378-a63c-7e9202db4eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_DOCS_KEPT = 120\n",
    "SKIP_SPANISH_DOCS = True\n",
    "\n",
    "\n",
    "def include_filename(name: str) -> bool:\n",
    "    \"\"\"Filter out likely Spanish/non-English docs (if SKIP_SPANISH_DOCS enabled)\"\"\"\n",
    "    if not name:\n",
    "        return False\n",
    "    if not SKIP_SPANISH_DOCS:\n",
    "        return True\n",
    "    name_l = name.lower()\n",
    "    if (\n",
    "        \"spanish\" in name_l\n",
    "        or \"tarjeta\" in name_l\n",
    "        or re.search(r\"espa[nñ]ol\", name_l)\n",
    "        or re.search(r\"[\\[\\(]esp?[\\]\\)]\", name_l)\n",
    "        or re.search(r\"cr[eé]dito\", name_l)\n",
    "    ):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "if N_DOCS_KEPT < len(annotated_doc_s3uris):\n",
    "    raise ValueError(\n",
    "        \"Existing annotations cannot be used for model training unless the target documents are \"\n",
    "        \"Textracted. To proceed with fewer docs than have already been annotated, you'll need to \"\n",
    "        \"`exclude_job_names` per the 'data/annotations' folder (e.g. ['augmentation-1']) AND \"\n",
    "        \"remember to not include them in notebook 2 (model training). Alternatively, increase \"\n",
    "        f\"your N_DOCS_KEPT. (Got {N_DOCS_KEPT} vs {len(annotated_doc_s3uris)} prev annotations).\"\n",
    "    )\n",
    "\n",
    "with open(\"data/raw-all.manifest.jsonl\") as f:\n",
    "    # First apply filtering rules:\n",
    "    sampled_docs = [\n",
    "        doc for doc in (json.loads(line) for line in f)\n",
    "        if include_filename(doc[\"raw-ref\"])\n",
    "    ]\n",
    "\n",
    "# Forcibly including the pre-annotated docs *after* the shuffling ensures that the order of\n",
    "# sampling new docs is independent of what/how many have been pre-annotated:\n",
    "required_docs = [d for d in sampled_docs if d[\"raw-ref\"] in annotated_doc_s3uris]\n",
    "random.Random(1337).shuffle(sampled_docs)\n",
    "new_docs = [d for d in sampled_docs if d[\"raw-ref\"] not in annotated_doc_s3uris]\n",
    "sampled_docs = sorted(\n",
    "    required_docs + new_docs[:N_DOCS_KEPT - len(required_docs)],\n",
    "    key=lambda doc: doc[\"raw-ref\"],\n",
    ")\n",
    "\n",
    "# Write the selected set to file:\n",
    "with open(\"data/raw-sample.manifest.jsonl\", \"w\") as f:\n",
    "    for d in sampled_docs:\n",
    "        f.write(json.dumps(d) + \"\\n\")\n",
    "\n",
    "print(f\"Extracted random sample of {len(sampled_docs)} docs\")\n",
    "sampled_docs[:5] + [\"...\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7285f0-0f3f-4697-ad70-e512a65310aa",
   "metadata": {},
   "source": [
    "> ▶️ In [data/raw-sample.manifest.jsonl](data/raw-sample.manifest.jsonl) you should now have an alphabetized list of the `N_DOCS_KEPT` randomly selected documents, which should include any documents referenced in existing annotations under `data/annotations`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a43ae7-0a41-471b-88bb-d6255c4e123f",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## OCR the input documents\n",
    "\n",
    "> ⚠️ **Note:** Refer to the [Amazon Textract Pricing Page](https://aws.amazon.com/textract/pricing/) for up-to-date guidance before running large extraction jobs.\n",
    ">\n",
    "> At the time of writing, the projected cost (in `us-east-1`, ignoring free tier allowances) of analyzing 100 documents with 10 pages on average was approximately \\\\$67 with `TABLES` and `FORMS` enabled, or \\\\$2 without. Across the full corpus, we measured the average number of pages per document at approximately 6.7.\n",
    "\n",
    "With (a subset of) the raw documents selected, the next ingredient is to link them with Amazon Textract-compatible OCR results in a new manifest - with entries something like:\n",
    "\n",
    "```json\n",
    "{\"raw-ref\": \"s3://doc-example-bucket/folder/mydoc.pdf\", \"textract-ref\": \"s3://doc-example-bucket/folder/mydoc-textracted.json\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53aec4c-882b-4a89-aafe-46fe1165489c",
   "metadata": {},
   "source": [
    "We need to be mindful of the service [quotas](https://docs.aws.amazon.com/general/latest/gr/textract.html#limits_textract) when processing large batches of documents with Amazon Textract, to avoid excessive rate limiting and retries. Since an OCR pipeline solution stack is already set up for this sample, you can use just the *Amazon Textract portion of the pipeline* to process the documents in bulk.\n",
    "\n",
    "> ⏰ This process took about 6 minutes to run against the 120-document sample set in our tests.\n",
    "\n",
    "> ⚠️ **If you see errors in the output:**\n",
    ">\n",
    "> - Try re-running the cell - Rate limiting can sometimes cause intermittent failures, and the function will skip successfully processed files in repeat runs.\n",
    "> - Persistent errors (on custom datasets) could be due to malformed files (remove them from the manifest) or very large files (see the [/CUSTOMIZATION_GUIDE.md](../CUSTOMIZATION_GUIDE.md) for tips on re-configuring your pipeline to handle very large documents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8456c95-7e74-477b-b0e3-36271f788a2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "textract_results = util.ocr.call_textract(\n",
    "    textract_sfn_arn=config.plain_textract_sfn_arn,\n",
    "    # Can instead use raw-all.manifest.jsonl to process whole dataset (see cost note above):\n",
    "    input_manifest=\"data/raw-sample.manifest.jsonl\",\n",
    "    manifest_raw_field=\"raw-ref\",\n",
    "    manifest_out_field=\"textract-ref\",\n",
    "    # Map subpaths of {input_base} to subpaths of {output_base}:\n",
    "    output_base_s3uri=textract_s3uri,\n",
    "    input_base_s3uri=raw_s3uri,\n",
    "    # Note that turning on additional features can have significant impact on API costs:\n",
    "    features=[\"FORMS\", \"TABLES\"],\n",
    "    skip_existing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f46c79-02e3-49ee-bbeb-083f730f3a0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this is annoying.\n",
    "# I am not sure where to look to \"Doc failed to process - see results for details\"\n",
    "# this is just doing OCR with Textracks and writting to here\n",
    "# maybe we can simulate this for now and use our own Textrack responses .json files \n",
    "# TO DO:  Use your own CODE TO OCR document to .JSON until we can figure out how to modify this code and view the log\n",
    "print (textract_s3uri)\n",
    "#  we are going to proceed with our .JSON files to bypass this step.\n",
    "\n",
    "# fix:  use the default bucket.   I don't thing the the cloud formation template setup an ROLE with \n",
    "# to ready S3, just the default bucket.   \n",
    "# What access role was created and if so can't we just update the rights to allow to read from S3?\n",
    "\n",
    "# Workaround :   Use the Default S3 bucket - sagemaker-us-east-1-015943506230"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3e0b6f-2fc0-49c5-9477-99e2eae9291a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print (textract_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a184c07-dd58-4912-8c6c-a2d14fcc88a7",
   "metadata": {},
   "source": [
    "Once the extraction is done, write (only successful items) to a manifest file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38867c33-0e99-4980-b94d-a185598ffd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_success = 0\n",
    "n_fail = 0\n",
    "with open(\"data/textracted-all.manifest.jsonl\", \"w\") as fout:\n",
    "    for ix, item in enumerate(textract_results):\n",
    "        if isinstance(item[\"textract-ref\"], str):\n",
    "            fout.write(json.dumps(item) + \"\\n\")\n",
    "            n_success += 1\n",
    "        else:\n",
    "            if n_fail == 0:\n",
    "                print ()\n",
    "                print (item)\n",
    "                print ()\n",
    "                logger.error(\"First failure at index %s:\\n%s\", ix, item[\"textract-ref\"])\n",
    "                print ()\n",
    "            n_fail += 1\n",
    "\n",
    "print(f\"{n_success} of {n_success + n_fail} docs processed successfully\")\n",
    "if n_fail > 0:\n",
    "    raise ValueError(\n",
    "        \"Are you sure you want to continue? Consider re-trying to process the failed docs\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c686fc8c-353b-487c-b664-1ebf4a87fa6f",
   "metadata": {},
   "source": [
    "> ▶️ You should now have a [data/textracted-all.manifest.jsonl](data/textracted-all.manifest.jsonl) JSON-Lines manifest file mapping source documents `raw-ref` to Amazon Textract result JSONs `textract-ref`: Both as `s3://...` URIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f08ef2c-08ee-4ed7-8abe-3586d6eb7100",
   "metadata": {},
   "source": [
    "### Collate OCR and image data for annotation\n",
    "\n",
    "Now we have a filtered corpus of documents with Amazon Textract results, plus cleaned and standardized images for each page - all available on Amazon S3.\n",
    "\n",
    "To prepare for data annotation and later model training, we'll need to collate these together with a **page-level manifest** in JSON-lines format, with records something like:\n",
    "\n",
    "```json\n",
    "{\"source-ref\": \"s3://doc-example-bucket/img-prefix/folder/filename-0001-01.png\", \"textract-ref\": \"s3://doc-example-bucket/tex-prefix/folder/filename.pdf/consolidated.json\", \"page-num\": 1}\n",
    "```\n",
    "\n",
    "Key features of the format are:\n",
    "- The `source-ref` is the path to a full-resolution cleaned page image (**not** a thumbnail), **but** model training in the next notebook will assume the equivalent thumbnail path is identical, except for some different s3://... bucket & prefix.\n",
    "- The `page-num` is one-based (always >= 1), and for model training must match the image to the appropriate page number **in the linked Textract JSON file**.\n",
    "    - For example if you have thumbnail `filename-0001-15.png` for page 15 of some long document, but for some reason your `textract-ref` JSON file contains *only* detections from page 15 of the document, you would set `\"page-num\": 1`.\n",
    "- Mapping through the `raw-ref` here is nice to have, but optional, as the model training won't refer to the original document.\n",
    "\n",
    "The key goal is to create a page-level catalogue that we're confident is correct, and for that reason the example function below will actually **validate that the artifacts are present on S3** in the expected locations.\n",
    "\n",
    "> ⏰ Because of these validation checks, the cell below may a minute or two to run against our 120-document sample set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8ec1ae-a892-4f55-8ac2-2f493a5a58b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "warnings = util.preproc.collate_data_manifest(\n",
    "    # Output file:\n",
    "    \"data/pages-all-sample.manifest.jsonl\",\n",
    "    # Input manifest:\n",
    "    input_manifest=\"data/textracted-all.manifest.jsonl\",\n",
    "    # s3://... base URI used to try and map 'textract-ref's to cleaned images:\n",
    "    textract_s3_prefix=textract_s3uri,\n",
    "    # The s3://... base URI under which page images are stored:\n",
    "    imgs_s3_prefix=imgs_s3uri,\n",
    "    # Optional s3://... base URI also used to try and map 'raw-ref's to images if present:\n",
    "    raw_s3_prefix=raw_s3uri,\n",
    "    # Other output manifest settings:\n",
    "    by=\"page\",\n",
    "    no_content=\"omit\",\n",
    ")\n",
    "\n",
    "if len(warnings):\n",
    "    raise ValueError(\n",
    "        \"Manifest usable but incomplete - %s docs failed. Please see `warnings` for details\"\n",
    "        % len(warnings)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bb7800-7bad-40bf-bcce-7e72296ad270",
   "metadata": {},
   "source": [
    "> ▶️ You should now have a page-level catalogue linking `source-ref`, `textract-ref`, `page-num` in [data/pages-all-sample.manifest.jsonl](data/pages-all-sample.manifest.jsonl)\n",
    "\n",
    "Let's briefly explore the catalogue we've created. Each line of the file is a JSON record identifying a particular page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ff4bfe-31d7-4f13-8eb9-42b270679238",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/pages-all-sample.manifest.jsonl\", \"r\") as f:\n",
    "    for ix, line in enumerate(f):\n",
    "        print(line, end=\"\")\n",
    "        if ix >= 2:\n",
    "            print(\"...\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52809830-8c28-474c-ba0a-fd879e3021b0",
   "metadata": {},
   "source": [
    "The credit cards corpus has a very skewed distribution of number of pages per document, with a few outliers dragging up the average significantly. In our tests on corpus-wide statistics:\n",
    "\n",
    "- The overall average was **~6.7 pages per document**\n",
    "- The 25th percentile was 3 pages; the 50th percentile was 6 pages; and the 75th percentile was 11 pages\n",
    "- The longest document was 402 pages\n",
    "\n",
    "Your results for sub-sampled sets will likely vary - but can be analyzed as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced078ea-1268-418c-96df-7ba6017a1d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/pages-all-sample.manifest.jsonl\", \"r\") as f:\n",
    "    manifest_df = pd.DataFrame([json.loads(line) for line in f])\n",
    "page_counts_by_doc = manifest_df.groupby(\"textract-ref\")[\"textract-ref\"].count()\n",
    "\n",
    "print(\"Document page count statistics\")\n",
    "page_counts_by_doc.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61da00fe-f0ed-4e7c-b1c8-d0ed13e5bfbf",
   "metadata": {},
   "source": [
    "---\n",
    "## Start the data labelling job\n",
    "\n",
    "Now we have a correlated set of cleaned page images and OCR results for each page, we're ready to start annotating entities to collect model training data. Typically this is an iterative process with multiple rounds of labelling to balance experimentation speed with model accuracy. Here though, we'll show setting up a single small labelling job and combine the results with pre-existing annotations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322cad31-3f79-4e25-8d12-c77bf13cd319",
   "metadata": {},
   "source": [
    "### Sample a dataset to label\n",
    "\n",
    "Below, we:\n",
    "\n",
    "- **Shuffle** our data (in a *reproducible*/deterministic way), to ensure we annotate documents/pages from a range of providers - not just concentrating on the first provider/doc(s)\n",
    "- **Exclude** any examples for which the page image has **already been labeled** in the `data/annotations` output folder\n",
    "- **Stratify** the sample, to obtain a specific (boosted) proportion of first-page samples, since we observed the first pages of documents to often be most useful for the fields of interest in the sample credit cards use case. (Many documents use the first page for a fact-sheet/summary, followed by subsequent pages of dense legal terms).\n",
    "\n",
    "Run the cells below to select a small subset of previously-unlabelled pages and build a manifest file listing them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85916f48-eaa0-4a1f-afa2-df80eb7d1641",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_job_name = \"cfpb-workshop-1\"  # What will this job be called?\n",
    "N_JOB_EXAMPLES = 15  # Select 15 new pages to annotate\n",
    "PCT_FIRST_PAGE = .4  # 40% of samples should be page-num 1\n",
    "\n",
    "preannotated_img_uris = [\n",
    "    f\"{imgs_s3uri}/{path}\"\n",
    "    for path in util.preproc.list_preannotated_img_paths(\n",
    "        annotations_folder=\"data/annotations\",\n",
    "        exclude_job_names=[],\n",
    "        key_prefix=\"data/imgs-clean/\",\n",
    "    )\n",
    "]\n",
    "\n",
    "job_input_manifest_file = f\"data/manifests/{annotation_job_name}.jsonl\"\n",
    "os.makedirs(\"data/manifests\", exist_ok=True)\n",
    "print(f\"'{annotation_job_name}' saving to: {job_input_manifest_file}\")\n",
    "\n",
    "with open(job_input_manifest_file, \"w\") as f:\n",
    "    for ix, example in enumerate(\n",
    "        util.preproc.stratified_sample_first_page_examples(\n",
    "            input_manifest_path=\"data/pages-all-sample.manifest.jsonl\",\n",
    "            n_examples=N_JOB_EXAMPLES,  \n",
    "            pct_first_page=PCT_FIRST_PAGE,\n",
    "            exclude_source_ref_uris=preannotated_img_uris,\n",
    "        )\n",
    "    ):\n",
    "        if ix < 3:\n",
    "            print(example)\n",
    "        elif ix == 3:\n",
    "            print(\"...\")\n",
    "        f.write(json.dumps(example) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0955cd-cf0c-4f9f-8e3f-ce04f494527a",
   "metadata": {},
   "source": [
    "To create the labelling job in SageMaker, this manifest file will also need to be uploaded to Amazon S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650135a2-65d7-4bf3-85f7-bee1d301793c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_manifest_s3uri = f\"s3://{bucket_name}/{bucket_prefix}{job_input_manifest_file}\"\n",
    "!aws s3 cp $job_input_manifest_file $input_manifest_s3uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785e618b-4dc8-4aca-bad5-424289813e8b",
   "metadata": {},
   "source": [
    "### Create the labelling job\n",
    "\n",
    "With a manifest file defining which pages should be included, and your \"work team\" already set up from earlier, you're ready to create your SageMaker Ground Truth labelling job.\n",
    "\n",
    "You could also explore creating this via the AWS Console for SageMaker, but the code below will set up the job with the correct settings for you automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809aa7b8-3f18-4ef8-803f-cf6da1de9682",
   "metadata": {},
   "outputs": [],
   "source": [
    "util.smgt.ensure_bucket_cors(bucket_name)\n",
    "\n",
    "print(f\"Starting labeling job {annotation_job_name}\\non data {input_manifest_s3uri}\\n\")\n",
    "create_labeling_job_resp = util.smgt.create_bbox_labeling_job(\n",
    "    annotation_job_name,\n",
    "    bucket_name=bucket_name,\n",
    "    execution_role_arn=sagemaker.get_execution_role(),\n",
    "    fields=fields,\n",
    "    input_manifest_s3uri=input_manifest_s3uri,\n",
    "    output_s3uri=annotations_base_s3uri,\n",
    "    workteam_arn=workteam_arn,\n",
    "    # To create a review/adjustment job from a manifest with existing labels in:\n",
    "    # reviewing_attribute_name=\"label\",\n",
    "    s3_inputs_prefix=f\"{bucket_prefix}data/manifests\",\n",
    ")\n",
    "print(f\"\\nLABELLING JOB STARTED:\\n{create_labeling_job_resp['LabelingJobArn']}\")\n",
    "print()\n",
    "print(input_manifest_s3uri)\n",
    "print(annotations_base_s3uri)\n",
    "print(sagemaker.get_execution_role())\n",
    "print(\"\\n\".join([\"\\nLabels:\", \"-------\"] + entity_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e294758e-1fd5-4558-b0a5-cf37798ba0ac",
   "metadata": {},
   "source": [
    "---\n",
    "## Before you label - build custom containers\n",
    "\n",
    "The entity recognition model we'll train later uses **customized containers**, which install extra libraries over the standard [SageMaker Hugging Face framework containers](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/index.html).\n",
    "\n",
    "> ⏰ Building these can take several minutes - so before you start labelling your documents in the SageMaker Ground Truth portal, **start the below cells running** to save some time.\n",
    ">\n",
    "> You don't need to wait for them to finish - just move on to the next \"Label the data\" section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1d114b-4f62-4866-8440-b7de14fbc228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations:\n",
    "hf_version = \"4.17\"\n",
    "py_version = \"py38\"\n",
    "pt_version = \"1.10\"\n",
    "train_repo_name = \"sm-ocr-training\"\n",
    "#train_repo_tag = f\"hf-{hf_version}-pt-gpu\"\n",
    "train_repo_tag = \"hf-4.26-pt-gpu\"  # (Base HF version is overridden in Dockerfile)\n",
    "inf_repo_name = \"sm-ocr-inference\"\n",
    "inf_repo_tag = train_repo_tag\n",
    "\n",
    "account_id = sagemaker.Session().account_id()\n",
    "region = os.environ[\"AWS_REGION\"]\n",
    "\n",
    "base_image_params = {\n",
    "    \"framework\": \"huggingface\",\n",
    "    \"region\": region,\n",
    "    \"instance_type\": \"ml.p3.2xlarge\",  # (Just used to check whether GPUs/accelerators are used)\n",
    "    \"py_version\": py_version,\n",
    "    \"version\": hf_version,\n",
    "    \"base_framework_version\": f\"pytorch{pt_version}\",\n",
    "}\n",
    "\n",
    "train_base_uri = sagemaker.image_uris.retrieve(**base_image_params, image_scope=\"training\")\n",
    "inf_base_uri = sagemaker.image_uris.retrieve(**base_image_params, image_scope=\"inference\")\n",
    "\n",
    "# Combine together into the final URIs:\n",
    "train_image_uri = f\"{account_id}.dkr.ecr.{region}.amazonaws.com/{train_repo_name}:{train_repo_tag}\"\n",
    "print(f\"Target training image: {train_image_uri}\")\n",
    "inf_image_uri = f\"{account_id}.dkr.ecr.{region}.amazonaws.com/{inf_repo_name}:{inf_repo_tag}\"\n",
    "print(f\"Target inference image: {inf_image_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2310bdf-17a5-49ba-a125-7aef8134fce4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# (No need to re-run this cell if your train image is already in ECR)\n",
    "\n",
    "# Build and push the training image:\n",
    "!cd custom-containers/train-inf && sm-docker build . \\\n",
    "    --compute-type BUILD_GENERAL1_LARGE \\\n",
    "    --repository {train_repo_name}:{train_repo_tag} \\\n",
    "    --role {config.sm_image_build_role} \\\n",
    "    --build-arg BASE_IMAGE={train_base_uri}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4bb843-284e-4b9e-b322-59c2ec98afff",
   "metadata": {},
   "source": [
    "Note that although our training and inference containers use the [same Dockerfile](custom-containers/train-inf/Dockerfile), they're built from different parent images so both are needed in ECR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6fd214-1739-4dc0-a14f-6702f5a9ecb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# (No need to re-run this cell if your inference image is already in ECR)\n",
    "\n",
    "# Build and push the inference image:\n",
    "!cd custom-containers/train-inf && sm-docker build . \\\n",
    "    --compute-type BUILD_GENERAL1_LARGE \\\n",
    "    --repository {inf_repo_name}:{inf_repo_tag} \\\n",
    "    --role {config.sm_image_build_role} \\\n",
    "    --build-arg BASE_IMAGE={inf_base_uri}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186cc52b-59ea-4d09-8c2e-893ed668eb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check from notebook whether the images were successfully created:\n",
    "ecr = boto3.client(\"ecr\")\n",
    "for repo, tag, uri in (\n",
    "    (train_repo_name, train_repo_tag, train_image_uri),\n",
    "    (inf_repo_name, inf_repo_tag, inf_image_uri)\n",
    "):\n",
    "    imgs_desc = ecr.describe_images(\n",
    "        registryId=account_id,\n",
    "        repositoryName=repo,\n",
    "        imageIds=[{\"imageTag\": tag}],\n",
    "    )\n",
    "    assert len(imgs_desc[\"imageDetails\"]) > 0, f\"Couldn't find ECR image {uri} after build\"\n",
    "    print(f\"Found {uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3576ffea-d0ae-4f45-924a-c1457a5a5af4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Label the data!\n",
    "\n",
    "Shortly after the labeling job has been created, you'll see a new task for your user in the SageMaker Ground Truth **labeling portal**. If you lost the portal link from your email, you can access it from the *Private* tab of the [SageMaker Ground Truth Workforces console](https://console.aws.amazon.com/sagemaker/groundtruth?#/labeling-workforces).\n",
    "\n",
    "▶️ Click **Start working** and annotate the examples until the all are finished and you're returned to the portal homepage.\n",
    "\n",
    "▶️ **Try to be as consistent as possible** in how you annotate the classes, because inconsistent annotations can significantly degrade final model accuracy. Refer to the guidance (in this notebook and the 'Full Instructions') that we applied when annotating the example set.\n",
    "\n",
    "![](img/smgt-task-pending.png \"Screenshot of SMGT labeling portal with pending task\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b2a5c2-f399-4f15-93ee-61715a141c2e",
   "metadata": {},
   "source": [
    "### Sync the results locally (and iterate?)\n",
    "\n",
    "Once you've finished annotating and the job shows as \"Complete\" in the [SMGT Console](https://console.aws.amazon.com/sagemaker/groundtruth?#/labeling-jobs) (which **might take an extra minute or two**, while your annotations are consolidated), you can download the results here to the notebook via the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ceb096a-3ecf-4112-a2bd-3899d59d7f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync --quiet $annotations_base_s3uri ./data/annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84178ce6-4794-49cf-8223-a00b7d4e06f1",
   "metadata": {},
   "source": [
    "You should see a subfolder created with the name of your annotation job, under which the **`manifests/output/output.manifest`** file contains the consolidated results of your labelling - again in the open JSON-Lines format.\n",
    "\n",
    "▶️ **Check** your results appear as expected, and explore the file format.\n",
    "\n",
    "> Because label outputs are in JSON-Lines, it's easy to consolidate, transform, and manipulate these results as required using open source tools!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ee9463-88eb-4eec-98eb-0d0b6eb10a64",
   "metadata": {},
   "source": [
    "---\n",
    "## Consolidate annotated data\n",
    "\n",
    "To construct a model training set, we'll typically need to consolidate the results of multiple SageMaker Ground Truth labelling jobs: Perhaps because the work was split up into more manageable chunks - or maybe because additional review/adjustment jobs were run to improve label quality.\n",
    "\n",
    "Inside your `data/annotations` folder, you'll find some **pre-annotated augmentation data** provided for you already (in the `augmentation-` subfolders). These datasets are not especially large or externally useful, but will help you train an example model without too much (or even any!) manual annotation effort.\n",
    "\n",
    "▶️ **Edit** the `include_jobs` line below to control which datasets (pre-provided and your own) will be included:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5524dde5-560f-4893-8b1e-1b3848624d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "include_jobs = [\n",
    "    \"layoutlm-boxes-2023-04-27-0251\",\n",
    "    \"layoutlm-boxes-2023-05-02-0045\",\n",
    "    # TODO: Can edit the below to include your custom data, if you were able to label it:\n",
    "    # \"cfpb-workshop-1\",\n",
    "]\n",
    "\n",
    "\n",
    "source_manifests = []\n",
    "for job_name in sorted(filter(\n",
    "    lambda n: os.path.isdir(f\"data/annotations/BT_annotations/{n}\"),\n",
    "    os.listdir(\"data/annotations/BT_annotations\")\n",
    ")):\n",
    "    if job_name not in include_jobs:\n",
    "        logger.warning(f\"Skipping {job_name} (not in include_jobs list)\")\n",
    "        continue\n",
    "    job_manifest_path = f\"data/annotations/BT_annotations/{job_name}/manifests/output/output.manifest\"\n",
    "    if not os.path.isfile(job_manifest_path):\n",
    "        raise RuntimeError(f\"Could not find job output manifest {job_manifest_path}\")\n",
    "    source_manifests.append({\"job_name\": job_name, \"manifest_path\": job_manifest_path})\n",
    "\n",
    "print(f\"Got {len(source_manifests)} annotated manifests:\")\n",
    "print(\"\\n\".join(map(lambda o: o[\"manifest_path\"], source_manifests)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d09939f-b6c3-41c3-b1ae-95bce8535e39",
   "metadata": {},
   "source": [
    "Note that to **combine multiple output manifests to a single dataset**:\n",
    "\n",
    "- The labels must be stored in the same attribute on every record (records use the labeling job name by default, which will be different between jobs).\n",
    "- If importing data collected from some other account (like the `augmentation-` sets), we'll need to **map the S3 URIs** to equivalent links on your own bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec049d0-7fc2-4f28-b2c9-0ea74aa5e122",
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_label_field = \"label\"\n",
    "\n",
    "print(\"Writing data/annotations/annotations-all.manifest.jsonl\")\n",
    "with open(\"data/annotations/annotations-all.manifest.jsonl\", \"w\") as fout:\n",
    "    util.preproc.consolidate_data_manifests(\n",
    "        source_manifests,\n",
    "        fout,\n",
    "        standard_label_field=standard_label_field,\n",
    "        bucket_mappings={\"DOC-EXAMPLE-BUCKET\": bucket_name},\n",
    "        prefix_mappings={\"EXAMPLE-PREFIX/\": bucket_prefix},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae7d66d-9d3a-4a26-9ed6-4dfad717d362",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_s3_path(s3_path):\n",
    "    path_parts=s3_path.replace(\"s3://\",\"\").split(\"/\")\n",
    "    bucket=path_parts.pop(0)\n",
    "    key=\"/\".join(path_parts [:-1])\n",
    "    filename = path_parts[-1]\n",
    "    return bucket, key, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbe5993-8e85-4241-a225-22de402af01d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# populate the dictionary of our annotation manafests created in Gound Truth \n",
    "annotation_manafest_dict = {}\n",
    "with open(\"data/annotations/annotations-all.manifest.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        annotation_json = json.loads(line)\n",
    "        bucket, key, filename = split_s3_path(  annotation_json['source-ref'])\n",
    "\n",
    "        # update the soure ref to our use our thumbnails we created.\n",
    "        # this is useful if you use LayoutLM2 and beyond\n",
    "        new_source_ref = r's3://' + bucket_name + r'/' + bucket_prefix + r'data/thumbnails/' + filename\n",
    "        annotation_json['source-ref'] = new_source_ref\n",
    "        #print (annotation_json)\n",
    "        annotation_manafest_dict[filename] = annotation_json\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b776a872-a0b6-46ff-8bd6-6b2dcdc7c821",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# enhance the textracks json that we will use for training data; we need\n",
    "# to add our Ground Truth annatations \n",
    "with open(\"data/textracted-BT-enhanced.manifest.jsonl\", \"w\") as updated_json:\n",
    "    with open(\"data/textracted-all.manifest.jsonl\", \"r\") as f:\n",
    "        for line in f:\n",
    "            texttracts_json = json.loads(line)\n",
    "            bucket, key, filename = split_s3_path( texttracts_json['raw-ref'])\n",
    "            #print (bucket)\n",
    "            #print (key)     \n",
    "            #print (filename)\n",
    "\n",
    "            # get the annotation json that matches the texttrack source file name\n",
    "            annotation_json = annotation_manafest_dict[filename]\n",
    "\n",
    "            # add the attributres form the annotation json to the text track json \n",
    "            texttracts_json['source-ref'] = annotation_json['source-ref']\n",
    "            texttracts_json['label'] = annotation_json['label'] \n",
    "            #print (texttracts_json)\n",
    "            #print ()\n",
    "            \n",
    "            json.dump(texttracts_json, updated_json)\n",
    "            updated_json.write(\"\\n\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6127578f-1f89-4aa0-8cca-3f988026482f",
   "metadata": {},
   "source": [
    "### Split training and test sets\n",
    "\n",
    "To get some insight on how well our model is generalizing to real-world data, we'll need to reserve some annotated data as a testing/validation set.\n",
    "\n",
    "Below, we randomly partition the data into training and test sets and then upload the two manifests to S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db93c29-1f24-40dc-a792-31b983c9c9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO:  Can we build a  \"data/annotations/annotations-all.manifest.jsonl\" with the .json docs?\n",
    "\n",
    "\n",
    "def split_manifest(f_in, f_train, f_test, train_pct=0.9, random_seed=1337):\n",
    "    \"\"\"Split `f_in` manifest file into `f_train`, `f_test`\"\"\"\n",
    "    logger.info(f\"Reading {f_in}\")\n",
    "    with open(f_in, \"r\") as fin:\n",
    "        lines = list(filter(lambda line: line, fin))\n",
    "    logger.info(\"Shuffling records\")\n",
    "    random.Random(random_seed).shuffle(lines)\n",
    "    n_train = round(len(lines) * train_pct)\n",
    "\n",
    "    with open(f_train, \"w\") as ftrain:\n",
    "        logger.info(f\"Writing {n_train} records to {f_train}\")\n",
    "        for line in lines[:n_train]:\n",
    "            \n",
    "            #line = line.replace('source-ref','textract-ref')\n",
    "            #line = line.replace('.jpeg','.json')\n",
    "            #line = line.replace('DynamicTableParser/4_TrainingData/Cycle-1','DynamicTableParser/data/textracted')           \n",
    "            #line = line.replace('DynamicTableParser/4_TrainingData/Cycle-0','DynamicTableParser/data/textracted')           \n",
    "                                   \n",
    "            #print (line)\n",
    "            #print ()\n",
    "            #print ()\n",
    "            \n",
    "            ftrain.write(line)\n",
    "            \n",
    "    with open(f_test, \"w\") as ftest:\n",
    "        logger.info(f\"Writing {len(lines) - n_train} records to {f_test}\")\n",
    "        for line in lines[n_train:]:\n",
    "            templine = line\n",
    "            \n",
    "            #line = line.replace('source-ref','textract-ref')\n",
    "            #line = line.replace('.jpeg','.json')\n",
    "            #line = line.replace('DynamicTableParser/4_TrainingData/Cycle-1','DynamicTableParser/data/textracted')           \n",
    "            #line = line.replace('DynamicTableParser/4_TrainingData/Cycle-0','DynamicTableParser/data/textracted')           \n",
    "            \n",
    "            #print ()\n",
    "            #print (templine)\n",
    "            #print ()\n",
    "            #print (line)\n",
    "            #print ()\n",
    "            #print ()\n",
    "            \n",
    "            ftest.write(line)\n",
    "            \n",
    "\n",
    "\n",
    "split_manifest(\n",
    "   # \"data/annotations/annotations-all.manifest.jsonl\",\n",
    "    \"data/pages-all-sample.manifest.jsonl\",\n",
    "    \"data/annotations/annotations-train.manifest.jsonl\",\n",
    "    \"data/annotations/annotations-test.manifest.jsonl\",\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294ea8f9-214c-4a3e-ada9-92b295f7c9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_manifest_s3uri = f\"s3://{bucket_name}/{bucket_prefix}data/annotations/annotations-train.manifest.jsonl\"\n",
    "!aws s3 cp data/annotations/annotations-train.manifest.jsonl $train_manifest_s3uri\n",
    "\n",
    "print ()\n",
    "test_manifest_s3uri = f\"s3://{bucket_name}/{bucket_prefix}data/annotations/annotations-test.manifest.jsonl\"\n",
    "!aws s3 cp data/annotations/annotations-test.manifest.jsonl $test_manifest_s3uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ff1563-6a77-468e-bea5-94efb4ee62f0",
   "metadata": {},
   "source": [
    "### Visualize the data\n",
    "\n",
    "Before training the model, we'll sense-check the data by plotting a few examples.\n",
    "\n",
    "The utility function below will overlay the page image with the annotated bounding boxes, the locations of `WORD` blocks detected from the Amazon Textract results, and the resulting classification of individual Textract `WORD`s. To render these results, the Amazon Textract OCR results need to be downloaded locally to the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2fbd5d-e416-46c5-9652-066fe5041f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "!aws s3 sync --quiet $textract_s3uri ./data/textracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05876b7-a669-4c21-bc5f-188c40eaf215",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"data/annotations/annotations-test.manifest.jsonl\", \"r\") as fman:\n",
    "    test_examples = [json.loads(line) for line in filter(lambda l: l, fman)]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1008cf47-fd81-495c-8aae-d4a3c84f37a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "with open(\"data/annotations/annotations-test.manifest.jsonl\", \"r\") as fman:\n",
    "    test_examples = [json.loads(line) for line in filter(lambda l: l, fman)]\n",
    "\n",
    "util.viz.draw_from_manifest_items(\n",
    "    test_examples,\n",
    "    standard_label_field,\n",
    "    entity_classes,\n",
    "    imgs_s3uri[len(\"s3://\"):].partition(\"/\")[2],\n",
    "    textract_s3key_prefix=textract_s3uri[len(\"s3://\"):].partition(\"/\")[2],\n",
    "    imgs_local_prefix=\"data/imgs-clean\",\n",
    "    textract_local_prefix=\"data/textracted\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf34f40-949a-470e-9e76-ad29fb15a20e",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Train the entity recognition model\n",
    "\n",
    "We now have all the data needed to train and validate an layout- and page-image-aware entity recognition model in a [SageMaker Training Job](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html).\n",
    "\n",
    "In this process:\n",
    "\n",
    "- SageMaker will run the job on a dedicated, managed instance of type we choose (we'll use `ml.p*` or `ml.g*` GPU-accelerated types), allowing us to keep this notebook's resources modest and only pay for the seconds of GPU time the training job needs.\n",
    "- The data as specified in the manifest files will be downloaded from Amazon S3.\n",
    "- The bundle of scripts we provide (in `src/`) will be transparently uploaded to S3 and then run inside the specified SageMaker-provided [framework container](https://docs.aws.amazon.com/sagemaker/latest/dg/docker-containers-prebuilt.html). There's no need for us to build our own container image or implement a serving stack for inference (although fully-custom containers are [also supported](https://docs.aws.amazon.com/sagemaker/latest/dg/docker-containers.html)).\n",
    "- Job hyperparameters will be passed through to our `src/` scripts as CLI arguments.\n",
    "- SageMaker will analyze the logs from the job (i.e. `print()` or `logger` calls from our script) with the regular expressions specified in `metric_definitions`, to scrape structured timeseries metrics like loss and accuracy.\n",
    "- When the job finishes, the contents of the `model` folder in the container will be automatically tarballed and uploaded to a `model.tar.gz` in Amazon S3.\n",
    "\n",
    "You can also refer to [Hugging Face's own docs for training on SageMaker](https://huggingface.co/transformers/sagemaker.html) for more information and examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6213b0b-22ac-41ce-bbee-be6a2ccefb72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print (sagemaker.get_execution_role() ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de122f49-cf2b-4331-86a4-08ba0c6bfed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace as HuggingFaceEstimator\n",
    "\n",
    "hyperparameters = {\n",
    "    # 2023-05-24\n",
    "    \"model_name_or_path\": \"microsoft/layoutxlm-base\",\n",
    "    #\"model_name_or_path\": \"microsoft/layoutlm-base-uncased\",  \n",
    "    #\"model_name_or_path\": \"microsoft/layoutlmv2-base-uncased\"\n",
    "\n",
    "    # (See src/code/config.py for more info on script parameters)\n",
    "    \"annotation_attr\": standard_label_field,\n",
    "    \"images_prefix\": imgs_s3uri[len(\"s3://\"):].partition(\"/\")[2],\n",
    "    \"textract_prefix\": textract_s3uri[len(\"s3://\"):].partition(\"/\")[2],\n",
    "    \"num_labels\": len(fields) + 1,  # +1 for \"other\"\n",
    "\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"per_device_eval_batch_size\": 4,\n",
    "\n",
    "    \"num_train_epochs\": 20,\n",
    "    \"early_stopping_patience\": 15,\n",
    "    \"metric_for_best_model\": \"eval_focus_else_acc_minus_one\",\n",
    "    \"greater_is_better\": \"true\",\n",
    "\n",
    "    # Early stopping implies checkpointing every evaluation (epoch), so limit the total checkpoints\n",
    "    # kept to avoid filling up disk:\n",
    "    \"save_total_limit\": 10,\n",
    "}\n",
    "\n",
    "metric_definitions = [\n",
    "    {\"Name\": \"epoch\", \"Regex\": util.training.get_hf_metric_regex(\"epoch\")},\n",
    "    {\"Name\": \"learning_rate\", \"Regex\": util.training.get_hf_metric_regex(\"learning_rate\")},\n",
    "    {\"Name\": \"train:loss\", \"Regex\": util.training.get_hf_metric_regex(\"loss\")},\n",
    "    {\n",
    "        \"Name\": \"validation:n_examples\",\n",
    "        \"Regex\": util.training.get_hf_metric_regex(\"eval_n_examples\"),\n",
    "    },\n",
    "    {\"Name\": \"validation:loss_avg\", \"Regex\": util.training.get_hf_metric_regex(\"eval_loss\")},\n",
    "    {\"Name\": \"validation:acc\", \"Regex\": util.training.get_hf_metric_regex(\"eval_acc\")},\n",
    "    {\n",
    "        \"Name\": \"validation:n_focus_examples\",\n",
    "        \"Regex\": util.training.get_hf_metric_regex(\"eval_n_focus_examples\"),\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"validation:focus_acc\",\n",
    "        \"Regex\": util.training.get_hf_metric_regex(\"eval_focus_acc\"),\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"validation:target\",\n",
    "        \"Regex\": util.training.get_hf_metric_regex(\"eval_focus_else_acc_minus_one\"),\n",
    "    },\n",
    "]\n",
    "\n",
    "estimator = HuggingFaceEstimator(\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"src\",\n",
    "    py_version=py_version,\n",
    "    pytorch_version=pt_version,\n",
    "    transformers_version=hf_version,\n",
    "    image_uri=train_image_uri,  # Use customized training container image\n",
    "\n",
    "    base_job_name=\"ws-xlm-cfpb-hf\",\n",
    "    output_path=f\"s3://{bucket_name}/{bucket_prefix}trainjobs\",\n",
    "\n",
    "    instance_type=\"ml.p3.2xlarge\",  # Could also consider ml.g4dn.xlarge\n",
    "    instance_count=1,\n",
    "    volume_size=80,\n",
    "\n",
    "    debugger_hook_config=False,\n",
    "\n",
    "    hyperparameters=hyperparameters,\n",
    "    metric_definitions=metric_definitions,\n",
    "    environment={\n",
    "        # Required for our custom dataset loading code (which depends on tokenizer):\n",
    "        \"TOKENIZERS_PARALLELISM\": \"false\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d9cdff-d997-4008-a340-3eb62beb1280",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print (thumbs_s3uri)\n",
    "print ()\n",
    "print (train_manifest_s3uri)\n",
    "print ()\n",
    "print (textract_s3uri)\n",
    "print ()\n",
    "print (test_manifest_s3uri)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2a23a1-2307-4e13-88c3-02059d64dbd2",
   "metadata": {},
   "source": [
    "Finally, the below cell will actually kick off the training job and stream logs from the running container.\n",
    "\n",
    "> ℹ️ You'll also be able to check the status of the job in the [Training jobs page of the SageMaker Console](https://console.aws.amazon.com/sagemaker/home?#/jobs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e327065b-bb2d-47c4-8920-343cdf8dd181",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    \"images\": thumbs_s3uri,  #2023-05-24 not needed for LayoutLM \n",
    "    \"train\": train_manifest_s3uri,\n",
    "    \"textract\": textract_s3uri + \"/\",\n",
    "    \"validation\": test_manifest_s3uri,\n",
    "}\n",
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f211cd-423d-4267-8432-9f044fcc6082",
   "metadata": {
    "tags": []
   },
   "source": [
    "### One-click model deployment\n",
    "\n",
    "Once the training job is complete, the model can be deployed to an endpoint via `estimator.deploy()` - specifying any extra parameters needed such as environment variables and, in this case, configurations for [Asynchronous Inference](https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference.html). Async inference endpoints in SageMaker can accept larger payloads and auto-scale down to 0 instances when not in use (if configured) - making them a useful option for many document processing use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5a0173-a3c1-486d-b7bc-c610eeb11020",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_name = estimator.latest_training_job.describe()[\"TrainingJobName\"]\n",
    "# Or:\n",
    "# training_job_name = tuner.best_training_job()\n",
    "# ws-xlm-cfpb-hf-2023-05-26-01-32-32-238\n",
    "# ws-xlm-cfpb-hf-2023-05-26-01-32-32-238\n",
    "# ws-xlm-cfpb-hf-2023-05-26-11-15-49-226\n",
    "print (training_job_name)\n",
    "\n",
    "\n",
    "predictor = estimator.deploy(\n",
    "    # Avoid us accidentally deploying the same model twice by setting name per training job:\n",
    "    endpoint_name=training_job_name + \"-11\",\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g4dn.xlarge\",  # Or try ml.m5.2xlarge\n",
    "    image_uri=inf_image_uri,\n",
    "\n",
    "    serializer=sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer=sagemaker.deserializers.JSONDeserializer(),\n",
    "\n",
    "    env={\n",
    "        \"PYTHONUNBUFFERED\": \"1\",  # TODO: Disable once debugging is done\n",
    "        \"MMS_MAX_REQUEST_SIZE\": str(100*1024*1024),  # Accept large payloads (docs)\n",
    "        \"MMS_MAX_RESPONSE_SIZE\": str(100*1024*1024),  # Allow large responses\n",
    "    },\n",
    "\n",
    "    # Deploy in Asynchronous mode, to support large req/res payloads:\n",
    "    async_inference_config=sagemaker.async_inference.AsyncInferenceConfig(\n",
    "        output_path=f\"s3://{config.model_results_bucket}\",\n",
    "        max_concurrent_invocations_per_instance=2,\n",
    "        notification_config={\n",
    "            \"SuccessTopic\": config.model_callback_topic_arn,\n",
    "            \"ErrorTopic\": config.model_callback_topic_arn,\n",
    "        },\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d4661a-fe4a-4e86-b97f-2b47b25a1cc2",
   "metadata": {},
   "source": [
    "If needed (for example, if your kernel crashes or restarts), you can also attach to previously deployed endpoints. Just look up the endpoint name from the SageMaker Console:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb81b22b-ff28-45f2-b391-b6f875045a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# endpoint_name=\"xlm-cfpb-hf-2022-05-23-14-10-19-602\"\n",
    "# predictor = sagemaker.predictor_async.AsyncPredictor(\n",
    "#     sagemaker.Predictor(\n",
    "#         endpoint_name,\n",
    "#         serializer=sagemaker.serializers.JSONSerializer(),\n",
    "#         deserializer=sagemaker.deserializers.JSONDeserializer(),\n",
    "#     ),\n",
    "#     name=endpoint_name,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c90243-db89-40d9-88ec-30e337b03571",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Extract clean input images on-demand\n",
    "\n",
    "Just as we generated page thumbnail images to originally train our model, online inference should be able to generate these input features on-demand. In this example, the same code we previously used in a batch processing job has already been automatically deployed to a SageMaker inference endpoint for you. We can look up the endpoint name from the deployed stack parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51024fa-68c4-4b9a-8ed9-952d4212ff05",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_endpoint_name = ssm.get_parameter(\n",
    "    Name=config.thumbnail_endpoint_name_param,\n",
    ")[\"Parameter\"][\"Value\"]\n",
    "print(f\"Pre-created thumbnailer endpoint name:\\n  {preproc_endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f788bbf2-8535-4f4e-a27b-fe380e70cd2a",
   "metadata": {},
   "source": [
    "The online thumbnail-generation endpoint accepts raw input documents (i.e. PDFs, images), and returns compressed arrays of page image data. From the name of the endpoint, you can configure I/O formats and connect from the notebook as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646e61ea-ff3a-44ae-9035-64e37e048b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    desc = smclient.describe_endpoint(EndpointName=preproc_endpoint_name)\n",
    "except smclient.exceptions.ClientError as e:\n",
    "    if e.response.get(\"Error\", {}).get(\"Message\", \"\").startswith(\"Could not find\"):\n",
    "        desc = None  # Endpoint does not exist\n",
    "    else:\n",
    "        raise e  # Some other unknown issue\n",
    "\n",
    "if desc is None:\n",
    "    raise ValueError(\n",
    "        \"The configured thumbnailing endpoint does not exist in SageMaker. See the 'Optional \"\n",
    "        \"Extras.ipynb' notebook for instructions to manually deploy the thumbnailer before \"\n",
    "        \"continuing. Missing endpoint: %s\" % preproc_endpoint_name\n",
    "    )\n",
    "\n",
    "preproc_predictor = sagemaker.predictor_async.AsyncPredictor(\n",
    "    sagemaker.Predictor(\n",
    "        preproc_endpoint_name,\n",
    "        serializer=util.deployment.FileSerializer.from_filename(\"any.pdf\"),\n",
    "        deserializer=util.deployment.CompressedNumpyDeserializer(),\n",
    "    ),\n",
    "    name=preproc_endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b199be3-eaf8-48b3-9607-3431650aec99",
   "metadata": {},
   "source": [
    "So how would it look to test the endpoint from Python? Let's see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284883db-5305-427e-b2ba-a2f463bf8e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Choose an input (document or image):\n",
    "input_file = \"data/raw/121 Financial Credit Union/Visa Credit Card Agreement.pdf\"\n",
    "#input_file = \"data/imgs-clean/121 Financial Credit Union/Visa Credit Card Agreement-0001-1.png\"\n",
    "\n",
    "# Ensure de/serializers are correctly set up (since depends on input file type):\n",
    "preproc_predictor.serializer = util.deployment.FileSerializer.from_filename(input_file)\n",
    "preproc_predictor.deserializer = util.deployment.CompressedNumpyDeserializer()\n",
    "# Duplication because of https://github.com/aws/sagemaker-python-sdk/issues/3100\n",
    "preproc_predictor.predictor.serializer = preproc_predictor.serializer\n",
    "preproc_predictor.predictor.deserializer = preproc_predictor.deserializer\n",
    "\n",
    "# Run prediction:\n",
    "print(\"Calling endpoint...\")\n",
    "resp = preproc_predictor.predict(input_file)\n",
    "print(f\"Got response of type {type(resp)}\")\n",
    "\n",
    "# Render result:\n",
    "util.viz.draw_thumbnails_response(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d1c312-a55d-4c64-877c-3ebf938a778f",
   "metadata": {},
   "source": [
    "---\n",
    "## Using the entity recognition model\n",
    "\n",
    "Once the deployment is complete and a page thumbnail generator is ready, we're ready to test out inference on some documents!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c845df49-d798-4a92-b6f3-ba3f5f834d95",
   "metadata": {},
   "source": [
    "### Making requests and rendering results\n",
    "\n",
    "At a high level, the layout+language model accepts Textract-like JSON (e.g. as returned by [AnalyzeDocument](https://docs.aws.amazon.com/textract/latest/dg/API_AnalyzeDocument.html#API_AnalyzeDocument_ResponseSyntax) or [DetectDocumentText](https://docs.aws.amazon.com/textract/latest/dg/API_DetectDocumentText.html#API_DetectDocumentText_ResponseSyntax) APIs) and classifies each `WORD` [block](https://docs.aws.amazon.com/textract/latest/dg/API_Block.html) according to the entity classes we defined earlier: Returning the same JSON with additional fields added to indicate the predictions.\n",
    "\n",
    "In addition (per the logic in [src/code/inference.py](src/code/inference.py)):\n",
    "\n",
    "- To incorporate image features (for models that support them), requests can also include an `S3Thumbnails: { Bucket, Key }` object pointing to a thumbnailer endpoint response on S3.\n",
    "- Instead of passing the (typically large and already-S3-resident) Amazon Textract JSON inline, an `S3Input: { Bucket, Key }` reference can be passed instead (and this is actually how the standard pipeline integration works).\n",
    "- Output could also be redirected by passing an `S3Output: { Bucket, Key }` field in the request, but this is ignored and not needed on async endpoint deployments.\n",
    "- `TargetPageNum` and `TargetPageOnly` fields can be specified to limit processing to a single page of the input document.\n",
    "\n",
    "We can use utility functions to render these predictions as we did the manual annotations previously:\n",
    "\n",
    "> ⏰ **Inference may take time in some cases:**\n",
    ">\n",
    "> - Although enabling thumbnails can increase demo inference time below by several seconds, the end-to-end pipeline generates these images in parallel with running Amazon Textract - so there's usually no significant impact in practice.\n",
    "> - If you enabled **auto-scale-to-zero** on your your thumbnailer and/or model endpoint, you may see a cold-start of several minutes.\n",
    "\n",
    "> ⚠️ **Check:** Because of the way the SageMaker Python SDK's [AsyncPredictor](https://sagemaker.readthedocs.io/en/stable/api/inference/predictor_async.html) emulates a synchronous `predict()` interface for async endpoints, you may find the notebook waits indefinitely instead of raising an error when something goes wrong. If an inference takes more than ~30s to complete, check the endpoint logs from your [SageMaker Console Endpoints page](https://console.aws.amazon.com/sagemaker/home?#/endpoints) to see if your request resulted in an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16db0e3-2c66-4032-8784-ccf7fdb5e255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import trp\n",
    "\n",
    "# Enabling thumbnails can significantly increase inference time here, but can improve results for\n",
    "# models that consume image features (like LayoutLMv2, XLM):\n",
    "include_thumbnails = False\n",
    "\n",
    "def predict_from_manifest_item(\n",
    "    item,\n",
    "    predictor,\n",
    "    imgs_s3key_prefix=imgs_s3uri[len(\"s3://\"):].partition(\"/\")[2],\n",
    "    raw_s3uri_prefix=raw_s3uri,\n",
    "    textract_s3key_prefix=textract_s3uri[len(\"s3://\"):].partition(\"/\")[2],\n",
    "    imgs_local_prefix=\"data/imgs-clean\",\n",
    "    textract_local_prefix=\"data/textracted\",\n",
    "    draw=True,\n",
    "):\n",
    "    paths = util.viz.local_paths_from_manifest_item(\n",
    "        item,\n",
    "        imgs_s3key_prefix,\n",
    "        textract_s3key_prefix=textract_s3key_prefix,\n",
    "        imgs_local_prefix=imgs_local_prefix,\n",
    "        textract_local_prefix=textract_local_prefix,\n",
    "    )\n",
    "\n",
    "    if include_thumbnails:\n",
    "        doc_textract_s3key = item[\"textract-ref\"][len(\"s3://\"):].partition(\"/\")[2]\n",
    "        doc_raw_s3uri = raw_s3uri_prefix + doc_textract_s3key[len(textract_s3key_prefix):].rpartition(\"/\")[0]\n",
    "        print(f\"Fetching thumbnails for {doc_raw_s3uri}\")\n",
    "        thumbs_async = preproc_predictor.predict_async(input_path=doc_raw_s3uri)\n",
    "        thumbs_bucket, _, thumbs_key = thumbs_async.output_path[len(\"s3://\"):].partition(\"/\")\n",
    "        # Wait for the request to complete:\n",
    "        thumbs_async.get_result(sagemaker.async_inference.WaiterConfig())\n",
    "        req_extras = {\"S3Thumbnails\": {\"Bucket\": thumbs_bucket, \"Key\": thumbs_key}}\n",
    "        print(\"Got thumbnails result\")\n",
    "    else:\n",
    "        req_extras = {}\n",
    "\n",
    "    result_json = predictor.predict({\n",
    "        \"S3Input\": {\"S3Uri\": item[\"textract-ref\"]},\n",
    "        \"TargetPageNum\": item[\"page-num\"],\n",
    "        \"TargetPageOnly\": True,\n",
    "        **req_extras,\n",
    "    })\n",
    "\n",
    "    if \"Warnings\" in result_json:\n",
    "        for warning in result_json[\"Warnings\"]:\n",
    "            logger.warning(warning)\n",
    "    result_trp = trp.Document(result_json)\n",
    "\n",
    "    if draw:\n",
    "        util.viz.draw_smgt_annotated_page(\n",
    "            paths[\"image\"],\n",
    "            entity_classes,\n",
    "            annotations=[],\n",
    "            textract_result=result_trp,\n",
    "            # Note that page_num should be item[\"page-num\"] if we requested the full set of pages\n",
    "            # from the model above:\n",
    "            page_num=1,\n",
    "        )\n",
    "    return result_trp\n",
    "\n",
    "\n",
    "widgets.interact(\n",
    "    lambda ix: predict_from_manifest_item(test_examples[ix], predictor),\n",
    "    ix=widgets.IntSlider(\n",
    "        min=0,\n",
    "        max=len(test_examples) - 1,\n",
    "        step=1,\n",
    "        value=0,\n",
    "        description=\"Example:\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6ddf5a-b833-4750-b953-acb8a0ecb741",
   "metadata": {},
   "source": [
    "### From token classification to entity detection\n",
    "\n",
    "You may have noticed a slight mismatch: We're talking about extracting 'fields' or 'entities' from the document, but our model just classifies individual words. Going from words to entities assumes we're able to understand which words go \"together\" and what order they should be read in.\n",
    "\n",
    "Fortunately, Amazon Textract helps us out with this too as the word blocks are already collected into `LINE`s.\n",
    "\n",
    "For many straightforward applications, we can simply loop through the lines on a page and define an \"entity detection\" as a contiguous group of the same class - as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040ad61c-9c32-49a5-b07b-acaf9585d7c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print (test_examples[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cea40eb-e46e-49d5-902a-f5b217f2ea31",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = predict_from_manifest_item(\n",
    "    test_examples[5],\n",
    "    predictor,\n",
    "    draw=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fd38ccf7-b594-4621-8c03-86e56df1a379",
   "metadata": {},
   "source": [
    "print (res) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49081201-35e1-4ac9-9fd6-04ab23f69966",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "other_cls = len(entity_classes)\n",
    "prev_cls = other_cls\n",
    "current_entity = \"\"\n",
    "\n",
    "for page in res.pages:\n",
    "    for line in page.lines:\n",
    "        for word in line.words:\n",
    "            pred_cls = word._block[\"PredictedClass\"]\n",
    "            if pred_cls != prev_cls:\n",
    "                if prev_cls != other_cls:\n",
    "                    print(f\"----------\\n{entity_classes[prev_cls]}:\\n{current_entity}\")\n",
    "                prev_cls = pred_cls\n",
    "                if pred_cls != other_cls:\n",
    "                    current_entity = word.text\n",
    "                else:\n",
    "                    current_entity = \"\"\n",
    "                continue\n",
    "            current_entity = \" \".join((current_entity, word.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4bd908-4a5c-437c-90e4-895c572745f0",
   "metadata": {},
   "source": [
    "Of course there may be some instances where this heuristic breaks down, but we still have access to all the position (and text) information from each `LINE` and `WORD` to write additional rules for reading order and separation if wanted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93670571-10cf-489b-8b97-cb5a20d8401f",
   "metadata": {},
   "source": [
    "---\n",
    "## Setting up the end-to-end pipeline\n",
    "\n",
    "### Integrating the entity detection model\n",
    "\n",
    "So far we've demonstrated running entity detection requests from here in the notebook, but how can this model be integrated into the end-to-end document processing pipeline stack?\n",
    "\n",
    "First, you'll identify the **endpoint name** of your deployed model and the **AWS Systems Manager Parameter** that configures the SageMaker endpoint parameter for the pipeline stack:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a21365-586a-4b83-8a7f-9a0b4cee0f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Endpoint name:\\n  {predictor.endpoint_name}\")\n",
    "print(f\"\\nEndpoint SSM param:\\n  {config.sagemaker_endpoint_name_param}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46912d03-8748-485b-9d50-841f154a670e",
   "metadata": {},
   "source": [
    "Finally, we'll update this SSM parameter to point to the deployed SageMaker endpoint.\n",
    "\n",
    "The below code should do this for you automatically:\n",
    "\n",
    "> ⚠️ **Note:** The [Lambda function](../pipeline/enrichment/fn-call-sagemaker/main.py) that calls your model from the OCR pipeline caches the endpoint name for a few minutes (`CACHE_TTL_SECONDS`) to reduce unnecessary ssm:GetParameter calls - so it may take a little time for an update here to take effect if you already processed a document recently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a6d62f-47bd-42ea-a74f-fed32fcd8f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_endpoint_name = predictor.endpoint_name\n",
    "\n",
    "print(f\"Configuring pipeline with model: {pipeline_endpoint_name}\")\n",
    "\n",
    "ssm.put_parameter(\n",
    "    Name=config.sagemaker_endpoint_name_param,\n",
    "    Overwrite=True,\n",
    "    Value=pipeline_endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32572c6-4fdc-4ade-8801-bfe8f76934c2",
   "metadata": {},
   "source": [
    "Alternatively, you could open the [AWS Systems Manager Parameter Store console](https://console.aws.amazon.com/systems-manager/parameters/?tab=Table) and click on the *name* of the parameter to open its detail page, then the **Edit** button in the top right corner as shown below:\n",
    "\n",
    "![](img/ssm-param-detail-screenshot.png \"Screenshot of SSM parameter detail page showing Edit button\")\n",
    "\n",
    "From this screen you can manually set the **Value** of the parameter and save the changes.\n",
    "\n",
    "Whether you updated the SSM parameters via code or the console, your the pre-processing and enrichment stages of your stack should now be configured to use your endpoints!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81811e20-7b0d-474d-9602-d260db900a86",
   "metadata": {},
   "source": [
    "### Updating the pipeline entity definitions\n",
    "\n",
    "As well as configuring the *enrichment* stage of the pipeline to reference the deployed version of the model, we need to configure the *post-processing* stage to match the model's **definition of entity/field types**.\n",
    "\n",
    "The entity configuration is as we saved in the previous notebook, but the `annotation_guidance` attributes are not needed:\n",
    "\n",
    "> ℹ️ **Note:** As well as the mapping from ID numbers (returned by the model) to human-readable class names, this configuration controls how the pipeline consolidates entity matches into \"fields\" of the document: E.g. choosing the \"most likely\" or \"first\" value between multiple detections, or setting up a multi-value field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4688cdf8-e448-45b5-ad5d-c0dbe5a46b04",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_entity_config = json.dumps([f.to_dict(omit=[\"annotation_guidance\"]) for f in fields], indent=2)\n",
    "print(pipeline_entity_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1b8289-1bab-414f-aadf-c50fb3d36464",
   "metadata": {},
   "source": [
    "As above, you *could* set this value manually in the SSM console for the parameter named as `EntityConfig`.\n",
    "\n",
    "...But we can make the same update via code through the APIs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bca521-6b34-47c9-9062-8d681d9ae817",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Setting pipeline entity configuration\")\n",
    "ssm.put_parameter(\n",
    "    Name=config.entity_config_param,\n",
    "    Overwrite=True,\n",
    "    Value=pipeline_entity_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11386c0a-ad81-4a20-93a8-3f89cdf8388c",
   "metadata": {},
   "source": [
    "### Set up online review with Amazon Augmented AI (A2I)\n",
    "\n",
    "Whereas our original batch annotation used the [built-in](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-task-types.html) image bounding box / object detection task UI, a custom task template is provided for online review.\n",
    "\n",
    "Since the template is built using a web framework (VueJS), we'll need to install some extra dependencies to enable building it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc459c88-e10b-46d5-82c3-24d74cc3dd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd review && npm install"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a638c64f-79a9-413f-8887-220b50da9526",
   "metadata": {},
   "source": [
    "Then, build the UI HTML template from source:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5b333b-a887-4f05-a87c-c605ca4fd8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd review && npm run build\n",
    "ui_template_file = \"review/dist/index.html\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ece1831-28dd-43f1-8077-91b064b6e531",
   "metadata": {},
   "source": [
    "Next, upload the built file as an A2I human review task UI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d403d4-08cb-4f4f-aefc-47fbfba80412",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ui_template_file, \"r\") as f:\n",
    "    create_template_resp = smclient.create_human_task_ui(\n",
    "        HumanTaskUiName=\"fields-validation-1\",  # (Can change this name as you like)\n",
    "        UiTemplate={\"Content\": f.read()},\n",
    "    )\n",
    "\n",
    "task_template_arn = create_template_resp[\"HumanTaskUiArn\"]\n",
    "print(f\"Created A2I task template:\\n{task_template_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933e1637-e04a-4078-9079-beb56f8e5f17",
   "metadata": {},
   "source": [
    "We already defined a \"team\" for tasks to be routed to above, for SageMaker Ground Truth, and can re-use that team for the online review flow.\n",
    "\n",
    "To finish setting up the workflow itself, we need 2 more pieces of information:\n",
    "\n",
    "- The **location in S3** where review outputs should be stored\n",
    "- An appropriate **execution role** which will give the A2I workflow to read input documents and write review results.\n",
    "\n",
    "These are determined by the **OCR pipeline solution stack**, because the reviews bucket is created by the pipeline with event triggers to resume the next stage when reviews are uploaded.\n",
    "\n",
    "The code below should be able to look up these parameters for you automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6f7c1f-d6d5-4d84-ba68-8bf9b86b812f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_bucket_name = config.pipeline_reviews_bucket_name\n",
    "print(reviews_bucket_name)\n",
    "reviews_role_arn = config.a2i_execution_role_arn\n",
    "print(reviews_role_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb724957-a783-48fb-85d3-39b16937e877",
   "metadata": {},
   "source": [
    "Alternatively, you may **find** your pipeline solution stack from the [AWS CloudFormation Console](https://console.aws.amazon.com/cloudformation/home?#/stacks) and click through to the stack detail page. From the **Outputs** tab, you should see the `A2IHumanReviewBucketName` and `A2IHumanReviewExecutionRoleArn` values as shown below.\n",
    "\n",
    "(You may also note the `A2IHumanReviewFlowParamName`, which we'll use in the next section)\n",
    "\n",
    "![](img/cfn-stack-outputs-a2i.png \"CloudFormation stack outputs for OCR pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e05cd6d-314b-435b-9994-8ec9bef79d4a",
   "metadata": {},
   "source": [
    "Once these values are populated, you're ready to create your review workflow by running the code below.\n",
    "\n",
    "Note that you can also manage flows via the [A2I Human Review Workflows Console](https://console.aws.amazon.com/a2i/home?#/human-review-workflows/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1f6192-99cb-4c4e-95da-656c64c3bebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_flow_resp = smclient.create_flow_definition(\n",
    "    FlowDefinitionName=\"ocr-fields-validation-1\",  # (Can change this name as you like)\n",
    "    HumanLoopConfig={\n",
    "        \"WorkteamArn\": workteam_arn,\n",
    "        \"HumanTaskUiArn\": task_template_arn,\n",
    "        \"TaskTitle\": \"Review OCR Field Extractions\",\n",
    "        \"TaskDescription\": \"Review and correct credit card agreement field extractions\",\n",
    "        \"TaskCount\": 1,  # One reviewer per item\n",
    "        \"TaskAvailabilityLifetimeInSeconds\": 60 * 60,  # Availability timeout\n",
    "        \"TaskTimeLimitInSeconds\": 60 * 60,  # Working timeout\n",
    "    },\n",
    "    OutputConfig={\n",
    "        \"S3OutputPath\": f\"s3://{reviews_bucket_name}/reviews\",\n",
    "    },\n",
    "    RoleArn=reviews_role_arn,\n",
    ")\n",
    "\n",
    "print(f\"Created review workflow:\\n{create_flow_resp['FlowDefinitionArn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed48057-1311-4609-839a-7cb5abf0b294",
   "metadata": {},
   "source": [
    "Finally, when the human review flow is created and registered, we can configure the document pipeline to use it - similarly to our SageMaker endpoint and entity configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb62b35-7fe9-4b9a-b6fe-8dd1e2bf6c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Configuring pipeline with review workflow: {create_flow_resp['FlowDefinitionArn']}\")\n",
    "\n",
    "ssm = boto3.client(\"ssm\")\n",
    "ssm.put_parameter(\n",
    "    Name=config.a2i_review_flow_arn_param,\n",
    "    Overwrite=True,\n",
    "    Value=create_flow_resp[\"FlowDefinitionArn\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087ba0fc-1470-4d51-ba47-ac3ced075e91",
   "metadata": {},
   "source": [
    "Alternatively through the console, you would follow these steps:\n",
    "\n",
    "▶️ **Check** the `A2IHumanReviewFlowParamName` output of your OCR pipeline stack in [CloudFormation](https://console.aws.amazon.com/cloudformation/home?#/stacks) (as we did above)\n",
    "\n",
    "▶️ **Open** the [AWS Systems Manager Parameter Store console](https://console.aws.amazon.com/systems-manager/parameters/?tab=Table) and **find the review flow parameter in the list**.\n",
    "\n",
    "▶️ **Click** on the name of the parameter to open its detail page, and then on the **Edit** button in the top right corner. Set the value to the **workflow ARN** (see previous code cell in this notebook) and save the changes.\n",
    "\n",
    "![](img/ssm-a2i-param-detail.png \"Screenshot of SSM parameter detail page for human workflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0768a157-9115-4261-aeae-6827bf7189f2",
   "metadata": {},
   "source": [
    "---\n",
    "## Final testing\n",
    "\n",
    "Your OCR pipeline should now be fully functional! Let's try it out:\n",
    "\n",
    "▶️ **Log in** to the labelling portal (URL available from the [SageMaker Ground Truth Workforces Console](https://console.aws.amazon.com/sagemaker/groundtruth?#/labeling-workforces) for your correct AWS Region)\n",
    "\n",
    "![](img/smgt-find-workforce-url.png \"Screenshot of SMGT console with workforce login URL\")\n",
    "\n",
    "▶️ **Upload** one of the sample documents to your pipeline's input bucket in Amazon S3, either using the code snippets below or drag and drop in the [Amazon S3 Console](https://console.aws.amazon.com/s3/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f202de-d041-4e94-af90-d80df1fe84f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfpaths = []\n",
    "for currpath, dirs, files in os.walk(\"data/raw\"):\n",
    "    if \"/.\" in currpath or \"__\" in currpath:\n",
    "        continue\n",
    "    pdfpaths += [\n",
    "        os.path.join(currpath, f) for f in files\n",
    "        if f.lower().endswith(\".pdf\")\n",
    "    ]\n",
    "pdfpaths = sorted(pdfpaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23811e21-bd19-45b1-ae92-a385f1981bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filepath = pdfpaths[14]\n",
    "test_s3uri = f\"s3://{config.pipeline_input_bucket_name}/{test_filepath}\"\n",
    "\n",
    "!aws s3 cp '{test_filepath}' '{test_s3uri}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68794a23-e2ec-49df-8d8b-7b5a8c375c2a",
   "metadata": {},
   "source": [
    "▶️ **Open up** your \"Processing Pipeline\" state machine in the [AWS Step Functions Console](https://console.aws.amazon.com/states/home?#/statemachines)\n",
    "\n",
    "After a few seconds you should find that a Step Function execution is automatically triggered and (since we enabled so many fields that at least one is always missing) the example is eventually forwarded for human review in A2I.\n",
    "\n",
    "As you'll see from the `ModelResult` field in your final *Step Output*, this pipeline produces a rich but usefully-structured output - with good opportunities for onward integration into further Step Functions steps or external systems. You can find more information and sample solutions for integrating AWS Step Functions in the [Step Functions Developer Guide](https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html).\n",
    "\n",
    "![](img/sfn-statemachine-success.png \"Screenshot of successful Step Function execution with output JSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bede76bf-3660-4418-82e3-ec303cb2a5a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this worked example we showed how advanced, open-source language processing models specifically tailored for document understanding can be integrated with [Amazon Textract](https://aws.amazon.com/textract/): providing a trainable, ML-driven framework for tackling more niche or complex requirements where Textract's [built-in structure extraction tools](https://aws.amazon.com/textract/features/) may not fully solve the challenges out-of-the-box.\n",
    "\n",
    "The underlying principle of the model - augmenting multi-task neural text processing architectures with positional data - is highly extensible, with potential to tackle a wide range of use cases where joint understanding of the content and presentation of text can deliver better results than considering text alone.\n",
    "\n",
    "We demonstrated how an end-to-end process automation pipeline applying this technology might look: Developing and deploying the model with [Amazon SageMaker](https://aws.amazon.com/sagemaker/), building a serverless workflow with [AWS Step Functions](https://aws.amazon.com/step-functions/) and [AWS Lambda](https://aws.amazon.com/lambda/), and driving quality with human review of low-confidence documents through [Amazon Augmented AI](https://aws.amazon.com/augmented-ai/).\n",
    "\n",
    "Thanks for following along, and for more information, don't forget to check out:\n",
    "\n",
    "- The other published [Amazon Textract Examples](https://docs.aws.amazon.com/textract/latest/dg/other-examples.html) listed in the [Textract Developer Guide](https://docs.aws.amazon.com/textract/latest/dg/what-is.html)\n",
    "- The extensive repository of [Amazon SageMaker Examples](https://github.com/aws/amazon-sagemaker-examples) and usage documentation in the [SageMaker Python SDK User Guide](https://sagemaker.readthedocs.io/en/stable/) - as well as the [SageMaker Developer Guide](https://docs.aws.amazon.com/sagemaker/index.html)\n",
    "- The wide range of other open algorithms and models published by [HuggingFace Transformers](https://huggingface.co/transformers/), and their specific documentation on [using the library with SageMaker](https://huggingface.co/transformers/sagemaker.html)\n",
    "- The conversational AI and NLP area (and others) of Amazon's own [Amazon.Science](https://www.amazon.science/conversational-ai-natural-language-processing) blog\n",
    "\n",
    "Happy building!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7671b24f-b77e-4ae4-9a7d-b8cc9259d748",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# shut down end point after use so we don't incur needless charges. \n",
    "predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3224dead-aae3-41fa-9d2c-91d66a2255c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
