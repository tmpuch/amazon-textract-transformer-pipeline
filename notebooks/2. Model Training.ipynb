{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Post-Processing Amazon Textract with Location-Aware Transformers**\n",
    "\n",
    "# Part 2: Data Consolidation and Model Training/Deployment\n",
    "\n",
    "> *This notebook works well with the `Data Science 3.0 (Python 3)` kernel on SageMaker Studio - use the same as for NB1*\n",
    "\n",
    "In the [first notebook](1.%20Data%20Preparation.ipynb) we worked through preparing a corpus with Amazon Textract and labelling a small sample to highlight entities of interest.\n",
    "\n",
    "In this part 2, we'll consolidate the labelling job results together with a pre-prepared augmentation set, and actually train and deploy a SageMaker model for word classification.\n",
    "\n",
    "First, as in the previous notebook, we'll start by importing the required libraries and loading configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Working in bucket s3://sagemaker-us-east-1-015943506230/textract-transformers/\n",
      "<util.project.ProjectSession(\n",
      "  project_id=ocr-transformers-demo,\n",
      "  a2i_review_flow_arn_param=/ocr-transformers-demo/config/HumanReviewFlowArn,\n",
      "  entity_config_param=/ocr-transformers-demo/config/EntityConfiguration,\n",
      "  sagemaker_endpoint_name_param=/ocr-transformers-demo/config/SageMakerEndpointName,\n",
      "  thumbnail_endpoint_name_param=/ocr-transformers-demo/config/ThumbnailEndpointName,\n",
      "  a2i_execution_role_arn=arn:aws:iam::015943506230:role/OCRPipelineDemo-ProcessingPipelineReviewStepProces-M6UD1K31HQ0Q,\n",
      "  pipeline_input_bucket_name=ocrpipelinedemo-pipelineinputbucket350ea1ae-1ffox60d5xk2z,\n",
      "  model_callback_topic_arn=arn:aws:sns:us-east-1:015943506230:OCRPipelineDemo-ProcessingPipelineEnrichmentStepNLPEnrichmentModelSageMakerAsyncNLPEnrichmentModelE62625A1-WxGfSPiKRll3,\n",
      "  model_results_bucket=ocrpipelinedemo-processingpipelineenrichedresults-bt6ehtmayta3,\n",
      "  pipeline_sfn_arn=arn:aws:states:us-east-1:015943506230:stateMachine:ProcessingPipelinePipelineStateMachineC698BCB6-OfF7h2R7eWfO,\n",
      "  plain_textract_sfn_arn=arn:aws:states:us-east-1:015943506230:stateMachine:ProcessingPipelineOCRStepTextractStepTextractStateMachineFA5B3847-JnTeNju1dIbg,\n",
      "  preproc_image_uri=015943506230.dkr.ecr.us-east-1.amazonaws.com/sm-ocr-preprocs:pytorch-1.10-inf-cpu,\n",
      "  pipeline_reviews_bucket_name=ocrpipelinedemo-processingpipelinehumanreviewsbuc-v3nqj65r0rt9,\n",
      "  sm_image_build_role=OCRPipelineDemo-AnnotationInfraSMImageBuildRole8DB-93NPCEWTHQPR,\n",
      "  thumbnails_callback_topic_arn=arn:aws:sns:us-east-1:015943506230:OCRPipelineDemo-ProcessingPipelineThumbnailStepSageMakerAsyncThumbnailStepD663EB88-T83wFi71Nvmf\n",
      ") at 0x7f6e08a3b6a0>\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Python Built-Ins:\n",
    "from datetime import datetime\n",
    "import json\n",
    "from logging import getLogger\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3  # AWS SDK for Python\n",
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFace as HuggingFaceEstimator, TrainingCompilerConfig\n",
    "from tqdm.notebook import tqdm  # Progress bars\n",
    "\n",
    "# Local Dependencies:\n",
    "import util\n",
    "\n",
    "logger = getLogger()\n",
    "\n",
    "# Manual configuration (check this matches notebook 1):\n",
    "bucket_name = sagemaker.Session().default_bucket()\n",
    "bucket_prefix = \"textract-transformers/\"\n",
    "print(f\"Working in bucket s3://{bucket_name}/{bucket_prefix}\")\n",
    "config = util.project.init(\"ocr-transformers-demo\")\n",
    "print(config)\n",
    "\n",
    "\n",
    "\n",
    "# S3 URIs as per first notebook:\n",
    "raw_s3uri = f\"s3://{bucket_name}/{bucket_prefix}data/raw\"\n",
    "imgs_s3uri = f\"s3://{bucket_name}/{bucket_prefix}data/imgs-clean\"\n",
    "textract_s3uri = f\"s3://{bucket_name}/{bucket_prefix}data/textracted\"\n",
    "thumbs_s3uri = f\"s3://{bucket_name}/{bucket_prefix}data/thumbnails\"\n",
    "annotations_base_s3uri = f\"s3://{bucket_name}/{bucket_prefix}data/annotations\"\n",
    "\n",
    "# AWS service clients:\n",
    "s3 = boto3.resource(\"s3\")\n",
    "smclient = boto3.client(\"sagemaker\")\n",
    "ssm = boto3.client(\"ssm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Local Dependencies:\n",
    "import util.postproc.config as upc\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Field configuration saved from first notebook:\n",
    "with open(\"data/field-config.json\", \"r\") as f:\n",
    "    fields = [\n",
    "        upc.FieldConfiguration.from_dict(cfg)\n",
    "        for cfg in json.loads(f.read())\n",
    "    ]\n",
    "entity_classes = [f.name for f in fields]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Consolidation\n",
    "\n",
    "To construct a training set, we'll typically need to consolidate the results of multiple SageMaker Ground Truth labelling jobs: Perhaps because the work was split up into more manageable chunks - or maybe because additional review/adjustment jobs were run to improve label quality.\n",
    "\n",
    "First, we'll download the output folders of all our labelling jobs to the local `data/annotations` folder: (The code here assumes you configured the same `annotations_base_s3uri` output folder for each job in SMGT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 sync --quiet $annotations_base_s3uri ./data/annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside this folder, you'll find some **pre-annotated augmentation data** provided for you already (in the `augmentation-` subfolders). These datasets are not especially large or externally useful, but will help you train an example model without too much (or even any!) manual annotation effort.\n",
    "\n",
    "▶️ **Edit** the `include_jobs` line below to control which datasets (pre-provided and your own) will be included:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-22 12:09:46,354 root [WARNING] Skipping ocr-fpb-boxes-1 (not in include_jobs list)\n",
      "2023-05-22 12:09:46,354 root [WARNING] Skipping ocr-fpb-boxes-2 (not in include_jobs list)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 2 annotated manifests:\n",
      "data/annotations/augmentation-1/manifests/output/output.manifest\n",
      "data/annotations/augmentation-2/manifests/output/output.manifest\n"
     ]
    }
   ],
   "source": [
    "include_jobs = [\n",
    "    \"augmentation-1\",\n",
    "    \"augmentation-2\",\n",
    "    # TODO: Adjust the below to include the labelling job(s) you created, if you finished labelling:\n",
    "    # \"cfpb-boxes-1\",\n",
    "]\n",
    "\n",
    "\n",
    "source_manifests = []\n",
    "for job_name in sorted(filter(\n",
    "    lambda n: os.path.isdir(f\"data/annotations/{n}\"),\n",
    "    os.listdir(\"data/annotations\")\n",
    ")):\n",
    "    if job_name not in include_jobs:\n",
    "        logger.warning(f\"Skipping {job_name} (not in include_jobs list)\")\n",
    "        continue\n",
    "    job_manifest_path = f\"data/annotations/{job_name}/manifests/output/output.manifest\"\n",
    "    if not os.path.isfile(job_manifest_path):\n",
    "        raise RuntimeError(f\"Could not find job output manifest {job_manifest_path}\")\n",
    "    source_manifests.append({\"job_name\": job_name, \"manifest_path\": job_manifest_path})\n",
    "\n",
    "print(f\"Got {len(source_manifests)} annotated manifests:\")\n",
    "print(\"\\n\".join(map(lambda o: o[\"manifest_path\"], source_manifests)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the results are downloaded, we're ready to **consolidate the output manifest files** from each one into a combined manifest file.\n",
    "\n",
    "Note that to combine multiple output manifests to a single dataset:\n",
    "\n",
    "- The labels must be stored in the same attribute on every record (records use the labeling job name by default, which will be different between jobs).\n",
    "- If importing data collected from some other account (like the `augmentation-` sets), we'll need to **map the S3 URIs** to equivalent links on your own bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data/annotations/annotations-all.manifest.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f55053007b6498f94d20f46b854a4f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Consolidating manifests...:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Annotations/labels will be standardized to this field on all records:\n",
    "standard_label_field = \"label\"\n",
    "\n",
    "# To import a manifest from somebody else, we of course need to map their bucket names and prefixes\n",
    "# to ours (and have equivalent files stored in the same locations after the mapping):\n",
    "BUCKET_MAPPINGS = {\"DOC-EXAMPLE-BUCKET\": bucket_name}\n",
    "PREFIX_MAPPINGS = {\"EXAMPLE-PREFIX/\": bucket_prefix}\n",
    "\n",
    "print(\"Writing data/annotations/annotations-all.manifest.jsonl\")\n",
    "with open(\"data/annotations/annotations-all.manifest.jsonl\", \"w\") as fout:\n",
    "    for source in tqdm(source_manifests, desc=\"Consolidating manifests...\"):\n",
    "        with open(source[\"manifest_path\"], \"r\") as fin:\n",
    "            for line in filter(lambda l: l, fin):\n",
    "                obj = json.loads(line)\n",
    "\n",
    "                # Import refs by applying BUCKET_MAPPINGS and PREFIX_MAPPINGS:\n",
    "                for k in filter(lambda k: k.endswith(\"-ref\"), obj.keys()):\n",
    "                    if not obj[k].lower().startswith(\"s3://\"):\n",
    "                        raise RuntimeError(\n",
    "                            \"Attr %s ends with -ref but does not start with 's3://'\\n%s\"\n",
    "                            % (k, obj)\n",
    "                        )\n",
    "                    obj_bucket, _, obj_key = obj[k][len(\"s3://\"):].partition(\"/\")\n",
    "                    obj_bucket = BUCKET_MAPPINGS.get(obj_bucket, obj_bucket)\n",
    "                    for old_prefix in PREFIX_MAPPINGS:\n",
    "                        if obj_key.startswith(old_prefix):\n",
    "                            obj_key = (\n",
    "                                PREFIX_MAPPINGS[old_prefix]\n",
    "                                + obj_key[len(old_prefix):]\n",
    "                            )\n",
    "                    obj[k] = f\"s3://{obj_bucket}/{obj_key}\"\n",
    "\n",
    "                # Find the job output field:\n",
    "                if source[\"job_name\"] in obj:\n",
    "                    source_label_attr = source[\"job_name\"]\n",
    "                elif standard_label_field in obj:\n",
    "                    source_label_attr = standard_label_field\n",
    "                else:\n",
    "                    raise RuntimeError(\"Couldn't find label field for entry in {}:\\n{}\".format(\n",
    "                        source[\"job_name\"],\n",
    "                        obj,\n",
    "                    ))\n",
    "                # Rename to standard:\n",
    "                obj[standard_label_field] = obj.pop(source_label_attr)\n",
    "                source_meta_attr = f\"{source_label_attr}-metadata\"\n",
    "                if source_meta_attr in obj:\n",
    "                    obj[f\"{standard_label_field}-metadata\"] = obj.pop(source_meta_attr)\n",
    "                # Write to output manifest:\n",
    "                fout.write(json.dumps(obj) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split training and test sets\n",
    "\n",
    "To get some insight on how well our model is generalizing to real-world data, we'll need to reserve some annotated data as a testing/validation set.\n",
    "\n",
    "Below, we randomly partition the data into training and test sets and then upload the two manifests to S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-22 12:09:49,108 root [INFO] Reading data/annotations/annotations-all.manifest.jsonl\n",
      "2023-05-22 12:09:49,118 root [INFO] Shuffling records\n",
      "2023-05-22 12:09:49,149 root [INFO] Writing 90 records to data/annotations/annotations-train.manifest.jsonl\n",
      "2023-05-22 12:09:49,194 root [INFO] Writing 10 records to data/annotations/annotations-test.manifest.jsonl\n"
     ]
    }
   ],
   "source": [
    "def split_manifest(f_in, f_train, f_test, train_pct=0.9, random_seed=1337):\n",
    "    \"\"\"Split `f_in` manifest file into `f_train`, `f_test`\"\"\"\n",
    "    logger.info(f\"Reading {f_in}\")\n",
    "    with open(f_in, \"r\") as fin:\n",
    "        lines = list(filter(lambda line: line, fin))\n",
    "    logger.info(\"Shuffling records\")\n",
    "    random.Random(random_seed).shuffle(lines)\n",
    "    n_train = round(len(lines) * train_pct)\n",
    "\n",
    "    with open(f_train, \"w\") as ftrain:\n",
    "        logger.info(f\"Writing {n_train} records to {f_train}\")\n",
    "        for line in lines[:n_train]:\n",
    "            ftrain.write(line)\n",
    "    with open(f_test, \"w\") as ftest:\n",
    "        logger.info(f\"Writing {len(lines) - n_train} records to {f_test}\")\n",
    "        for line in lines[n_train:]:\n",
    "            ftest.write(line)\n",
    "\n",
    "\n",
    "split_manifest(\n",
    "    \"data/annotations/annotations-all.manifest.jsonl\",\n",
    "    \"data/annotations/annotations-train.manifest.jsonl\",\n",
    "    \"data/annotations/annotations-test.manifest.jsonl\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: data/annotations/annotations-train.manifest.jsonl to s3://sagemaker-us-east-1-015943506230/textract-transformers/data/annotations/annotations-train.manifest.jsonl\n",
      "upload: data/annotations/annotations-test.manifest.jsonl to s3://sagemaker-us-east-1-015943506230/textract-transformers/data/annotations/annotations-test.manifest.jsonl\n"
     ]
    }
   ],
   "source": [
    "train_manifest_s3uri = f\"s3://{bucket_name}/{bucket_prefix}data/annotations/annotations-train.manifest.jsonl\"\n",
    "!aws s3 cp data/annotations/annotations-train.manifest.jsonl $train_manifest_s3uri\n",
    "\n",
    "test_manifest_s3uri = f\"s3://{bucket_name}/{bucket_prefix}data/annotations/annotations-test.manifest.jsonl\"\n",
    "!aws s3 cp data/annotations/annotations-test.manifest.jsonl $test_manifest_s3uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the data\n",
    "\n",
    "Before training the model, we'll sense-check the data by plotting a few examples.\n",
    "\n",
    "The utility function below will overlay the page image with the annotated bounding boxes, the locations of `WORD` blocks detected from the Amazon Textract results, and the resulting classification of individual Textract `WORD`s.\n",
    "\n",
    "> ⏰ If you Textracted a large number of documents and haven't previously synced them to the notebook, the initial download here may take a few minutes to complete. For our sample set of 120, typically only ~20s is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.1 ms, sys: 22.5 ms, total: 60.6 ms\n",
      "Wall time: 3.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!aws s3 sync --quiet $textract_s3uri ./data/textracted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ⚠️ **Note:** For the interactive visualization widgets in this notebook to work correctly, you'll need the [IPyWidgets extension for JupyterLab](https://ipywidgets.readthedocs.io/en/latest/user_install.html).\n",
    ">\n",
    "> On [SageMaker Studio](https://aws.amazon.com/sagemaker/studio/), this should be installed by default. On the classic [SageMaker Notebook Instances](https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html) though, you'll need to install the `@jupyter-widgets/jupyterlab-manager` extension (from `Settings > Extension Manager`, or using a [lifecycle configuration](https://docs.aws.amazon.com/sagemaker/latest/dg/notebook-lifecycle-config.html) similar to [this sample](https://github.com/aws-samples/amazon-sagemaker-notebook-instance-lifecycle-config-samples/tree/master/scripts/install-lab-extension)) - or just use plain `Jupyter` instead of `JupyterLab`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "291cc40546f849c49bcfb5a53c8b906c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Example:', max=9), Output()), _dom_classes=('widget-inte…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function util.viz.draw_from_manifest_items.<locals>.draw(ix)>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"data/annotations/annotations-test.manifest.jsonl\", \"r\") as fman:\n",
    "    test_examples = [json.loads(line) for line in filter(lambda l: l, fman)]\n",
    "\n",
    "util.viz.draw_from_manifest_items(\n",
    "    test_examples,\n",
    "    standard_label_field,\n",
    "    entity_classes,\n",
    "    imgs_s3uri[len(\"s3://\"):].partition(\"/\")[2],\n",
    "    textract_s3key_prefix=textract_s3uri[len(\"s3://\"):].partition(\"/\")[2],\n",
    "    imgs_local_prefix=\"data/imgs-clean\",\n",
    "    textract_local_prefix=\"data/textracted\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepare custom training and inference containers\n",
    "\n",
    "SageMaker framework containers like those [for PyTorch](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html) and [Hugging Face](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/index.html) support `pip` runtime dependency injection by specifying a `requirements.txt` file in your source bundle - and a specimen requirements file is included in [src/requirements.txt](src/requirements.txt).\n",
    "\n",
    "This can make experimenting with different library versions faster. **However**, running the installs at each training job / endpoint start-up can make experimenting with script code changes slower. \n",
    "\n",
    "Some of the extra computer vision dependencies required for this use case can take a while to install, so in this example we'll build customized containers in advance (as shown in notebook 1 for pre-processing) and leave our requirements.txt empty:\n",
    "\n",
    "> ℹ️ **Alternatively:** If needed (for example, to experiment with [SageMaker Training Compiler](https://docs.aws.amazon.com/sagemaker/latest/dg/training-compiler.html) which doesn't support customized containers at the time of writing), you could instead:\n",
    ">\n",
    "> 1. Uncomment the dependencies listed in [src/requirements.txt](src/requirements.txt)\n",
    "> 2. Skip the `sm-docker build` steps below, and\n",
    "> 3. Remove the `image_uri=` arguments later in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target training image: 015943506230.dkr.ecr.us-east-1.amazonaws.com/sm-ocr-training:hf-4.26-pt-gpu\n",
      "Target inference image: 015943506230.dkr.ecr.us-east-1.amazonaws.com/sm-ocr-inference:hf-4.26-pt-gpu\n"
     ]
    }
   ],
   "source": [
    "# Configurations:\n",
    "hf_version = \"4.17\"\n",
    "py_version = \"py38\"\n",
    "pt_version = \"1.10\"\n",
    "train_repo_name = \"sm-ocr-training\"\n",
    "train_repo_tag = \"hf-4.26-pt-gpu\"  # (Base HF version is overridden in Dockerfile)\n",
    "inf_repo_name = \"sm-ocr-inference\"\n",
    "inf_repo_tag = train_repo_tag\n",
    "\n",
    "account_id = sagemaker.Session().account_id()\n",
    "region = os.environ[\"AWS_REGION\"]\n",
    "\n",
    "base_image_params = {\n",
    "    \"framework\": \"huggingface\",\n",
    "    \"region\": region,\n",
    "    \"instance_type\": \"ml.p3.2xlarge\",  # (Just used to check whether GPUs/accelerators are used)\n",
    "    \"py_version\": py_version,\n",
    "    \"version\": hf_version,\n",
    "    \"base_framework_version\": f\"pytorch{pt_version}\",\n",
    "}\n",
    "\n",
    "train_base_uri = sagemaker.image_uris.retrieve(**base_image_params, image_scope=\"training\")\n",
    "inf_base_uri = sagemaker.image_uris.retrieve(**base_image_params, image_scope=\"inference\")\n",
    "\n",
    "# Combine together into the final URIs:\n",
    "train_image_uri = f\"{account_id}.dkr.ecr.{region}.amazonaws.com/{train_repo_name}:{train_repo_tag}\"\n",
    "print(f\"Target training image: {train_image_uri}\")\n",
    "inf_image_uri = f\"{account_id}.dkr.ecr.{region}.amazonaws.com/{inf_repo_name}:{inf_repo_tag}\"\n",
    "print(f\"Target inference image: {inf_image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `--compute-type` parameter below is optional, but can help to speed up image build versus the [default](https://github.com/aws-samples/sagemaker-studio-image-build-cli/blob/87c25051ab033dc81ae1f388515315a70b701157/sagemaker_studio_image_build/cli.py#L100) `BUILD_GENERAL1_SMALL`.\n",
    "\n",
    "> ⏰ These image builds may take ~12 mins each, but once complete the images will be stored in your Amazon ECR registry and ready to re-use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............[Container] 2023/05/22 12:12:35 Waiting for agent ping\n",
      "\n",
      "[Container] 2023/05/22 12:12:36 Waiting for DOWNLOAD_SOURCE\n",
      "[Container] 2023/05/22 12:12:39 Phase is DOWNLOAD_SOURCE\n",
      "[Container] 2023/05/22 12:12:39 CODEBUILD_SRC_DIR=/codebuild/output/src254403882/src\n",
      "[Container] 2023/05/22 12:12:39 YAML location is /codebuild/output/src254403882/src/buildspec.yml\n",
      "[Container] 2023/05/22 12:12:39 Setting HTTP client timeout to higher timeout for S3 source\n",
      "[Container] 2023/05/22 12:12:39 Processing environment variables\n",
      "[Container] 2023/05/22 12:12:39 No runtime version selected in buildspec.\n",
      "[Container] 2023/05/22 12:12:39 Moving to directory /codebuild/output/src254403882/src\n",
      "[Container] 2023/05/22 12:12:39 Configuring ssm agent with target id: codebuild:582c9577-9cbe-4973-931b-921ecfaa86cf\n",
      "[Container] 2023/05/22 12:12:39 Successfully updated ssm agent configuration\n",
      "[Container] 2023/05/22 12:12:39 Registering with agent\n",
      "[Container] 2023/05/22 12:12:39 Phases found in YAML: 3\n",
      "[Container] 2023/05/22 12:12:39  PRE_BUILD: 9 commands\n",
      "[Container] 2023/05/22 12:12:39  BUILD: 4 commands\n",
      "[Container] 2023/05/22 12:12:39  POST_BUILD: 3 commands\n",
      "[Container] 2023/05/22 12:12:39 Phase complete: DOWNLOAD_SOURCE State: SUCCEEDED\n",
      "[Container] 2023/05/22 12:12:39 Phase context status code:  Message:\n",
      "[Container] 2023/05/22 12:12:39 Entering phase INSTALL\n",
      "[Container] 2023/05/22 12:12:39 Phase complete: INSTALL State: SUCCEEDED\n",
      "[Container] 2023/05/22 12:12:39 Phase context status code:  Message:\n",
      "[Container] 2023/05/22 12:12:39 Entering phase PRE_BUILD\n",
      "[Container] 2023/05/22 12:12:39 Running command echo Logging in to Amazon ECR...\n",
      "Logging in to Amazon ECR...\n",
      "\n",
      "[Container] 2023/05/22 12:12:39 Running command $(aws ecr get-login --no-include-email --region $AWS_DEFAULT_REGION)\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /root/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "\n",
      "[Container] 2023/05/22 12:12:39 Running command $(aws ecr get-login --no-include-email --region $AWS_DEFAULT_REGION --registry-ids 763104351884)\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /root/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "\n",
      "[Container] 2023/05/22 12:12:40 Running command $(aws ecr get-login --no-include-email --region $AWS_DEFAULT_REGION --registry-ids 217643126080)\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /root/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "\n",
      "[Container] 2023/05/22 12:12:41 Running command $(aws ecr get-login --no-include-email --region $AWS_DEFAULT_REGION --registry-ids 727897471807)\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /root/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "\n",
      "[Container] 2023/05/22 12:12:41 Running command $(aws ecr get-login --no-include-email --region $AWS_DEFAULT_REGION --registry-ids 626614931356)\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /root/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "\n",
      "[Container] 2023/05/22 12:12:42 Running command $(aws ecr get-login --no-include-email --region $AWS_DEFAULT_REGION --registry-ids 683313688378)\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /root/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "\n",
      "[Container] 2023/05/22 12:12:42 Running command $(aws ecr get-login --no-include-email --region $AWS_DEFAULT_REGION --registry-ids 520713654638)\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /root/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "\n",
      "[Container] 2023/05/22 12:12:43 Running command $(aws ecr get-login --no-include-email --region $AWS_DEFAULT_REGION --registry-ids 462105765813)\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /root/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "\n",
      "[Container] 2023/05/22 12:12:43 Phase complete: PRE_BUILD State: SUCCEEDED\n",
      "[Container] 2023/05/22 12:12:43 Phase context status code:  Message:\n",
      "[Container] 2023/05/22 12:12:43 Entering phase BUILD\n",
      "[Container] 2023/05/22 12:12:43 Running command echo Build started on `date`\n",
      "Build started on Mon May 22 12:12:43 UTC 2023\n",
      "\n",
      "[Container] 2023/05/22 12:12:43 Running command echo Building the Docker image...\n",
      "Building the Docker image...\n",
      "\n",
      "[Container] 2023/05/22 12:12:43 Running command docker build -t $IMAGE_REPO_NAME:$IMAGE_TAG . --build-arg BASE_IMAGE=763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-training:1.10-transformers4.17-gpu-py38-cu113-ubuntu20.04\n",
      "Sending build context to Docker daemon  14.34kB\n",
      "Step 1/11 : ARG BASE_IMAGE\n",
      "Step 2/11 : FROM ${BASE_IMAGE}\n",
      "1.10-transformers4.17-gpu-py38-cu113-ubuntu20.04: Pulling from huggingface-pytorch-training\n",
      "d5fd17ec1767: Pulling fs layer\n",
      "602a45a9c0c5: Pulling fs layer\n",
      "e1bae4c1f40f: Pulling fs layer\n",
      "d9d586ab2510: Pulling fs layer\n",
      "2b44adc78060: Pulling fs layer\n",
      "730d884dbef8: Pulling fs layer\n",
      "50ad21df9a3e: Pulling fs layer\n",
      "9eafb11052ac: Pulling fs layer\n",
      "5fe87f5ae3ac: Pulling fs layer\n",
      "c8f6c9dd8923: Pulling fs layer\n",
      "2b44adc78060: Waiting\n",
      "730d884dbef8: Waiting\n",
      "31bf103fe6b3: Pulling fs layer\n",
      "9eafb11052ac: Waiting\n",
      "562ac9a3d216: Pulling fs layer\n",
      "54fe2abe727c: Pulling fs layer\n",
      "dc7cd22e079c: Pulling fs layer\n",
      "0eddd014f811: Pulling fs layer\n",
      "64f77656d17f: Pulling fs layer\n",
      "d6c570538b14: Pulling fs layer\n",
      "d9d586ab2510: Waiting\n",
      "5fe87f5ae3ac: Waiting\n",
      "c8f6c9dd8923: Waiting\n",
      "54fe2abe727c: Waiting\n",
      "50ad21df9a3e: Waiting\n",
      "0eddd014f811: Waiting\n",
      "dc7cd22e079c: Waiting\n",
      "a533383da43a: Pulling fs layer\n",
      "d6c570538b14: Waiting\n",
      "35298f177473: Pulling fs layer\n",
      "35298f177473: Waiting\n",
      "9d5ee67f9f1f: Pulling fs layer\n",
      "3014f3448d61: Pulling fs layer\n",
      "f2be9444d784: Pulling fs layer\n",
      "a19ae770a881: Pulling fs layer\n",
      "7d260e48db29: Pulling fs layer\n",
      "d8e27c3d1895: Pulling fs layer\n",
      "535794a75fce: Pulling fs layer\n",
      "8818779e0684: Pulling fs layer\n",
      "abd39f77bdfc: Pulling fs layer\n",
      "d29b5a4cfb05: Pulling fs layer\n",
      "763cd34b2834: Pulling fs layer\n",
      "989c8cbc23e4: Pulling fs layer\n",
      "3c5e96222af0: Pulling fs layer\n",
      "00161462f926: Pulling fs layer\n",
      "9d56caf04fdb: Pulling fs layer\n",
      "9d5ee67f9f1f: Waiting\n",
      "963f539973b7: Pulling fs layer\n",
      "3014f3448d61: Waiting\n",
      "7278be1128d0: Pulling fs layer\n",
      "8cf3cf15916d: Pulling fs layer\n",
      "f2be9444d784: Waiting\n",
      "49b52e822b07: Pulling fs layer\n",
      "83a67dac4a1e: Pulling fs layer\n",
      "d5f9b62fcc5a: Pulling fs layer\n",
      "a19ae770a881: Waiting\n",
      "6e2956251efa: Pulling fs layer\n",
      "7d260e48db29: Waiting\n",
      "8b165a00ee12: Pulling fs layer\n",
      "f566c9c65b31: Pulling fs layer\n",
      "8932a548d301: Pulling fs layer\n",
      "d8e27c3d1895: Waiting\n",
      "535794a75fce: Waiting\n",
      "3cedeec97e56: Pulling fs layer\n",
      "8818779e0684: Waiting\n",
      "392d23d51eea: Pulling fs layer\n",
      "abd39f77bdfc: Waiting\n",
      "e7e8664f7c7b: Pulling fs layer\n",
      "d29b5a4cfb05: Waiting\n",
      "c29d0dc79985: Pulling fs layer\n",
      "763cd34b2834: Waiting\n",
      "c91e430fae61: Pulling fs layer\n",
      "5ce2e2174ce9: Pulling fs layer\n",
      "35d427b6cc06: Pulling fs layer\n",
      "989c8cbc23e4: Waiting\n",
      "3c5e96222af0: Waiting\n",
      "d5f9b62fcc5a: Waiting\n",
      "6e2956251efa: Waiting\n",
      "e7e8664f7c7b: Waiting\n",
      "8b165a00ee12: Waiting\n",
      "00161462f926: Waiting\n",
      "8932a548d301: Waiting\n",
      "c29d0dc79985: Waiting\n",
      "392d23d51eea: Waiting\n",
      "35d427b6cc06: Waiting\n",
      "c91e430fae61: Waiting\n",
      "3cedeec97e56: Waiting\n",
      "8cf3cf15916d: Waiting\n",
      "963f539973b7: Waiting\n",
      "49b52e822b07: Waiting\n",
      "f566c9c65b31: Waiting\n",
      "83a67dac4a1e: Waiting\n",
      "9d56caf04fdb: Waiting\n",
      "7278be1128d0: Waiting\n",
      "602a45a9c0c5: Verifying Checksum\n",
      "602a45a9c0c5: Download complete\n",
      "e1bae4c1f40f: Verifying Checksum\n",
      "e1bae4c1f40f: Download complete\n",
      "d9d586ab2510: Verifying Checksum\n",
      "d9d586ab2510: Download complete\n",
      "2b44adc78060: Verifying Checksum\n",
      "2b44adc78060: Download complete\n",
      "50ad21df9a3e: Verifying Checksum\n",
      "50ad21df9a3e: Download complete\n",
      "d5fd17ec1767: Verifying Checksum\n",
      "d5fd17ec1767: Download complete\n",
      "9eafb11052ac: Verifying Checksum\n",
      "9eafb11052ac: Download complete\n",
      "730d884dbef8: Verifying Checksum\n",
      "730d884dbef8: Download complete\n",
      "31bf103fe6b3: Download complete\n",
      "d5fd17ec1767: Pull complete\n",
      "562ac9a3d216: Verifying Checksum\n",
      "562ac9a3d216: Download complete\n",
      "602a45a9c0c5: Pull complete\n",
      "e1bae4c1f40f: Pull complete\n",
      "d9d586ab2510: Pull complete\n",
      "2b44adc78060: Pull complete\n",
      "730d884dbef8: Pull complete\n",
      "50ad21df9a3e: Pull complete\n",
      "9eafb11052ac: Pull complete\n",
      "54fe2abe727c: Verifying Checksum\n",
      "54fe2abe727c: Download complete\n",
      "dc7cd22e079c: Download complete\n",
      "0eddd014f811: Verifying Checksum\n",
      "0eddd014f811: Download complete\n",
      "64f77656d17f: Verifying Checksum\n",
      "64f77656d17f: Download complete\n",
      "c8f6c9dd8923: Verifying Checksum\n",
      "c8f6c9dd8923: Download complete\n",
      "a533383da43a: Verifying Checksum\n",
      "a533383da43a: Download complete\n",
      "35298f177473: Verifying Checksum\n",
      "35298f177473: Download complete\n",
      "9d5ee67f9f1f: Verifying Checksum\n",
      "9d5ee67f9f1f: Download complete\n",
      "3014f3448d61: Verifying Checksum\n",
      "3014f3448d61: Download complete\n",
      "f2be9444d784: Verifying Checksum\n",
      "f2be9444d784: Download complete\n",
      "a19ae770a881: Verifying Checksum\n",
      "a19ae770a881: Download complete\n",
      "7d260e48db29: Verifying Checksum\n",
      "7d260e48db29: Download complete\n",
      "d8e27c3d1895: Verifying Checksum\n",
      "d8e27c3d1895: Download complete\n",
      "535794a75fce: Download complete\n",
      "8818779e0684: Download complete\n",
      "abd39f77bdfc: Verifying Checksum\n",
      "abd39f77bdfc: Download complete\n",
      "d29b5a4cfb05: Verifying Checksum\n",
      "d29b5a4cfb05: Download complete\n",
      "763cd34b2834: Verifying Checksum\n",
      "763cd34b2834: Download complete\n",
      "989c8cbc23e4: Verifying Checksum\n",
      "989c8cbc23e4: Download complete\n",
      "3c5e96222af0: Verifying Checksum\n",
      "3c5e96222af0: Download complete\n",
      "00161462f926: Verifying Checksum\n",
      "00161462f926: Download complete\n",
      "9d56caf04fdb: Download complete\n",
      "963f539973b7: Verifying Checksum\n",
      "963f539973b7: Download complete\n",
      "7278be1128d0: Verifying Checksum\n",
      "7278be1128d0: Download complete\n",
      "8cf3cf15916d: Verifying Checksum\n",
      "8cf3cf15916d: Download complete\n",
      "49b52e822b07: Download complete\n",
      "d6c570538b14: Verifying Checksum\n",
      "d6c570538b14: Download complete\n",
      "d5f9b62fcc5a: Verifying Checksum\n",
      "d5f9b62fcc5a: Download complete\n",
      "6e2956251efa: Download complete\n",
      "8b165a00ee12: Verifying Checksum\n",
      "8b165a00ee12: Download complete\n",
      "f566c9c65b31: Download complete\n",
      "8932a548d301: Verifying Checksum\n",
      "8932a548d301: Download complete\n",
      "3cedeec97e56: Verifying Checksum\n",
      "3cedeec97e56: Download complete\n",
      "392d23d51eea: Verifying Checksum\n",
      "392d23d51eea: Download complete\n",
      "e7e8664f7c7b: Verifying Checksum\n",
      "e7e8664f7c7b: Download complete\n",
      "c29d0dc79985: Verifying Checksum\n",
      "c29d0dc79985: Download complete\n",
      "c91e430fae61: Verifying Checksum\n",
      "c91e430fae61: Download complete\n",
      "5ce2e2174ce9: Verifying Checksum\n",
      "5ce2e2174ce9: Download complete\n",
      "35d427b6cc06: Verifying Checksum\n",
      "35d427b6cc06: Download complete\n",
      "5fe87f5ae3ac: Verifying Checksum\n",
      "5fe87f5ae3ac: Download complete\n",
      "83a67dac4a1e: Verifying Checksum\n",
      "83a67dac4a1e: Download complete\n",
      "5fe87f5ae3ac: Pull complete\n",
      "c8f6c9dd8923: Pull complete\n",
      "31bf103fe6b3: Pull complete\n",
      "562ac9a3d216: Pull complete\n",
      "54fe2abe727c: Pull complete\n",
      "dc7cd22e079c: Pull complete\n",
      "0eddd014f811: Pull complete\n",
      "64f77656d17f: Pull complete\n",
      "d6c570538b14: Pull complete\n",
      "a533383da43a: Pull complete\n",
      "35298f177473: Pull complete\n",
      "9d5ee67f9f1f: Pull complete\n",
      "3014f3448d61: Pull complete\n",
      "f2be9444d784: Pull complete\n",
      "a19ae770a881: Pull complete\n",
      "7d260e48db29: Pull complete\n",
      "d8e27c3d1895: Pull complete\n",
      "535794a75fce: Pull complete\n",
      "8818779e0684: Pull complete\n",
      "abd39f77bdfc: Pull complete\n",
      "d29b5a4cfb05: Pull complete\n",
      "763cd34b2834: Pull complete\n",
      "989c8cbc23e4: Pull complete\n",
      "3c5e96222af0: Pull complete\n",
      "00161462f926: Pull complete\n",
      "9d56caf04fdb: Pull complete\n",
      "963f539973b7: Pull complete\n",
      "7278be1128d0: Pull complete\n",
      "8cf3cf15916d: Pull complete\n",
      "49b52e822b07: Pull complete\n",
      "83a67dac4a1e: Pull complete\n",
      "d5f9b62fcc5a: Pull complete\n",
      "6e2956251efa: Pull complete\n",
      "8b165a00ee12: Pull complete\n",
      "f566c9c65b31: Pull complete\n",
      "8932a548d301: Pull complete\n",
      "3cedeec97e56: Pull complete\n",
      "392d23d51eea: Pull complete\n",
      "e7e8664f7c7b: Pull complete\n",
      "c29d0dc79985: Pull complete\n",
      "c91e430fae61: Pull complete\n",
      "5ce2e2174ce9: Pull complete\n",
      "35d427b6cc06: Pull complete\n",
      "Digest: sha256:59feb63a9b7e0c60a9c3147f4acf8cfb9301f835d9c7281052f582826285a436\n",
      "Status: Downloaded newer image for 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-training:1.10-transformers4.17-gpu-py38-cu113-ubuntu20.04\n",
      " ---> c51c6b7cbd14\n",
      "Step 3/11 : RUN apt-get update -y && apt-get install -y --no-install-recommends build-essential gcc                                         libsndfile1\n",
      " ---> Running in f99763abbb4b\n",
      "Get:1 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
      "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease [1581 B]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu focal InRelease [265 kB]\n",
      "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  Packages [1010 kB]\n",
      "Get:5 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [2240 kB]\n",
      "Get:6 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1049 kB]\n",
      "Get:7 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [28.5 kB]\n",
      "Get:8 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2699 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu focal/multiverse amd64 Packages [177 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu focal/main amd64 Packages [1275 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu focal/restricted amd64 Packages [33.4 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu focal/universe amd64 Packages [11.3 MB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [31.2 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3197 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [2408 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1344 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu focal-backports/universe amd64 Packages [28.6 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu focal-backports/main amd64 Packages [55.2 kB]\n",
      "Fetched 27.5 MB in 2s (13.0 MB/s)\n",
      "Reading package lists...\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "gcc is already the newest version (4:9.3.0-1ubuntu2).\n",
      "gcc set to manually installed.\n",
      "build-essential is already the newest version (12.8ubuntu1.1).\n",
      "The following NEW packages will be installed:\n",
      "  libflac8 libsndfile1 libvorbisenc2\n",
      "0 upgraded, 3 newly installed, 0 to remove and 95 not upgraded.\n",
      "Need to get 344 kB of archives.\n",
      "After this operation, 1554 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libflac8 amd64 1.3.3-1ubuntu0.1 [103 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu focal/main amd64 libvorbisenc2 amd64 1.3.6-2ubuntu1 [70.7 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libsndfile1 amd64 1.0.28-7ubuntu0.1 [170 kB]\n",
      "\u001b[91mdebconf: delaying package configuration, since apt-utils is not installed\n",
      "\u001b[0mFetched 344 kB in 0s (3125 kB/s)\n",
      "Selecting previously unselected package libflac8:amd64.\n",
      "(Reading database ... 44929 files and directories currently installed.)\n",
      "Preparing to unpack .../libflac8_1.3.3-1ubuntu0.1_amd64.deb ...\n",
      "Unpacking libflac8:amd64 (1.3.3-1ubuntu0.1) ...\n",
      "Selecting previously unselected package libvorbisenc2:amd64.\n",
      "Preparing to unpack .../libvorbisenc2_1.3.6-2ubuntu1_amd64.deb ...\n",
      "Unpacking libvorbisenc2:amd64 (1.3.6-2ubuntu1) ...\n",
      "Selecting previously unselected package libsndfile1:amd64.\n",
      "Preparing to unpack .../libsndfile1_1.0.28-7ubuntu0.1_amd64.deb ...\n",
      "Unpacking libsndfile1:amd64 (1.0.28-7ubuntu0.1) ...\n",
      "Setting up libflac8:amd64 (1.3.3-1ubuntu0.1) ...\n",
      "Setting up libvorbisenc2:amd64 (1.3.6-2ubuntu1) ...\n",
      "Setting up libsndfile1:amd64 (1.0.28-7ubuntu0.1) ...\n",
      "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n",
      "Removing intermediate container f99763abbb4b\n",
      " ---> 61babd570787\n",
      "Step 4/11 : RUN pip install SoundFile\n",
      " ---> Running in 36c9ceb74bd8\n",
      "Requirement already satisfied: SoundFile in /opt/conda/lib/python3.8/site-packages (0.11.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.8/site-packages (from SoundFile) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.0->SoundFile) (2.21)\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.1.2\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mRemoving intermediate container 36c9ceb74bd8\n",
      " ---> d36b85752c42\n",
      "Step 5/11 : RUN pip install librosa\n",
      " ---> Running in 9c23d3f9b9ca\n",
      "Requirement already satisfied: librosa in /opt/conda/lib/python3.8/site-packages (0.9.2)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /opt/conda/lib/python3.8/site-packages (from librosa) (1.9.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /opt/conda/lib/python3.8/site-packages (from librosa) (1.22.2)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /opt/conda/lib/python3.8/site-packages (from librosa) (1.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from librosa) (21.3)\n",
      "Requirement already satisfied: soundfile>=0.10.2 in /opt/conda/lib/python3.8/site-packages (from librosa) (0.11.0)\n",
      "Requirement already satisfied: numba>=0.45.1 in /opt/conda/lib/python3.8/site-packages (from librosa) (0.53.1)\n",
      "Requirement already satisfied: joblib>=0.14 in /opt/conda/lib/python3.8/site-packages (from librosa) (1.2.0)\n",
      "Requirement already satisfied: resampy>=0.2.2 in /opt/conda/lib/python3.8/site-packages (from librosa) (0.4.2)\n",
      "Requirement already satisfied: decorator>=4.0.10 in /opt/conda/lib/python3.8/site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: pooch>=1.0 in /opt/conda/lib/python3.8/site-packages (from librosa) (1.6.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /opt/conda/lib/python3.8/site-packages (from librosa) (3.0.0)\n",
      "Requirement already satisfied: llvmlite<0.37,>=0.36.0rc1 in /opt/conda/lib/python3.8/site-packages (from numba>=0.45.1->librosa) (0.36.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from numba>=0.45.1->librosa) (65.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->librosa) (3.0.9)\n",
      "Requirement already satisfied: appdirs>=1.3.0 in /opt/conda/lib/python3.8/site-packages (from pooch>=1.0->librosa) (1.4.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.8/site-packages (from pooch>=1.0->librosa) (2.28.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=0.19.1->librosa) (3.1.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.8/site-packages (from soundfile>=0.10.2->librosa) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.21)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2022.9.24)\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.1.2\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mRemoving intermediate container 9c23d3f9b9ca\n",
      " ---> 4f9b682ab522\n",
      "Step 6/11 : RUN pip install \"amazon-textract-response-parser>=0.1,<0.2\" \"Pillow>=8,<9\"     && PT_VER=`pip show torch | grep 'Version:' | sed 's/Version: //'`     && pip install git+https://github.com/facebookresearch/detectron2.git setuptools==59.5.0         torch==$PT_VER \"torchvision>=0.11.3,<0.15\" \"datasets>=2.4,<3\" \"protobuf<3.21\"         \"transformers>=4.25,<4.27\"\n",
      " ---> Running in 91e64730e874\n",
      "Collecting amazon-textract-response-parser<0.2,>=0.1\n",
      "  Downloading amazon_textract_response_parser-0.1.46-py2.py3-none-any.whl (29 kB)\n",
      "Collecting Pillow<9,>=8\n",
      "  Downloading Pillow-8.4.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 98.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.8/site-packages (from amazon-textract-response-parser<0.2,>=0.1) (1.24.82)\n",
      "Collecting marshmallow<4,>=3.14\n",
      "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 49.1/49.1 kB 11.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.8/site-packages (from marshmallow<4,>=3.14->amazon-textract-response-parser<0.2,>=0.1) (21.3)\n",
      "Requirement already satisfied: botocore<1.28.0,>=1.27.82 in /opt/conda/lib/python3.8/site-packages (from boto3->amazon-textract-response-parser<0.2,>=0.1) (1.27.82)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from boto3->amazon-textract-response-parser<0.2,>=0.1) (0.6.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.8/site-packages (from boto3->amazon-textract-response-parser<0.2,>=0.1) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.8/site-packages (from botocore<1.28.0,>=1.27.82->boto3->amazon-textract-response-parser<0.2,>=0.1) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.8/site-packages (from botocore<1.28.0,>=1.27.82->boto3->amazon-textract-response-parser<0.2,>=0.1) (1.26.12)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=17.0->marshmallow<4,>=3.14->amazon-textract-response-parser<0.2,>=0.1) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.28.0,>=1.27.82->boto3->amazon-textract-response-parser<0.2,>=0.1) (1.16.0)\n",
      "Installing collected packages: Pillow, marshmallow, amazon-textract-response-parser\n",
      "  Attempting uninstall: Pillow\n",
      "    Found existing installation: Pillow 9.2.0\n",
      "    Uninstalling Pillow-9.2.0:\n",
      "      Successfully uninstalled Pillow-9.2.0\n",
      "Successfully installed Pillow-8.4.0 amazon-textract-response-parser-0.1.46 marshmallow-3.19.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.1.2\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mCollecting git+https://github.com/facebookresearch/detectron2.git\n",
      "  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-d6amr70_\n",
      "\u001b[91m  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-d6amr70_\n",
      "\u001b[0m  Resolved https://github.com/facebookresearch/detectron2.git to commit 2c6c380f94a27bd8455a39506c9105f652b9f760\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting setuptools==59.5.0\n",
      "  Downloading setuptools-59.5.0-py3-none-any.whl (952 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 952.4/952.4 kB 47.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: torch==1.10.2+cu113 in /opt/conda/lib/python3.8/site-packages (1.10.2+cu113)\n",
      "Requirement already satisfied: torchvision<0.15,>=0.11.3 in /opt/conda/lib/python3.8/site-packages (0.11.3)\n",
      "Collecting datasets<3,>=2.4\n",
      "  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 474.6/474.6 kB 49.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: protobuf<3.21 in /opt/conda/lib/python3.8/site-packages (3.19.5)\n",
      "Collecting transformers<4.27,>=4.25\n",
      "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.3/6.3 MB 91.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch==1.10.2+cu113) (4.3.0)\n",
      "Requirement already satisfied: Pillow>=7.1 in /opt/conda/lib/python3.8/site-packages (from detectron2==0.6) (8.4.0)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.8/site-packages (from detectron2==0.6) (3.6.0)\n",
      "Collecting pycocotools>=2.0.2\n",
      "  Downloading pycocotools-2.0.6.tar.gz (24 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting termcolor>=1.1\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Collecting yacs>=0.1.8\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: tabulate in /opt/conda/lib/python3.8/site-packages (from detectron2==0.6) (0.8.10)\n",
      "Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.8/site-packages (from detectron2==0.6) (2.2.0)\n",
      "Requirement already satisfied: tqdm>4.29.0 in /opt/conda/lib/python3.8/site-packages (from detectron2==0.6) (4.64.0)\n",
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/5.6 MB 123.2 MB/s eta 0:00:00\n",
      "Collecting fvcore<0.1.6,>=0.1.5\n",
      "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.2/50.2 kB 16.3 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting iopath<0.1.10,>=0.1.7\n",
      "  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
      "Collecting omegaconf>=2.1\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79.5/79.5 kB 23.0 MB/s eta 0:00:00\n",
      "Collecting hydra-core>=1.1\n",
      "  Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 154.5/154.5 kB 42.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: black in /opt/conda/lib/python3.8/site-packages (from detectron2==0.6) (22.8.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from detectron2==0.6) (21.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from torchvision<0.15,>=0.11.3) (1.22.2)\n",
      "Collecting huggingface-hub<1.0.0,>=0.11.0\n",
      "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 224.5/224.5 kB 52.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets<3,>=2.4) (1.5.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.8/site-packages (from datasets<3,>=2.4) (2.28.1)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.8/site-packages (from datasets<3,>=2.4) (0.18.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.8/site-packages (from datasets<3,>=2.4) (2022.8.2)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.8/site-packages (from datasets<3,>=2.4) (0.3.5.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets<3,>=2.4) (9.0.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from datasets<3,>=2.4) (3.8.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.8/site-packages (from datasets<3,>=2.4) (3.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from datasets<3,>=2.4) (5.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets<3,>=2.4) (0.70.13)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.8/site-packages (from transformers<4.27,>=4.25) (0.13.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers<4.27,>=4.25) (3.8.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers<4.27,>=4.25) (2022.9.13)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets<3,>=2.4) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets<3,>=2.4) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets<3,>=2.4) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets<3,>=2.4) (21.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets<3,>=2.4) (2.0.12)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets<3,>=2.4) (6.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets<3,>=2.4) (1.2.0)\n",
      "Collecting antlr4-python3-runtime==4.9.*\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 117.0/117.0 kB 37.3 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: importlib-resources in /opt/conda/lib/python3.8/site-packages (from hydra-core>=1.1->detectron2==0.6) (5.9.0)\n",
      "Collecting portalocker\n",
      "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->detectron2==0.6) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->detectron2==0.6) (1.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib->detectron2==0.6) (4.37.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.8/site-packages (from matplotlib->detectron2==0.6) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib->detectron2==0.6) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->detectron2==0.6) (1.0.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets<3,>=2.4) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets<3,>=2.4) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets<3,>=2.4) (1.26.12)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.8/site-packages (from black->detectron2==0.6) (8.1.3)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in /opt/conda/lib/python3.8/site-packages (from black->detectron2==0.6) (0.10.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /opt/conda/lib/python3.8/site-packages (from black->detectron2==0.6) (0.4.3)\n",
      "Requirement already satisfied: platformdirs>=2 in /opt/conda/lib/python3.8/site-packages (from black->detectron2==0.6) (2.5.2)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.8/site-packages (from black->detectron2==0.6) (2.0.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets<3,>=2.4) (2022.2.1)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.18.1-py2.py3-none-any.whl (178 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 178.9/178.9 kB 49.1 MB/s eta 0:00:00\n",
      "Collecting protobuf<3.21\n",
      "  Downloading protobuf-3.20.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 110.1 MB/s eta 0:00:00\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.0-py3-none-manylinux2014_x86_64.whl (6.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 122.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.8/site-packages (from tensorboard->detectron2==0.6) (0.37.1)\n",
      "Collecting grpcio>=1.48.2\n",
      "  Downloading grpcio-1.54.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.1/5.1 MB 130.1 MB/s eta 0:00:00\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard->detectron2==0.6) (2.2.2)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 93.9/93.9 kB 30.2 MB/s eta 0:00:00\n",
      "Collecting absl-py>=0.4\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 126.5/126.5 kB 32.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (1.16.0)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 181.3/181.3 kB 51.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (4.7.2)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard->detectron2==0.6) (4.12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (2.1.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.8/site-packages (from importlib-resources->hydra-core>=1.1->detectron2==0.6) (3.8.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.7/151.7 kB 44.7 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: detectron2, fvcore, antlr4-python3-runtime, pycocotools\n",
      "  Building wheel for detectron2 (setup.py): started\n",
      "  Building wheel for detectron2 (setup.py): finished with status 'done'\n",
      "  Created wheel for detectron2: filename=detectron2-0.6-cp38-cp38-linux_x86_64.whl size=890279 sha256=30149d9d17a75114296cc998a8a05aacec40053e7005b668805e8fb4fcb20940\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-d1zxk1_j/wheels/19/ac/65/e48e5e4ec2702274d927c5a6efb75709b24014371d3bb778f2\n",
      "  Building wheel for fvcore (setup.py): started\n",
      "  Building wheel for fvcore (setup.py): finished with status 'done'\n",
      "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61406 sha256=65ae6f195fb41347d3a251f51145ac48114073b4548e659e9c5173d43f989fa6\n",
      "  Stored in directory: /root/.cache/pip/wheels/b8/79/07/c0e9367f5b5ea325e246bd73651e8af175fabbef943043b1cc\n",
      "  Building wheel for antlr4-python3-runtime (setup.py): started\n",
      "  Building wheel for antlr4-python3-runtime (setup.py): finished with status 'done'\n",
      "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=02db2f169669c4898dde6131ee8cc880eb2bf560a29d4237ef0579033355dd92\n",
      "  Stored in directory: /root/.cache/pip/wheels/b1/a3/c2/6df046c09459b73cc9bb6c4401b0be6c47048baf9a1617c485\n",
      "  Building wheel for pycocotools (pyproject.toml): started\n",
      "  Building wheel for pycocotools (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pycocotools: filename=pycocotools-2.0.6-cp38-cp38-linux_x86_64.whl size=105340 sha256=0a25424c71272995d5b46452c1f548502bd0c51c41d1c7d05c9d4716adb1edf1\n",
      "  Stored in directory: /root/.cache/pip/wheels/3e/08/ac/58126fe59992032701437336493f6132e1b72381a62d00b595\n",
      "Successfully built detectron2 fvcore antlr4-python3-runtime pycocotools\n",
      "Installing collected packages: antlr4-python3-runtime, yacs, termcolor, tensorboard-data-server, setuptools, pyasn1-modules, protobuf, portalocker, omegaconf, oauthlib, grpcio, cachetools, absl-py, requests-oauthlib, markdown, iopath, hydra-core, huggingface-hub, google-auth, transformers, pycocotools, google-auth-oauthlib, fvcore, tensorboard, datasets, detectron2\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 65.4.0\n",
      "    Uninstalling setuptools-65.4.0:\n",
      "      Successfully uninstalled setuptools-65.4.0\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.19.5\n",
      "    Uninstalling protobuf-3.19.5:\n",
      "      Successfully uninstalled protobuf-3.19.5\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.10.0\n",
      "    Uninstalling huggingface-hub-0.10.0:\n",
      "      Successfully uninstalled huggingface-hub-0.10.0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.17.0\n",
      "    Uninstalling transformers-4.17.0:\n",
      "      Successfully uninstalled transformers-4.17.0\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 1.18.4\n",
      "    Uninstalling datasets-1.18.4:\n",
      "      Successfully uninstalled datasets-1.18.4\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sagemaker-training 4.2.9 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
      "smdebug 1.0.22b20220929 requires protobuf<=3.20.1, but you have protobuf 3.20.3 which is incompatible.\n",
      "\u001b[0mSuccessfully installed absl-py-1.4.0 antlr4-python3-runtime-4.9.3 cachetools-5.3.0 datasets-2.12.0 detectron2-0.6 fvcore-0.1.5.post20221221 google-auth-2.18.1 google-auth-oauthlib-1.0.0 grpcio-1.54.2 huggingface-hub-0.14.1 hydra-core-1.3.2 iopath-0.1.9 markdown-3.4.3 oauthlib-3.2.2 omegaconf-2.3.0 portalocker-2.7.0 protobuf-3.20.3 pyasn1-modules-0.3.0 pycocotools-2.0.6 requests-oauthlib-1.3.1 setuptools-59.5.0 tensorboard-2.13.0 tensorboard-data-server-0.7.0 termcolor-2.3.0 transformers-4.26.1 yacs-0.1.8\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.1.2\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mRemoving intermediate container 91e64730e874\n",
      " ---> f228660ea4c9\n",
      "Step 7/11 : RUN PT_VER=`pip show torch | grep 'Version:' | sed 's/Version: //'`     && pip install pytesseract torch==$PT_VER\n",
      " ---> Running in 067f1d46b763\n",
      "Collecting pytesseract\n",
      "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: torch==1.10.2+cu113 in /opt/conda/lib/python3.8/site-packages (1.10.2+cu113)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch==1.10.2+cu113) (4.3.0)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in /opt/conda/lib/python3.8/site-packages (from pytesseract) (8.4.0)\n",
      "Requirement already satisfied: packaging>=21.3 in /opt/conda/lib/python3.8/site-packages (from pytesseract) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=21.3->pytesseract) (3.0.9)\n",
      "Installing collected packages: pytesseract\n",
      "Successfully installed pytesseract-0.3.10\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.1.2\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mRemoving intermediate container 067f1d46b763\n",
      " ---> 1e68fae534fc\n",
      "Step 8/11 : ARG INCLUDE_NOTEBOOK_KERNEL\n",
      " ---> Running in 993ac83da2c0\n",
      "Removing intermediate container 993ac83da2c0\n",
      " ---> 21b95a457c35\n",
      "Step 9/11 : RUN if test -z \"$INCLUDE_NOTEBOOK_KERNEL\" ;     then         echo Skipping notebook kernel dependencies     ; else         conda install -y -c conda-forge poppler tesseract &&         PT_VER=`pip show torch | grep 'Version:' | sed 's/Version: //'` &&         pip install easyocr ipykernel \"ipywidgets>=7,<8\" pdf2image pytesseract sagemaker             torch==$PT_VER &&         export TESSDATA_PREFIX='/opt/conda/share/tessdata' &&         python -m ipykernel install --sys-prefix     ; fi\n",
      " ---> Running in 76a427b6db9d\n",
      "Skipping notebook kernel dependencies\n",
      "Removing intermediate container 76a427b6db9d\n",
      " ---> 364bcd64e86f\n",
      "Step 10/11 : ENV USE_SMDEBUG=${INCLUDE_NOTEBOOK_KERNEL:+false}\n",
      " ---> Running in 5eaf52a80494\n",
      "Removing intermediate container 5eaf52a80494\n",
      " ---> 739bb00004c1\n",
      "Step 11/11 : ENV USE_SMDEBUG=${USE_SMDEBUG:-true}\n",
      " ---> Running in 155c032924b3\n",
      "Removing intermediate container 155c032924b3\n",
      " ---> 31c76c322cd4\n",
      "Successfully built 31c76c322cd4\n",
      "Successfully tagged sm-ocr-training:hf-4.26-pt-gpu\n",
      "\n",
      "[Container] 2023/05/22 12:19:32 Running command docker tag $IMAGE_REPO_NAME:$IMAGE_TAG $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME:$IMAGE_TAG\n",
      "\n",
      "[Container] 2023/05/22 12:19:32 Phase complete: BUILD State: SUCCEEDED\n",
      "[Container] 2023/05/22 12:19:32 Phase context status code:  Message:\n",
      "[Container] 2023/05/22 12:19:32 Entering phase POST_BUILD\n",
      "[Container] 2023/05/22 12:19:32 Running command echo Build completed on `date`\n",
      "Build completed on Mon May 22 12:19:32 UTC 2023\n",
      "\n",
      "[Container] 2023/05/22 12:19:32 Running command echo Pushing the Docker image...\n",
      "Pushing the Docker image...\n",
      "\n",
      "[Container] 2023/05/22 12:19:32 Running command docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME:$IMAGE_TAG\n",
      "The push refers to repository [015943506230.dkr.ecr.us-east-1.amazonaws.com/sm-ocr-training]\n",
      "067ef2c55f63: Preparing\n",
      "ca9f13fe2a01: Preparing\n",
      "64021fa91f30: Preparing\n",
      "d30cfc0f86c7: Preparing\n",
      "9aecae81d74f: Preparing\n",
      "33e1d370ef6f: Preparing\n",
      "a211db39931f: Preparing\n",
      "6f7979972917: Preparing\n",
      "0c2c96f19e7e: Preparing\n",
      "950537c911fb: Preparing\n",
      "ae4dbdaf0a42: Preparing\n",
      "a8314fdb7a9a: Preparing\n",
      "af46be8fe5b3: Preparing\n",
      "729f955cd7b6: Preparing\n",
      "9d2e456c4838: Preparing\n",
      "f1cef918b34b: Preparing\n",
      "bedf69f2a960: Preparing\n",
      "3d98fc0e4180: Preparing\n",
      "13287adca40d: Preparing\n",
      "da474668a1da: Preparing\n",
      "f151d62068aa: Preparing\n",
      "37a2fa00b4b5: Preparing\n",
      "07dd9026be78: Preparing\n",
      "724832325ffe: Preparing\n",
      "8cfa8ed5996e: Preparing\n",
      "6991eda8000e: Preparing\n",
      "75b7d929d919: Preparing\n",
      "0db32f284644: Preparing\n",
      "cc0318edd61c: Preparing\n",
      "151b234befb4: Preparing\n",
      "84cb160c6565: Preparing\n",
      "b3b6ce9a03e1: Preparing\n",
      "8b7b3ac71c19: Preparing\n",
      "33e1d370ef6f: Waiting\n",
      "65554af8799a: Preparing\n",
      "efb35d06e22a: Preparing\n",
      "a211db39931f: Waiting\n",
      "13287adca40d: Waiting\n",
      "770ea2bf418f: Preparing\n",
      "6f7979972917: Waiting\n",
      "aabc5635ad79: Preparing\n",
      "21ff56507102: Preparing\n",
      "209e9c00089a: Preparing\n",
      "0f2e9049a095: Preparing\n",
      "0c2c96f19e7e: Waiting\n",
      "bedf69f2a960: Waiting\n",
      "9d2e456c4838: Waiting\n",
      "da474668a1da: Waiting\n",
      "0fba11f2e9f4: Preparing\n",
      "3d98fc0e4180: Waiting\n",
      "f1cef918b34b: Waiting\n",
      "f151d62068aa: Waiting\n",
      "950537c911fb: Waiting\n",
      "60afc926faef: Preparing\n",
      "a8314fdb7a9a: Waiting\n",
      "ae4dbdaf0a42: Waiting\n",
      "1b385db6ff56: Preparing\n",
      "af46be8fe5b3: Waiting\n",
      "4998a58b44cd: Preparing\n",
      "37a2fa00b4b5: Waiting\n",
      "8cfa8ed5996e: Waiting\n",
      "07dd9026be78: Waiting\n",
      "bd232184a32a: Preparing\n",
      "6991eda8000e: Waiting\n",
      "61d1e896198b: Preparing\n",
      "75b7d929d919: Waiting\n",
      "5419752ac5f2: Preparing\n",
      "209e9c00089a: Waiting\n",
      "21ff56507102: Waiting\n",
      "efb35d06e22a: Waiting\n",
      "79c61687bb4a: Preparing\n",
      "151b234befb4: Waiting\n",
      "cc0318edd61c: Waiting\n",
      "aabc5635ad79: Waiting\n",
      "0db32f284644: Waiting\n",
      "770ea2bf418f: Waiting\n",
      "0f2e9049a095: Waiting\n",
      "84cb160c6565: Waiting\n",
      "4998a58b44cd: Waiting\n",
      "b3b6ce9a03e1: Waiting\n",
      "0fba11f2e9f4: Waiting\n",
      "60afc926faef: Waiting\n",
      "8b7b3ac71c19: Waiting\n",
      "e0fc66b168a8: Preparing\n",
      "f15b265376bf: Preparing\n",
      "79c61687bb4a: Waiting\n",
      "b8262d5200e5: Preparing\n",
      "e592fe6d10a9: Preparing\n",
      "f42691182163: Preparing\n",
      "f15b265376bf: Waiting\n",
      "e592fe6d10a9: Waiting\n",
      "68016c5bb65c: Preparing\n",
      "b8262d5200e5: Waiting\n",
      "8034550a3bbe: Preparing\n",
      "f42691182163: Waiting\n",
      "68016c5bb65c: Waiting\n",
      "bf8cedc62fb3: Preparing\n",
      "65554af8799a: Waiting\n",
      "bf8cedc62fb3: Waiting\n",
      "8034550a3bbe: Waiting\n",
      "d30cfc0f86c7: Pushed\n",
      "64021fa91f30: Pushed\n",
      "33e1d370ef6f: Layer already exists\n",
      "067ef2c55f63: Pushed\n",
      "a211db39931f: Layer already exists\n",
      "6f7979972917: Layer already exists\n",
      "0c2c96f19e7e: Layer already exists\n",
      "950537c911fb: Layer already exists\n",
      "ae4dbdaf0a42: Layer already exists\n",
      "a8314fdb7a9a: Layer already exists\n",
      "af46be8fe5b3: Layer already exists\n",
      "729f955cd7b6: Layer already exists\n",
      "f1cef918b34b: Layer already exists\n",
      "9d2e456c4838: Layer already exists\n",
      "bedf69f2a960: Layer already exists\n",
      "3d98fc0e4180: Layer already exists\n",
      "13287adca40d: Layer already exists\n",
      "da474668a1da: Layer already exists\n",
      "f151d62068aa: Layer already exists\n",
      "37a2fa00b4b5: Layer already exists\n",
      "07dd9026be78: Layer already exists\n",
      "724832325ffe: Layer already exists\n",
      "8cfa8ed5996e: Layer already exists\n",
      "6991eda8000e: Layer already exists\n",
      "75b7d929d919: Layer already exists\n",
      "0db32f284644: Layer already exists\n",
      "cc0318edd61c: Layer already exists\n",
      "151b234befb4: Layer already exists\n",
      "84cb160c6565: Layer already exists\n",
      "b3b6ce9a03e1: Layer already exists\n",
      "8b7b3ac71c19: Layer already exists\n",
      "65554af8799a: Layer already exists\n",
      "efb35d06e22a: Layer already exists\n",
      "770ea2bf418f: Layer already exists\n",
      "aabc5635ad79: Layer already exists\n",
      "21ff56507102: Layer already exists\n",
      "209e9c00089a: Layer already exists\n",
      "0f2e9049a095: Layer already exists\n",
      "0fba11f2e9f4: Layer already exists\n",
      "1b385db6ff56: Layer already exists\n",
      "60afc926faef: Layer already exists\n",
      "4998a58b44cd: Layer already exists\n",
      "bd232184a32a: Layer already exists\n",
      "61d1e896198b: Layer already exists\n",
      "5419752ac5f2: Layer already exists\n",
      "e0fc66b168a8: Layer already exists\n",
      "79c61687bb4a: Layer already exists\n",
      "f15b265376bf: Layer already exists\n",
      "b8262d5200e5: Layer already exists\n",
      "e592fe6d10a9: Layer already exists\n",
      "f42691182163: Layer already exists\n",
      "68016c5bb65c: Layer already exists\n",
      "8034550a3bbe: Layer already exists\n",
      "bf8cedc62fb3: Layer already exists\n",
      "9aecae81d74f: Pushed\n",
      "ca9f13fe2a01: Pushed\n",
      "hf-4.26-pt-gpu: digest: sha256:99672b89a676258f667efbd19b3936b8f2c0a46d0a6e494cb1a0ed3cbca65e42 size: 12097\n",
      "\n",
      "[Container] 2023/05/22 12:19:41 Phase complete: POST_BUILD State: SUCCEEDED\n",
      "[Container] 2023/05/22 12:19:41 Phase context status code:  Message:\n",
      "\n",
      "Image URI: 015943506230.dkr.ecr.us-east-1.amazonaws.com/sm-ocr-training:hf-4.26-pt-gpu\n",
      "CPU times: user 9.28 s, sys: 1.81 s, total: 11.1 s\n",
      "Wall time: 10min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# (No need to re-run this cell if your train image is already in ECR)\n",
    "\n",
    "# Build and push the training image:\n",
    "!cd custom-containers/train-inf && sm-docker build . \\\n",
    "    --compute-type BUILD_GENERAL1_LARGE \\\n",
    "    --repository {train_repo_name}:{train_repo_tag} \\\n",
    "    --role {config.sm_image_build_role} \\\n",
    "    --build-arg BASE_IMAGE={train_base_uri}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that although our training and inference containers use the [same Dockerfile](custom-containers/train-inf/Dockerfile), they're built from different parent images so both are needed in ECR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............[Container] 2023/05/22 12:22:21 Waiting for agent ping\n",
      "\n",
      "[Container] 2023/05/22 12:22:22 Waiting for DOWNLOAD_SOURCE\n",
      "[Container] 2023/05/22 12:22:24 Phase is DOWNLOAD_SOURCE\n",
      "[Container] 2023/05/22 12:22:24 CODEBUILD_SRC_DIR=/codebuild/output/src980511363/src\n",
      "[Container] 2023/05/22 12:22:24 YAML location is /codebuild/output/src980511363/src/buildspec.yml\n",
      "[Container] 2023/05/22 12:22:24 Setting HTTP client timeout to higher timeout for S3 source\n",
      "[Container] 2023/05/22 12:22:24 Processing environment variables\n",
      "[Container] 2023/05/22 12:22:24 No runtime version selected in buildspec.\n",
      "[Container] 2023/05/22 12:22:24 Moving to directory /codebuild/output/src980511363/src\n",
      "[Container] 2023/05/22 12:22:25 Configuring ssm agent with target id: codebuild:990aedcc-a2db-4d48-84f7-9a27f290cd27\n",
      "[Container] 2023/05/22 12:22:25 Successfully updated ssm agent configuration\n",
      "[Container] 2023/05/22 12:22:25 Registering with agent\n",
      "[Container] 2023/05/22 12:22:25 Phases found in YAML: 3\n",
      "[Container] 2023/05/22 12:22:25  PRE_BUILD: 9 commands\n",
      "[Container] 2023/05/22 12:22:25  BUILD: 4 commands\n",
      "[Container] 2023/05/22 12:22:25  POST_BUILD: 3 commands\n",
      "[Container] 2023/05/22 12:22:25 Phase complete: DOWNLOAD_SOURCE State: SUCCEEDED\n",
      "[Container] 2023/05/22 12:22:25 Phase context status code:  Message:\n",
      "[Container] 2023/05/22 12:22:25 Entering phase INSTALL\n",
      "[Container] 2023/05/22 12:22:25 Phase complete: INSTALL State: SUCCEEDED\n",
      "[Container] 2023/05/22 12:22:25 Phase context status code:  Message:\n",
      "[Container] 2023/05/22 12:22:25 Entering phase PRE_BUILD\n",
      "[Container] 2023/05/22 12:22:25 Running command echo Logging in to Amazon ECR...\n",
      "Logging in to Amazon ECR...\n",
      "\n",
      "[Container] 2023/05/22 12:22:25 Running command $(aws ecr get-login --no-include-email --region $AWS_DEFAULT_REGION)\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /root/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "\n",
      "[Container] 2023/05/22 12:22:25 Running command $(aws ecr get-login --no-include-email --region $AWS_DEFAULT_REGION --registry-ids 763104351884)\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /root/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "\n",
      "[Container] 2023/05/22 12:22:26 Running command $(aws ecr get-login --no-include-email --region $AWS_DEFAULT_REGION --registry-ids 217643126080)\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /root/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "\n",
      "[Container] 2023/05/22 12:22:26 Running command $(aws ecr get-login --no-include-email --region $AWS_DEFAULT_REGION --registry-ids 727897471807)\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /root/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "\n",
      "[Container] 2023/05/22 12:22:27 Running command $(aws ecr get-login --no-include-email --region $AWS_DEFAULT_REGION --registry-ids 626614931356)\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /root/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "\n",
      "[Container] 2023/05/22 12:22:27 Running command $(aws ecr get-login --no-include-email --region $AWS_DEFAULT_REGION --registry-ids 683313688378)\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /root/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "\n",
      "[Container] 2023/05/22 12:22:28 Running command $(aws ecr get-login --no-include-email --region $AWS_DEFAULT_REGION --registry-ids 520713654638)\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /root/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "\n",
      "[Container] 2023/05/22 12:22:29 Running command $(aws ecr get-login --no-include-email --region $AWS_DEFAULT_REGION --registry-ids 462105765813)\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /root/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "\n",
      "[Container] 2023/05/22 12:22:30 Phase complete: PRE_BUILD State: SUCCEEDED\n",
      "[Container] 2023/05/22 12:22:30 Phase context status code:  Message:\n",
      "[Container] 2023/05/22 12:22:30 Entering phase BUILD\n",
      "[Container] 2023/05/22 12:22:30 Running command echo Build started on `date`\n",
      "Build started on Mon May 22 12:22:30 UTC 2023\n",
      "\n",
      "[Container] 2023/05/22 12:22:30 Running command echo Building the Docker image...\n",
      "Building the Docker image...\n",
      "\n",
      "[Container] 2023/05/22 12:22:30 Running command docker build -t $IMAGE_REPO_NAME:$IMAGE_TAG . --build-arg BASE_IMAGE=763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-inference:1.10-transformers4.17-gpu-py38-cu113-ubuntu20.04\n",
      "Sending build context to Docker daemon  14.34kB\n",
      "Step 1/11 : ARG BASE_IMAGE\n",
      "Step 2/11 : FROM ${BASE_IMAGE}\n",
      "1.10-transformers4.17-gpu-py38-cu113-ubuntu20.04: Pulling from huggingface-pytorch-inference\n",
      "d5fd17ec1767: Pulling fs layer\n",
      "602a45a9c0c5: Pulling fs layer\n",
      "e1bae4c1f40f: Pulling fs layer\n",
      "d9d586ab2510: Pulling fs layer\n",
      "2b44adc78060: Pulling fs layer\n",
      "4661d2000a2b: Pulling fs layer\n",
      "35df3dae375c: Pulling fs layer\n",
      "251b562af5ee: Pulling fs layer\n",
      "4777f9177f23: Pulling fs layer\n",
      "13aee822afa1: Pulling fs layer\n",
      "05d325eb80b4: Pulling fs layer\n",
      "7db49f926d6d: Pulling fs layer\n",
      "d9d586ab2510: Waiting\n",
      "d03f7b84fec8: Pulling fs layer\n",
      "14576d6c5f79: Pulling fs layer\n",
      "2b44adc78060: Waiting\n",
      "349de6509daa: Pulling fs layer\n",
      "0c8e79862051: Pulling fs layer\n",
      "590505fcb4db: Pulling fs layer\n",
      "746149b30c68: Pulling fs layer\n",
      "aa08096ce9d7: Pulling fs layer\n",
      "4661d2000a2b: Waiting\n",
      "bc5ed5ce5bd2: Pulling fs layer\n",
      "0f7c567c85bd: Pulling fs layer\n",
      "b8b80bf3671c: Pulling fs layer\n",
      "ea7db7615731: Pulling fs layer\n",
      "65a9027471c2: Pulling fs layer\n",
      "2b47564ffeba: Pulling fs layer\n",
      "1256480abccc: Pulling fs layer\n",
      "4777f9177f23: Waiting\n",
      "35df3dae375c: Waiting\n",
      "251b562af5ee: Waiting\n",
      "13aee822afa1: Waiting\n",
      "349de6509daa: Waiting\n",
      "05d325eb80b4: Waiting\n",
      "aa08096ce9d7: Waiting\n",
      "7db49f926d6d: Waiting\n",
      "0c8e79862051: Waiting\n",
      "590505fcb4db: Waiting\n",
      "bc5ed5ce5bd2: Waiting\n",
      "d03f7b84fec8: Waiting\n",
      "746149b30c68: Waiting\n",
      "0f7c567c85bd: Waiting\n",
      "14576d6c5f79: Waiting\n",
      "ea7db7615731: Waiting\n",
      "65a9027471c2: Waiting\n",
      "2b47564ffeba: Waiting\n",
      "1256480abccc: Waiting\n",
      "602a45a9c0c5: Verifying Checksum\n",
      "602a45a9c0c5: Download complete\n",
      "d9d586ab2510: Verifying Checksum\n",
      "d9d586ab2510: Download complete\n",
      "e1bae4c1f40f: Verifying Checksum\n",
      "e1bae4c1f40f: Download complete\n",
      "2b44adc78060: Download complete\n",
      "d5fd17ec1767: Verifying Checksum\n",
      "d5fd17ec1767: Download complete\n",
      "4661d2000a2b: Verifying Checksum\n",
      "4661d2000a2b: Download complete\n",
      "d5fd17ec1767: Pull complete\n",
      "602a45a9c0c5: Pull complete\n",
      "e1bae4c1f40f: Pull complete\n",
      "d9d586ab2510: Pull complete\n",
      "2b44adc78060: Pull complete\n",
      "4661d2000a2b: Pull complete\n",
      "4777f9177f23: Verifying Checksum\n",
      "4777f9177f23: Download complete\n",
      "13aee822afa1: Download complete\n",
      "05d325eb80b4: Download complete\n",
      "7db49f926d6d: Verifying Checksum\n",
      "7db49f926d6d: Download complete\n",
      "d03f7b84fec8: Verifying Checksum\n",
      "d03f7b84fec8: Download complete\n",
      "251b562af5ee: Verifying Checksum\n",
      "251b562af5ee: Download complete\n",
      "349de6509daa: Verifying Checksum\n",
      "349de6509daa: Download complete\n",
      "0c8e79862051: Verifying Checksum\n",
      "0c8e79862051: Download complete\n",
      "590505fcb4db: Verifying Checksum\n",
      "590505fcb4db: Download complete\n",
      "746149b30c68: Verifying Checksum\n",
      "746149b30c68: Download complete\n",
      "aa08096ce9d7: Verifying Checksum\n",
      "aa08096ce9d7: Download complete\n",
      "14576d6c5f79: Verifying Checksum\n",
      "14576d6c5f79: Download complete\n",
      "bc5ed5ce5bd2: Verifying Checksum\n",
      "bc5ed5ce5bd2: Download complete\n",
      "0f7c567c85bd: Verifying Checksum\n",
      "0f7c567c85bd: Download complete\n",
      "b8b80bf3671c: Verifying Checksum\n",
      "b8b80bf3671c: Download complete\n",
      "65a9027471c2: Verifying Checksum\n",
      "65a9027471c2: Download complete\n",
      "2b47564ffeba: Verifying Checksum\n",
      "2b47564ffeba: Download complete\n",
      "1256480abccc: Verifying Checksum\n",
      "1256480abccc: Download complete\n",
      "ea7db7615731: Verifying Checksum\n",
      "ea7db7615731: Download complete\n",
      "35df3dae375c: Verifying Checksum\n",
      "35df3dae375c: Download complete\n",
      "35df3dae375c: Pull complete\n",
      "251b562af5ee: Pull complete\n",
      "4777f9177f23: Pull complete\n",
      "13aee822afa1: Pull complete\n",
      "05d325eb80b4: Pull complete\n",
      "7db49f926d6d: Pull complete\n",
      "d03f7b84fec8: Pull complete\n",
      "14576d6c5f79: Pull complete\n",
      "349de6509daa: Pull complete\n",
      "0c8e79862051: Pull complete\n",
      "590505fcb4db: Pull complete\n",
      "746149b30c68: Pull complete\n",
      "aa08096ce9d7: Pull complete\n",
      "bc5ed5ce5bd2: Pull complete\n",
      "0f7c567c85bd: Pull complete\n",
      "b8b80bf3671c: Pull complete\n",
      "ea7db7615731: Pull complete\n",
      "65a9027471c2: Pull complete\n",
      "2b47564ffeba: Pull complete\n",
      "1256480abccc: Pull complete\n",
      "Digest: sha256:17e776fd3295cc6dfee4e122618f5bab7ef04e87ed0490ce6b64722a60f03333\n",
      "Status: Downloaded newer image for 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-inference:1.10-transformers4.17-gpu-py38-cu113-ubuntu20.04\n",
      " ---> 27b277000343\n",
      "Step 3/11 : RUN apt-get update -y && apt-get install -y --no-install-recommends build-essential gcc                                         libsndfile1\n",
      " ---> Running in 5c8ad429b473\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal InRelease [265 kB]\n",
      "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease [1581 B]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
      "Get:5 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu focal InRelease [23.8 kB]\n",
      "Get:6 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
      "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  Packages [1010 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu focal/universe amd64 Packages [11.3 MB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu focal/restricted amd64 Packages [33.4 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu focal/main amd64 Packages [1275 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu focal/multiverse amd64 Packages [177 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1344 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [2408 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [31.2 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3197 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu focal-backports/main amd64 Packages [55.2 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu focal-backports/universe amd64 Packages [28.6 kB]\n",
      "Get:18 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu focal/main amd64 Packages [16.5 kB]\n",
      "Get:19 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2699 kB]\n",
      "Get:20 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1049 kB]\n",
      "Get:21 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [28.5 kB]\n",
      "Get:22 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [2240 kB]\n",
      "Fetched 27.6 MB in 2s (17.4 MB/s)\n",
      "Reading package lists...\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "gcc is already the newest version (4:9.3.0-1ubuntu2).\n",
      "gcc set to manually installed.\n",
      "build-essential is already the newest version (12.8ubuntu1.1).\n",
      "libsndfile1 is already the newest version (1.0.28-7ubuntu0.1).\n",
      "libsndfile1 set to manually installed.\n",
      "0 upgraded, 0 newly installed, 0 to remove and 101 not upgraded.\n",
      "Removing intermediate container 5c8ad429b473\n",
      " ---> 071b9da5692f\n",
      "Step 4/11 : RUN pip install SoundFile\n",
      " ---> Running in e873b8e9b2c6\n",
      "Requirement already satisfied: SoundFile in /opt/conda/lib/python3.8/site-packages (0.11.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.8/site-packages (from SoundFile) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.0->SoundFile) (2.21)\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.1.2\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mRemoving intermediate container e873b8e9b2c6\n",
      " ---> 63c060056ff3\n",
      "Step 5/11 : RUN pip install librosa\n",
      " ---> Running in e28aadf70489\n",
      "Requirement already satisfied: librosa in /opt/conda/lib/python3.8/site-packages (0.9.2)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /opt/conda/lib/python3.8/site-packages (from librosa) (1.23.3)\n",
      "Requirement already satisfied: soundfile>=0.10.2 in /opt/conda/lib/python3.8/site-packages (from librosa) (0.11.0)\n",
      "Requirement already satisfied: decorator>=4.0.10 in /opt/conda/lib/python3.8/site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /opt/conda/lib/python3.8/site-packages (from librosa) (3.0.0)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /opt/conda/lib/python3.8/site-packages (from librosa) (1.1.2)\n",
      "Requirement already satisfied: numba>=0.45.1 in /opt/conda/lib/python3.8/site-packages (from librosa) (0.56.2)\n",
      "Requirement already satisfied: joblib>=0.14 in /opt/conda/lib/python3.8/site-packages (from librosa) (1.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from librosa) (21.3)\n",
      "Requirement already satisfied: pooch>=1.0 in /opt/conda/lib/python3.8/site-packages (from librosa) (1.6.0)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /opt/conda/lib/python3.8/site-packages (from librosa) (1.9.1)\n",
      "Requirement already satisfied: resampy>=0.2.2 in /opt/conda/lib/python3.8/site-packages (from librosa) (0.4.2)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.8/site-packages (from numba>=0.45.1->librosa) (5.0.0)\n",
      "Requirement already satisfied: setuptools<60 in /opt/conda/lib/python3.8/site-packages (from numba>=0.45.1->librosa) (59.8.0)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /opt/conda/lib/python3.8/site-packages (from numba>=0.45.1->librosa) (0.39.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->librosa) (3.0.9)\n",
      "Requirement already satisfied: appdirs>=1.3.0 in /opt/conda/lib/python3.8/site-packages (from pooch>=1.0->librosa) (1.4.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.8/site-packages (from pooch>=1.0->librosa) (2.28.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=0.19.1->librosa) (3.1.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.8/site-packages (from soundfile>=0.10.2->librosa) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.21)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.26.11)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata->numba>=0.45.1->librosa) (3.8.1)\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.1.2\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mRemoving intermediate container e28aadf70489\n",
      " ---> b6a2dea214a1\n",
      "Step 6/11 : RUN pip install \"amazon-textract-response-parser>=0.1,<0.2\" \"Pillow>=8,<9\"     && PT_VER=`pip show torch | grep 'Version:' | sed 's/Version: //'`     && pip install git+https://github.com/facebookresearch/detectron2.git setuptools==59.5.0         torch==$PT_VER \"torchvision>=0.11.3,<0.15\" \"datasets>=2.4,<3\" \"protobuf<3.21\"         \"transformers>=4.25,<4.27\"\n",
      " ---> Running in 160b2490bcc7\n",
      "Collecting amazon-textract-response-parser<0.2,>=0.1\n",
      "  Downloading amazon_textract_response_parser-0.1.46-py2.py3-none-any.whl (29 kB)\n",
      "Collecting Pillow<9,>=8\n",
      "  Downloading Pillow-8.4.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 96.9 MB/s eta 0:00:00\n",
      "Collecting marshmallow<4,>=3.14\n",
      "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 49.1/49.1 kB 12.5 MB/s eta 0:00:00\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.26.137-py3-none-any.whl (135 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 135.6/135.6 kB 28.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.8/site-packages (from marshmallow<4,>=3.14->amazon-textract-response-parser<0.2,>=0.1) (21.3)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.8/site-packages (from boto3->amazon-textract-response-parser<0.2,>=0.1) (1.0.1)\n",
      "Collecting s3transfer<0.7.0,>=0.6.0\n",
      "  Downloading s3transfer-0.6.1-py3-none-any.whl (79 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79.8/79.8 kB 26.2 MB/s eta 0:00:00\n",
      "Collecting botocore<1.30.0,>=1.29.137\n",
      "  Downloading botocore-1.29.137-py3-none-any.whl (10.8 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.8/10.8 MB 112.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.8/site-packages (from botocore<1.30.0,>=1.29.137->boto3->amazon-textract-response-parser<0.2,>=0.1) (1.26.11)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.8/site-packages (from botocore<1.30.0,>=1.29.137->boto3->amazon-textract-response-parser<0.2,>=0.1) (2.8.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=17.0->marshmallow<4,>=3.14->amazon-textract-response-parser<0.2,>=0.1) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.137->boto3->amazon-textract-response-parser<0.2,>=0.1) (1.16.0)\n",
      "Installing collected packages: Pillow, marshmallow, botocore, s3transfer, boto3, amazon-textract-response-parser\n",
      "  Attempting uninstall: Pillow\n",
      "    Found existing installation: Pillow 9.2.0\n",
      "    Uninstalling Pillow-9.2.0:\n",
      "      Successfully uninstalled Pillow-9.2.0\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.27.84\n",
      "    Uninstalling botocore-1.27.84:\n",
      "      Successfully uninstalled botocore-1.27.84\n",
      "Successfully installed Pillow-8.4.0 amazon-textract-response-parser-0.1.46 boto3-1.26.137 botocore-1.29.137 marshmallow-3.19.0 s3transfer-0.6.1\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.1.2\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mCollecting git+https://github.com/facebookresearch/detectron2.git\n",
      "  Cloning https://github.com/facebookresearch/detectron2.git to /home/model-server/tmp/pip-req-build-prkb3ztv\n",
      "\u001b[91m  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /home/model-server/tmp/pip-req-build-prkb3ztv\n",
      "\u001b[0m  Resolved https://github.com/facebookresearch/detectron2.git to commit 2c6c380f94a27bd8455a39506c9105f652b9f760\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting setuptools==59.5.0\n",
      "  Downloading setuptools-59.5.0-py3-none-any.whl (952 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 952.4/952.4 kB 44.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: torch==1.10.2+cu113 in /opt/conda/lib/python3.8/site-packages (1.10.2+cu113)\n",
      "Collecting torchvision<0.15,>=0.11.3\n",
      "  Downloading torchvision-0.14.1-cp38-cp38-manylinux1_x86_64.whl (24.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 24.2/24.2 MB 83.5 MB/s eta 0:00:00\n",
      "Collecting datasets<3,>=2.4\n",
      "  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 474.6/474.6 kB 76.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: protobuf<3.21 in /opt/conda/lib/python3.8/site-packages (3.19.6)\n",
      "Collecting transformers<4.27,>=4.25\n",
      "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.3/6.3 MB 103.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch==1.10.2+cu113) (4.3.0)\n",
      "Requirement already satisfied: Pillow>=7.1 in /opt/conda/lib/python3.8/site-packages (from detectron2==0.6) (8.4.0)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.7.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.2/9.2 MB 139.1 MB/s eta 0:00:00\n",
      "Collecting pycocotools>=2.0.2\n",
      "  Downloading pycocotools-2.0.6.tar.gz (24 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting termcolor>=1.1\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Collecting yacs>=0.1.8\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: tabulate in /opt/conda/lib/python3.8/site-packages (from detectron2==0.6) (0.8.10)\n",
      "Collecting cloudpickle\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: tqdm>4.29.0 in /opt/conda/lib/python3.8/site-packages (from detectron2==0.6) (4.64.1)\n",
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/5.6 MB 98.2 MB/s eta 0:00:00\n",
      "Collecting fvcore<0.1.6,>=0.1.5\n",
      "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.2/50.2 kB 17.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting iopath<0.1.10,>=0.1.7\n",
      "  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
      "Collecting omegaconf>=2.1\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79.5/79.5 kB 18.4 MB/s eta 0:00:00\n",
      "Collecting hydra-core>=1.1\n",
      "  Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 154.5/154.5 kB 47.1 MB/s eta 0:00:00\n",
      "Collecting black\n",
      "  Downloading black-23.3.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 123.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from detectron2==0.6) (21.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from torchvision<0.15,>=0.11.3) (2.28.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from torchvision<0.15,>=0.11.3) (1.23.3)\n",
      "Collecting torchvision<0.15,>=0.11.3\n",
      "  Downloading torchvision-0.14.0-cp38-cp38-manylinux1_x86_64.whl (24.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 24.3/24.3 MB 82.1 MB/s eta 0:00:00\n",
      "  Downloading torchvision-0.13.1-cp38-cp38-manylinux1_x86_64.whl (19.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.1/19.1 MB 105.5 MB/s eta 0:00:00\n",
      "  Downloading torchvision-0.13.0-cp38-cp38-manylinux1_x86_64.whl (19.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.1/19.1 MB 121.1 MB/s eta 0:00:00\n",
      "  Downloading torchvision-0.12.0-cp38-cp38-manylinux1_x86_64.whl (21.0 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.0/21.0 MB 106.1 MB/s eta 0:00:00\n",
      "  Downloading torchvision-0.11.3-cp38-cp38-manylinux1_x86_64.whl (23.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 23.2/23.2 MB 92.6 MB/s eta 0:00:00\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 117.7 MB/s eta 0:00:00\n",
      "Collecting dill<0.3.7,>=0.3.0\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 110.5/110.5 kB 37.4 MB/s eta 0:00:00\n",
      "Collecting huggingface-hub<1.0.0,>=0.11.0\n",
      "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 224.5/224.5 kB 59.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from datasets<3,>=2.4) (6.0)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 213.0/213.0 kB 55.0 MB/s eta 0:00:00\n",
      "Collecting fsspec[http]>=2021.11.1\n",
      "  Downloading fsspec-2023.5.0-py3-none-any.whl (160 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 160.1/160.1 kB 38.8 MB/s eta 0:00:00\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 132.0/132.0 kB 38.6 MB/s eta 0:00:00\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.3/12.3 MB 58.7 MB/s eta 0:00:00\n",
      "Collecting pyarrow>=8.0.0\n",
      "  Downloading pyarrow-12.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.0 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 39.0/39.0 MB 67.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers<4.27,>=4.25) (2022.9.13)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.8/site-packages (from transformers<4.27,>=4.25) (0.13.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers<4.27,>=4.25) (3.8.0)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.9.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (266 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 266.9/266.9 kB 60.0 MB/s eta 0:00:00\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 161.3/161.3 kB 43.3 MB/s eta 0:00:00\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.3/121.3 kB 39.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets<3,>=2.4) (2.0.12)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets<3,>=2.4) (22.1.0)\n",
      "Collecting antlr4-python3-runtime==4.9.*\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 117.0/117.0 kB 35.1 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: importlib-resources in /opt/conda/lib/python3.8/site-packages (from hydra-core>=1.1->detectron2==0.6) (5.9.0)\n",
      "Collecting portalocker\n",
      "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->detectron2==0.6) (3.0.9)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.0.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 300.0/300.0 kB 64.6 MB/s eta 0:00:00\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.39.4-py3-none-any.whl (1.0 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 98.1 MB/s eta 0:00:00\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.4-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 119.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.8/site-packages (from matplotlib->detectron2==0.6) (2.8.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision<0.15,>=0.11.3) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision<0.15,>=0.11.3) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision<0.15,>=0.11.3) (1.26.11)\n",
      "Collecting platformdirs>=2\n",
      "  Downloading platformdirs-3.5.1-py3-none-any.whl (15 kB)\n",
      "Collecting tomli>=1.1.0\n",
      "  Downloading tomli-2.0.1-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.8/site-packages (from black->detectron2==0.6) (8.1.3)\n",
      "Collecting mypy-extensions>=0.4.3\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Collecting packaging\n",
      "  Downloading packaging-23.1-py3-none-any.whl (48 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 48.9/48.9 kB 13.8 MB/s eta 0:00:00\n",
      "Collecting pathspec>=0.9.0\n",
      "  Downloading pathspec-0.11.1-py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets<3,>=2.4) (2022.4)\n",
      "Collecting tzdata>=2022.1\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 341.8/341.8 kB 67.0 MB/s eta 0:00:00\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading Werkzeug-2.3.4-py3-none-any.whl (242 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 242.5/242.5 kB 60.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.8/site-packages (from tensorboard->detectron2==0.6) (0.37.1)\n",
      "Collecting grpcio>=1.48.2\n",
      "  Downloading grpcio-1.54.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.1/5.1 MB 137.3 MB/s eta 0:00:00\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 93.9/93.9 kB 23.2 MB/s eta 0:00:00\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.18.1-py2.py3-none-any.whl (178 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 178.9/178.9 kB 38.7 MB/s eta 0:00:00\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.0-py3-none-manylinux2014_x86_64.whl (6.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 130.2 MB/s eta 0:00:00\n",
      "Collecting absl-py>=0.4\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 126.5/126.5 kB 37.1 MB/s eta 0:00:00\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 181.3/181.3 kB 45.3 MB/s eta 0:00:00\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (1.16.0)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.8/site-packages (from importlib-resources->hydra-core>=1.1->detectron2==0.6) (3.8.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard->detectron2==0.6) (5.0.0)\n",
      "Collecting MarkupSafe>=2.1.1\n",
      "  Downloading MarkupSafe-2.1.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 83.9/83.9 kB 21.1 MB/s eta 0:00:00\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.7/151.7 kB 38.2 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: detectron2, fvcore, antlr4-python3-runtime, pycocotools\n",
      "  Building wheel for detectron2 (setup.py): started\n",
      "  Building wheel for detectron2 (setup.py): finished with status 'done'\n",
      "  Created wheel for detectron2: filename=detectron2-0.6-cp38-cp38-linux_x86_64.whl size=6427420 sha256=5dd666e0a35046b5a50402cd11f01cab3fd152f0298397d09f1e5e5efa2e9b22\n",
      "  Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-0x3u9_qh/wheels/19/ac/65/e48e5e4ec2702274d927c5a6efb75709b24014371d3bb778f2\n",
      "  Building wheel for fvcore (setup.py): started\n",
      "  Building wheel for fvcore (setup.py): finished with status 'done'\n",
      "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61431 sha256=d8c2e9d566068a76a81e7fdf2e2bf98e4e45b3e8cc51a695b889b7711b351960\n",
      "  Stored in directory: /root/.cache/pip/wheels/b8/79/07/c0e9367f5b5ea325e246bd73651e8af175fabbef943043b1cc\n",
      "  Building wheel for antlr4-python3-runtime (setup.py): started\n",
      "  Building wheel for antlr4-python3-runtime (setup.py): finished with status 'done'\n",
      "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144575 sha256=dd2d18ad7e3748a94e9385faa728b73a548842995ce06cfa0adec5c4ac7f72e2\n",
      "  Stored in directory: /root/.cache/pip/wheels/b1/a3/c2/6df046c09459b73cc9bb6c4401b0be6c47048baf9a1617c485\n",
      "  Building wheel for pycocotools (pyproject.toml): started\n",
      "  Building wheel for pycocotools (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pycocotools: filename=pycocotools-2.0.6-cp38-cp38-linux_x86_64.whl size=425678 sha256=0ccbf1b78d6b5147f1764699c0df21838a58017bfe9e79815b5d9b0e7ef604e8\n",
      "  Stored in directory: /root/.cache/pip/wheels/3e/08/ac/58126fe59992032701437336493f6132e1b72381a62d00b595\n",
      "Successfully built detectron2 fvcore antlr4-python3-runtime pycocotools\n",
      "Installing collected packages: antlr4-python3-runtime, yacs, xxhash, tzdata, tomli, termcolor, tensorboard-data-server, setuptools, pyasn1, pyarrow, portalocker, platformdirs, pathspec, packaging, omegaconf, oauthlib, mypy-extensions, multidict, MarkupSafe, kiwisolver, grpcio, fsspec, frozenlist, fonttools, dill, cycler, contourpy, cloudpickle, cachetools, async-timeout, absl-py, yarl, werkzeug, torchvision, rsa, responses, requests-oauthlib, pyasn1-modules, pandas, multiprocess, matplotlib, markdown, iopath, hydra-core, huggingface-hub, black, aiosignal, transformers, pycocotools, google-auth, fvcore, aiohttp, google-auth-oauthlib, tensorboard, datasets, detectron2\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 59.8.0\n",
      "    Uninstalling setuptools-59.8.0:\n",
      "      Successfully uninstalled setuptools-59.8.0\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 21.3\n",
      "    Uninstalling packaging-21.3:\n",
      "      Successfully uninstalled packaging-21.3\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.10.0\n",
      "    Uninstalling huggingface-hub-0.10.0:\n",
      "      Successfully uninstalled huggingface-hub-0.10.0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.17.0\n",
      "    Uninstalling transformers-4.17.0:\n",
      "      Successfully uninstalled transformers-4.17.0\n",
      "Successfully installed MarkupSafe-2.1.2 absl-py-1.4.0 aiohttp-3.8.4 aiosignal-1.3.1 antlr4-python3-runtime-4.9.3 async-timeout-4.0.2 black-23.3.0 cachetools-5.3.0 cloudpickle-2.2.1 contourpy-1.0.7 cycler-0.11.0 datasets-2.12.0 detectron2-0.6 dill-0.3.6 fonttools-4.39.4 frozenlist-1.3.3 fsspec-2023.5.0 fvcore-0.1.5.post20221221 google-auth-2.18.1 google-auth-oauthlib-1.0.0 grpcio-1.54.2 huggingface-hub-0.14.1 hydra-core-1.3.2 iopath-0.1.9 kiwisolver-1.4.4 markdown-3.4.3 matplotlib-3.7.1 multidict-6.0.4 multiprocess-0.70.14 mypy-extensions-1.0.0 oauthlib-3.2.2 omegaconf-2.3.0 packaging-23.1 pandas-2.0.1 pathspec-0.11.1 platformdirs-3.5.1 portalocker-2.7.0 pyarrow-12.0.0 pyasn1-0.5.0 pyasn1-modules-0.3.0 pycocotools-2.0.6 requests-oauthlib-1.3.1 responses-0.18.0 rsa-4.9 setuptools-59.5.0 tensorboard-2.13.0 tensorboard-data-server-0.7.0 termcolor-2.3.0 tomli-2.0.1 torchvision-0.11.3 transformers-4.26.1 tzdata-2023.3 werkzeug-2.3.4 xxhash-3.2.0 yacs-0.1.8 yarl-1.9.2\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.1.2\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mRemoving intermediate container 160b2490bcc7\n",
      " ---> 14885abd0e03\n",
      "Step 7/11 : RUN PT_VER=`pip show torch | grep 'Version:' | sed 's/Version: //'`     && pip install pytesseract torch==$PT_VER\n",
      " ---> Running in ddc0d7eb7cc8\n",
      "Collecting pytesseract\n",
      "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: torch==1.10.2+cu113 in /opt/conda/lib/python3.8/site-packages (1.10.2+cu113)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch==1.10.2+cu113) (4.3.0)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in /opt/conda/lib/python3.8/site-packages (from pytesseract) (8.4.0)\n",
      "Requirement already satisfied: packaging>=21.3 in /opt/conda/lib/python3.8/site-packages (from pytesseract) (23.1)\n",
      "Installing collected packages: pytesseract\n",
      "Successfully installed pytesseract-0.3.10\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.1.2\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mRemoving intermediate container ddc0d7eb7cc8\n",
      " ---> 845c6af752bc\n",
      "Step 8/11 : ARG INCLUDE_NOTEBOOK_KERNEL\n",
      " ---> Running in 07e31a9d04a3\n",
      "Removing intermediate container 07e31a9d04a3\n",
      " ---> 40d6a2e2638f\n",
      "Step 9/11 : RUN if test -z \"$INCLUDE_NOTEBOOK_KERNEL\" ;     then         echo Skipping notebook kernel dependencies     ; else         conda install -y -c conda-forge poppler tesseract &&         PT_VER=`pip show torch | grep 'Version:' | sed 's/Version: //'` &&         pip install easyocr ipykernel \"ipywidgets>=7,<8\" pdf2image pytesseract sagemaker             torch==$PT_VER &&         export TESSDATA_PREFIX='/opt/conda/share/tessdata' &&         python -m ipykernel install --sys-prefix     ; fi\n",
      " ---> Running in abde18b8714f\n",
      "Skipping notebook kernel dependencies\n",
      "Removing intermediate container abde18b8714f\n",
      " ---> faff1829cc42\n",
      "Step 10/11 : ENV USE_SMDEBUG=${INCLUDE_NOTEBOOK_KERNEL:+false}\n",
      " ---> Running in 8a364275f7b9\n",
      "Removing intermediate container 8a364275f7b9\n",
      " ---> 287b0dc59cef\n",
      "Step 11/11 : ENV USE_SMDEBUG=${USE_SMDEBUG:-true}\n",
      " ---> Running in 8ed4efa6ef8f\n",
      "Removing intermediate container 8ed4efa6ef8f\n",
      " ---> dae1597040b4\n",
      "Successfully built dae1597040b4\n",
      "Successfully tagged sm-ocr-inference:hf-4.26-pt-gpu\n",
      "\n",
      "[Container] 2023/05/22 12:27:17 Running command docker tag $IMAGE_REPO_NAME:$IMAGE_TAG $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME:$IMAGE_TAG\n",
      "\n",
      "[Container] 2023/05/22 12:27:17 Phase complete: BUILD State: SUCCEEDED\n",
      "[Container] 2023/05/22 12:27:17 Phase context status code:  Message:\n",
      "[Container] 2023/05/22 12:27:17 Entering phase POST_BUILD\n",
      "[Container] 2023/05/22 12:27:17 Running command echo Build completed on `date`\n",
      "Build completed on Mon May 22 12:27:17 UTC 2023\n",
      "\n",
      "[Container] 2023/05/22 12:27:17 Running command echo Pushing the Docker image...\n",
      "Pushing the Docker image...\n",
      "\n",
      "[Container] 2023/05/22 12:27:17 Running command docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME:$IMAGE_TAG\n",
      "The push refers to repository [015943506230.dkr.ecr.us-east-1.amazonaws.com/sm-ocr-inference]\n",
      "d678a3859d98: Preparing\n",
      "64fe79fe4238: Preparing\n",
      "a02c65a512a7: Preparing\n",
      "f7a487f3a55c: Preparing\n",
      "53266df13189: Preparing\n",
      "3ba6fe4b2b66: Preparing\n",
      "b0134fef65ae: Preparing\n",
      "1bfa98cef2c8: Preparing\n",
      "b8bcd343d9df: Preparing\n",
      "83e6952e8e57: Preparing\n",
      "721041a08a77: Preparing\n",
      "ca548dd9f6b6: Preparing\n",
      "ac0871e16dd1: Preparing\n",
      "981540c7bf62: Preparing\n",
      "57c2137bc54a: Preparing\n",
      "553f00fe9414: Preparing\n",
      "1bfa98cef2c8: Waiting\n",
      "3ba6fe4b2b66: Waiting\n",
      "c290b09be722: Preparing\n",
      "b0134fef65ae: Waiting\n",
      "2e36dd432e28: Preparing\n",
      "b8bcd343d9df: Waiting\n",
      "335dafe074cd: Preparing\n",
      "ac0871e16dd1: Waiting\n",
      "83e6952e8e57: Waiting\n",
      "721041a08a77: Waiting\n",
      "8480a7f24d40: Preparing\n",
      "981540c7bf62: Waiting\n",
      "57c2137bc54a: Waiting\n",
      "ca548dd9f6b6: Waiting\n",
      "a688b15f64fd: Preparing\n",
      "553f00fe9414: Waiting\n",
      "542221373db2: Preparing\n",
      "9295157e90fd: Preparing\n",
      "c290b09be722: Waiting\n",
      "335dafe074cd: Waiting\n",
      "0430d270e456: Preparing\n",
      "d2d399deceb2: Preparing\n",
      "8480a7f24d40: Waiting\n",
      "e995384f1642: Preparing\n",
      "e592fe6d10a9: Preparing\n",
      "a688b15f64fd: Waiting\n",
      "f42691182163: Preparing\n",
      "68016c5bb65c: Preparing\n",
      "8034550a3bbe: Preparing\n",
      "bf8cedc62fb3: Preparing\n",
      "542221373db2: Waiting\n",
      "9295157e90fd: Waiting\n",
      "8034550a3bbe: Waiting\n",
      "f42691182163: Waiting\n",
      "0430d270e456: Waiting\n",
      "68016c5bb65c: Waiting\n",
      "e995384f1642: Waiting\n",
      "bf8cedc62fb3: Waiting\n",
      "a02c65a512a7: Pushed\n",
      "3ba6fe4b2b66: Layer already exists\n",
      "d678a3859d98: Pushed\n",
      "f7a487f3a55c: Pushed\n",
      "1bfa98cef2c8: Layer already exists\n",
      "b0134fef65ae: Layer already exists\n",
      "b8bcd343d9df: Layer already exists\n",
      "83e6952e8e57: Layer already exists\n",
      "721041a08a77: Layer already exists\n",
      "ca548dd9f6b6: Layer already exists\n",
      "ac0871e16dd1: Layer already exists\n",
      "57c2137bc54a: Layer already exists\n",
      "981540c7bf62: Layer already exists\n",
      "553f00fe9414: Layer already exists\n",
      "2e36dd432e28: Layer already exists\n",
      "c290b09be722: Layer already exists\n",
      "8480a7f24d40: Layer already exists\n",
      "335dafe074cd: Layer already exists\n",
      "a688b15f64fd: Layer already exists\n",
      "9295157e90fd: Layer already exists\n",
      "542221373db2: Layer already exists\n",
      "0430d270e456: Layer already exists\n",
      "d2d399deceb2: Layer already exists\n",
      "e995384f1642: Layer already exists\n",
      "e592fe6d10a9: Layer already exists\n",
      "f42691182163: Layer already exists\n",
      "68016c5bb65c: Layer already exists\n",
      "8034550a3bbe: Layer already exists\n",
      "bf8cedc62fb3: Layer already exists\n",
      "53266df13189: Pushed\n",
      "64fe79fe4238: Pushed\n",
      "hf-4.26-pt-gpu: digest: sha256:1b9dc018a0a4a42f0b7c3087477f1db29a192028fae3aa0596413d2c47e10afb size: 6828\n",
      "\n",
      "[Container] 2023/05/22 12:27:51 Phase complete: POST_BUILD State: SUCCEEDED\n",
      "[Container] 2023/05/22 12:27:51 Phase context status code:  Message:\n",
      "\n",
      "Image URI: 015943506230.dkr.ecr.us-east-1.amazonaws.com/sm-ocr-inference:hf-4.26-pt-gpu\n",
      "CPU times: user 7.59 s, sys: 1.22 s, total: 8.82 s\n",
      "Wall time: 8min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# (No need to re-run this cell if your inference image is already in ECR)\n",
    "\n",
    "# Build and push the inference image:\n",
    "!cd custom-containers/train-inf && sm-docker build . \\\n",
    "    --compute-type BUILD_GENERAL1_LARGE \\\n",
    "    --repository {inf_repo_name}:{inf_repo_tag} \\\n",
    "    --role {config.sm_image_build_role} \\\n",
    "    --build-arg BASE_IMAGE={inf_base_uri}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 015943506230.dkr.ecr.us-east-1.amazonaws.com/sm-ocr-training:hf-4.26-pt-gpu\n",
      "Found 015943506230.dkr.ecr.us-east-1.amazonaws.com/sm-ocr-inference:hf-4.26-pt-gpu\n"
     ]
    }
   ],
   "source": [
    "# Check from notebook whether the images were successfully created:\n",
    "ecr = boto3.client(\"ecr\")\n",
    "for repo, tag, uri in (\n",
    "    (train_repo_name, train_repo_tag, train_image_uri),\n",
    "    (inf_repo_name, inf_repo_tag, inf_image_uri)\n",
    "):\n",
    "    imgs_desc = ecr.describe_images(\n",
    "        registryId=account_id,\n",
    "        repositoryName=repo,\n",
    "        imageIds=[{\"imageTag\": tag}],\n",
    "    )\n",
    "    assert len(imgs_desc[\"imageDetails\"]) > 0, f\"Couldn't find ECR image {uri} after build\"\n",
    "    print(f\"Found {uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## (Optional) Self-supervised pre-training\n",
    "\n",
    "You can run the cell below and **skip the rest of this section**, unless you'd like to dive deeper on this topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pretrain = False  # Set this True instead to run pre-training (details below).\n",
    "\n",
    "pretrained_s3_uri = None  # Will be overwritten later if pretrain is enabled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "In many cases, businesses have a great deal more relevant *unlabelled* data available in addition to the manually labeled dataset. For example, you might have many more historical documents available (with OCR results already, or able to be processed with Amazon Textract) than you're reasonably able to annotate entities on - just as we do in the credit cards example!\n",
    "\n",
    "Large-scale language models are typically **pre-trained** to unlabelled data in a **self-supervised** pattern: Teaching the model to predict some implicit task in the data like, for example, masking a few words on the page and predicting what words should go in the gaps.\n",
    "\n",
    "This pre-training doesn't directly teach the model to perform the target task (classifying entities), but forces the core of the model to learn intrinsic patterns in the data. When we then replace the output layers and **fine-tune** towards the target task with human-labelled data, the model is able to learn the target task more effectively.\n",
    "\n",
    "By default, for speed, the configuration below will use a public pre-trained model from the [Hugging Face Transformers Model Hub](https://huggingface.co/models?search=layoutxlm). This allows us to focus immediately on fine-tuning to our task; but also means accuracy may be degraded if our documents are very different from the original corpus the model was trained on.\n",
    "\n",
    "**Alternatively, set `pretrain = True` above** to *further* pre-train this same base public model on your own Textracted (but unlabelled) documents first.\n",
    "\n",
    "> ⚠️ Both these options **use a pre-trained model as a base**: Do check out the licensing and other details for your selected pre-trained `model_name_or_path` on the model hub, as some published models are licensed for non-commercial use only. If you're interested in pre-training your own models from scratch rather than continuation, please let us know on the [existing GitHub issue thread](https://github.com/aws-samples/amazon-textract-transformer-pipeline/issues/19).\n",
    "\n",
    "Pre-training is most likely to be valuable when:\n",
    "\n",
    "1. You have a significantly broader range of data available than the core supervised/annotated dataset (e.g. hundreds to thousands of documents or more are available)\n",
    "2. Your data is usefully *diverse* (millions of nearly-identical proformas may not teach the model very much useful structure, and could pull it away from learning general grammar patterns)\n",
    "3. ...But within an *unusual or specialized domain* (for example with industry jargon or product names, or a language that's less well-represented in the public model's pre-training - like Indonesian in LayoutXLM).\n",
    "4. Understanding these language patterns appears to be a limiting factor on model performance (rather than e.g. being just very strongly constrained by lack of annotations or noise in the annotated data).\n",
    "\n",
    "> ⚠️ If you followed through [Notebook 1](1.%20Data%20Preparation.ipynb) with the default settings to Amazon Textract only a small sample of the documents, you may like to go back, increase `N_DOCS_KEPT`, and Textract some more of the source documents first before trying pre-training.\n",
    "\n",
    "**In our tests with the Credit Card Agreements sample dataset**, LayoutXLM improved from ~68% to ~74% in downstream NER `eval_focus_else_acc_minus_one` by continuation pre-training on the full ~2,541 document corpus, when averaged over different random seed initializations (standard deviations ~3% over random seeds in each configuration). LayoutLMv1 also appeared to consistently benefit from pre-training, but only very slightly at <1% change in focus accuracy.\n",
    "\n",
    "> ⚠️ **Note:** Refer to the [Amazon SageMaker Pricing Page](https://aws.amazon.com/sagemaker/pricing/) for up-to-date guidance before running large pre-training jobs.\n",
    ">\n",
    "> In our tests at the time of writing:\n",
    ">\n",
    "> - Pre-training LayoutXLM on the full ~2,500-document corpus for 25 epochs took around 10 hours on a single-GPU `ml.p3.2xlarge` instance with per-device batch size 2\n",
    "> - Pre-training LayoutLMv1 on the full ~2,500-document corpus for 25 epochs took around 4 hours on an `ml.p3.8xlarge` instance with per-device batch size 4\n",
    "\n",
    "> ℹ️ **Notes on *from-scratch* pre-training:**\n",
    ">\n",
    "> When particularly large and diverse corpora are available relative to what public models have been trained on (especially when working with low-resource languages for example), you might be interested to try from-scratch pre-training rather than continuing from a public model checkpoint.\n",
    ">\n",
    "> To explore this, be aware that:\n",
    "> - The MLM task implemented in this example is simpler than, and may be different from, the full pre-training objective used by most of these models. For example the [LayoutXLM paper](https://arxiv.org/abs/2104.08836) discusses 3 parallel objectives: Extra work would be required to implement these.\n",
    "> - Without a good volume and diversity of documents, your results will likely be poor. Check how your dataset(s) compare to the overall size and diversity of those used for pre-training by your target model's original authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For self-supervised pre-training, you can utilize the full available corpus of Textract-processed documents: Not just the subset of documents and pages you have annotations for. Reserving some documents for validation is still a good idea though, to understand if and when the model starts to over-fit.\n",
    "\n",
    "Arguably, including pages from the entity recognition validation dataset in pre-training constitutes [leakage](https://en.wikipedia.org/wiki/Leakage_(machine_learning)): Because even though we're not including any information about the entity labels the NER model will predict, we're teaching the model information about patterns of content in the hold-out pages.\n",
    "\n",
    "Therefore, the below code takes a conservative view to avoid possibly over-estimating the added benefits of pre-training: Constructing manifests to route *any document with pages in the entity recognition validation set* to also be in the validation set for pre-training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 10 docs to pre-training validation set\n",
      "Added 110 docs to pre-training set\n"
     ]
    }
   ],
   "source": [
    "selfsup_train_manifest_s3uri = f\"s3://{bucket_name}/{bucket_prefix}data/docs-train.manifest.jsonl\"\n",
    "selfsup_val_manifest_s3uri = f\"s3://{bucket_name}/{bucket_prefix}data/docs-val.manifest.jsonl\"\n",
    "\n",
    "# To avoid information leakage, take the validation set = the set of all documents with *any* pages\n",
    "# mentioned in the validation set:\n",
    "val_textract_s3uris = set()\n",
    "with open(\"data/annotations/annotations-test.manifest.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        val_textract_s3uris.add(json.loads(line)[\"textract-ref\"])\n",
    "with open(\"data/docs-val.manifest.jsonl\", \"w\") as f:\n",
    "    for uri in val_textract_s3uris:\n",
    "        f.write(json.dumps({\"textract-ref\": uri}) + \"\\n\")\n",
    "print(f\"Added {len(val_textract_s3uris)} docs to pre-training validation set\")\n",
    "\n",
    "# Any Textracted docs not mentioned in validation can go to training:\n",
    "train_textract_s3uris = set()\n",
    "with open(\"data/textracted-all.manifest.jsonl\", \"r\") as fner:\n",
    "    with open(\"data/docs-train.manifest.jsonl\", \"w\") as f:\n",
    "        for line in fner:\n",
    "            uri = json.loads(line)[\"textract-ref\"]\n",
    "            if (uri in val_textract_s3uris) or (uri in train_textract_s3uris):\n",
    "                continue\n",
    "            else:\n",
    "                train_textract_s3uris.add(uri)\n",
    "                f.write(json.dumps({\"textract-ref\": uri}) + \"\\n\")\n",
    "print(f\"Added {len(train_textract_s3uris)} docs to pre-training set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: Skipping file /root/amazon-textract-transformer-pipeline/notebooks/data/docs-train.manifest.jsonl. File does not exist.\n",
      "                                                              \n",
      "warning: Skipping file /root/amazon-textract-transformer-pipeline/notebooks/data/docs-val.manifest.jsonl. File does not exist.\n",
      "                                                              \n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp data/docs-train.manifest.jsonl {selfsup_train_manifest_s3uri}\n",
    "!aws s3 cp data/docs-val.manifest.jsonl {selfsup_val_manifest_s3uri}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the Amazon Textract JSONs prepared on S3 and split between training and validation via manifests, we're ready to run the pre-training.\n",
    "\n",
    "> ▶️ See the following *Fine-tuning on annotated data* section for more parameter details and links on how model training works in SageMaker - which are omitted here since this section is optional.\n",
    "\n",
    "In general, available hyperparameters are based on the [Hugging Face TrainingArguments parser](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments) with [customizations applied in src/code/config.py](src/code/config.py). See the **\"Scaling and optimizing model training\"** section of the [Customization Guide](../CUSTOMIZATION_GUIDE.md) for more details on adjusting parallelism and instance type/count - which is particularly relevant for self-supervised pre-training on large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    # (See src/code/config.py for more info on script parameters)\n",
    "    \"task_name\": \"mlm\",\n",
    "    \"images_prefix\": imgs_s3uri[len(\"s3://\"):].partition(\"/\")[2],\n",
    "    \"textract_prefix\": textract_s3uri[len(\"s3://\"):].partition(\"/\")[2],\n",
    "\n",
    "    # LayoutXLM multi-lingual model by default. Other tested base models include:\n",
    "    # - LayoutLMv2: \"microsoft/layoutlmv2-base-uncased\"\n",
    "    # - LayoutLMv1: \"microsoft/layoutlm-base-uncased\"\n",
    "    \n",
    "    #\"model_name_or_path\": \"microsoft/layoutxlm-base\",\n",
    "    \"model_name_or_path\": \"microsoft/layoutlm-base-uncased\",   \n",
    "\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"per_device_eval_batch_size\": 4,\n",
    "\n",
    "    \"num_train_epochs\": 25,\n",
    "    \"early_stopping_patience\": 10,\n",
    "    \"metric_for_best_model\": \"eval_loss\",\n",
    "    \"greater_is_better\": \"false\",\n",
    "\n",
    "    # Early stopping implies checkpointing every evaluation (epoch), so limit the total checkpoints\n",
    "    # kept to avoid filling up disk:\n",
    "    \"save_total_limit\": 10,\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "metric_definitions = [\n",
    "    {\"Name\": \"epoch\", \"Regex\": util.training.get_hf_metric_regex(\"epoch\")},\n",
    "    {\"Name\": \"learning_rate\", \"Regex\": util.training.get_hf_metric_regex(\"learning_rate\")},\n",
    "    {\"Name\": \"train:loss\", \"Regex\": util.training.get_hf_metric_regex(\"loss\")},\n",
    "    {\"Name\": \"validation:loss\", \"Regex\": util.training.get_hf_metric_regex(\"eval_loss\")},\n",
    "    {\n",
    "        \"Name\": \"validation:samples_per_sec\",\n",
    "        \"Regex\": util.training.get_hf_metric_regex(\"eval_samples_per_second\"),\n",
    "    },\n",
    "]\n",
    "\n",
    "pre_estimator = HuggingFaceEstimator(\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    # Use \"ddp_launcher.py\" for native PyTorch DDP, \"train.py\" for single-GPU or SageMaker DDP:\n",
    "    entry_point=\"ddp_launcher.py\",\n",
    "    source_dir=\"src\",\n",
    "    py_version=py_version,\n",
    "    pytorch_version=pt_version,\n",
    "    transformers_version=hf_version,\n",
    "    image_uri=train_image_uri,  # Use customized training container image\n",
    "\n",
    "    base_job_name=\"xlm-cfpb-pretrain\",\n",
    "    output_path=f\"s3://{bucket_name}/{bucket_prefix}trainjobs\",\n",
    "\n",
    "    instance_type=\"ml.p3.16xlarge\",  # Or ml.p3.8xlarge, etc.\n",
    "    instance_count=1,\n",
    "    volume_size=150,\n",
    "\n",
    "    debugger_hook_config=False,  # (Required for LayoutLMv2/XLM, not for v1)\n",
    "    # To enable SageMaker DDP (on supported instance types):\n",
    "    # distribution={\"smdistributed\": {\"dataparallel\": {\"enabled\": True}}},\n",
    "\n",
    "    hyperparameters=hyperparameters,\n",
    "    metric_definitions=metric_definitions,\n",
    "    environment={\n",
    "        # Required for our custom dataset loading code (which depends on tokenizer):\n",
    "        \"TOKENIZERS_PARALLELISM\": \"false\",\n",
    "        # May be useful for debugging some DDP issues:\n",
    "        # \"TORCH_DISTRIBUTED_DEBUG\": \"INFO\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if pretrain:\n",
    "    pre_estimator.fit(\n",
    "        inputs={\n",
    "            \"images\": thumbs_s3uri,  # (Can omit this channel with LayoutLMv1 for performance)\n",
    "            \"train\": selfsup_train_manifest_s3uri,\n",
    "            \"textract\": textract_s3uri + \"/\",\n",
    "            \"validation\": selfsup_val_manifest_s3uri,\n",
    "        },\n",
    "        #wait=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the pre-training is complete, fetch the output model S3 URI to use as input for the fine-tuning stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom pre-trained model: None\n"
     ]
    }
   ],
   "source": [
    "if pretrain:\n",
    "    # Un-comment this first line to load an previous pre-training job instead:\n",
    "    # pre_estimator = HuggingFaceEstimator.attach(\"layoutlm-cfpb-pretrain-2021-11-17-01-53-05-786\")\n",
    "\n",
    "    pretraining_job_desc = pre_estimator.latest_training_job.describe()\n",
    "    pretrained_s3_uri = pretraining_job_desc[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
    "\n",
    "print(f\"Custom pre-trained model: {pretrained_s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fine-tuning on annotated data\n",
    "\n",
    "In this section we'll run a [SageMaker Training Job](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html) to fine-tune the model on our annotated dataset.\n",
    "\n",
    "In this process:\n",
    "\n",
    "- SageMaker will run the job on a dedicated, managed instance of type we choose (we'll use `ml.p*` or `ml.g*` GPU-accelerated types), allowing us to keep this notebook's resources modest and only pay for the seconds of GPU time the training job needs.\n",
    "- The data as specified in the manifest files will be downloaded from Amazon S3.\n",
    "- The bundle of scripts we provide (in `src/`) will be transparently uploaded to S3 and then run inside the specified SageMaker-provided [framework container](https://docs.aws.amazon.com/sagemaker/latest/dg/docker-containers-prebuilt.html). There's no need for us to build our own container image or implement a serving stack for inference (although fully-custom containers are [also supported](https://docs.aws.amazon.com/sagemaker/latest/dg/docker-containers.html)).\n",
    "- Job hyperparameters will be passed through to our `src/` scripts as CLI arguments.\n",
    "- SageMaker will analyze the logs from the job (i.e. `print()` or `logger` calls from our script) with the regular expressions specified in `metric_definitions`, to scrape structured timeseries metrics like loss and accuracy.\n",
    "- When the job finishes, the contents of the `model` folder in the container will be automatically tarballed and uploaded to a `model.tar.gz` in Amazon S3.\n",
    "\n",
    "Rather than orchestrating this process through the low-level [SageMaker API](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateTrainingJob.html) (e.g. via [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_training_job)), we'll use the open-source [SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/) (`sagemaker`) for convenience. You can also refer to [Hugging Face's own docs for training on SageMaker](https://huggingface.co/transformers/sagemaker.html) for more information and examples.\n",
    "\n",
    "First, we'll configure some parameters you may **sometimes wish to re-use across training jobs**. Continuation jobs may want to use the same checkpoint location in S3, while from-scratch training should start fresh\n",
    "\n",
    "▶️ You can choose when to re-run this cell between experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoints to collection checkpoints-2023-05-22-12-28-38\n"
     ]
    }
   ],
   "source": [
    "checkpoint_collection_name = \"checkpoints-\" + datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "print(f\"Saving checkpoints to collection {checkpoint_collection_name}\")\n",
    "\n",
    "checkpoint_s3_uri = f\"s3://{bucket_name}/{bucket_prefix}checkpoints/{checkpoint_collection_name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define the core configuration for our training job:\n",
    "\n",
    "▶️ This should usually be re-run for every new training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    # (See src/code/config.py for more info on script parameters)\n",
    "    \"annotation_attr\": standard_label_field,\n",
    "    \"images_prefix\": imgs_s3uri[len(\"s3://\"):].partition(\"/\")[2],\n",
    "    \"textract_prefix\": textract_s3uri[len(\"s3://\"):].partition(\"/\")[2],\n",
    "    \"num_labels\": len(fields) + 1,  # +1 for \"other\"\n",
    "\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"per_device_eval_batch_size\": 4,\n",
    "\n",
    "    \"num_train_epochs\": 150,  # Set high for automatic HP tuning later\n",
    "    \"early_stopping_patience\": 15,  # Usually stops after <25 epochs on this sample data+config\n",
    "    \"metric_for_best_model\": \"eval_focus_else_acc_minus_one\",\n",
    "    \"greater_is_better\": \"true\",\n",
    "\n",
    "    # Early stopping implies checkpointing every evaluation (epoch), so limit the total checkpoints\n",
    "    # kept to avoid filling up disk:\n",
    "    \"save_total_limit\": 10,\n",
    "}\n",
    "if not pretrained_s3_uri:\n",
    "    # LayoutXLM multi-lingual model by default. Other tested base models include:\n",
    "    # - LayoutLMv2: \"microsoft/layoutlmv2-base-uncased\"\n",
    "    # - LayoutLMv1: \"microsoft/layoutlm-base-uncased\"\n",
    "    # See HF model hub for licensing & details of pre-trained models: https://huggingface.co/models\n",
    "    hyperparameters[\"model_name_or_path\"] = \"microsoft/layoutxlm-base\"\n",
    "\n",
    "\n",
    "metric_definitions = [\n",
    "    {\"Name\": \"epoch\", \"Regex\": util.training.get_hf_metric_regex(\"epoch\")},\n",
    "    {\"Name\": \"learning_rate\", \"Regex\": util.training.get_hf_metric_regex(\"learning_rate\")},\n",
    "    {\"Name\": \"train:loss\", \"Regex\": util.training.get_hf_metric_regex(\"loss\")},\n",
    "    {\n",
    "        \"Name\": \"validation:n_examples\",\n",
    "        \"Regex\": util.training.get_hf_metric_regex(\"eval_n_examples\"),\n",
    "    },\n",
    "    {\"Name\": \"validation:loss_avg\", \"Regex\": util.training.get_hf_metric_regex(\"eval_loss\")},\n",
    "    {\"Name\": \"validation:acc\", \"Regex\": util.training.get_hf_metric_regex(\"eval_acc\")},\n",
    "    {\n",
    "        \"Name\": \"validation:n_focus_examples\",\n",
    "        \"Regex\": util.training.get_hf_metric_regex(\"eval_n_focus_examples\"),\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"validation:focus_acc\",\n",
    "        \"Regex\": util.training.get_hf_metric_regex(\"eval_focus_acc\"),\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"validation:target\",\n",
    "        \"Regex\": util.training.get_hf_metric_regex(\"eval_focus_else_acc_minus_one\"),\n",
    "    },\n",
    "]\n",
    "\n",
    "estimator = HuggingFaceEstimator(\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"src\",\n",
    "    py_version=py_version,\n",
    "    pytorch_version=pt_version,\n",
    "    transformers_version=hf_version,\n",
    "    image_uri=train_image_uri,  # Use customized training container image\n",
    "\n",
    "    base_job_name=\"xlm-cfpb-hf\",\n",
    "    output_path=f\"s3://{bucket_name}/{bucket_prefix}trainjobs\",\n",
    "    # checkpoint_s3_uri=checkpoint_s3_uri,  # Un-comment to turn on checkpoint upload to S3\n",
    "\n",
    "    instance_type=\"ml.g4dn.xlarge\",  # Could also consider ml.p3.2xlarge\n",
    "    instance_count=1,\n",
    "    volume_size=80,\n",
    "\n",
    "    debugger_hook_config=False,  # (Required for LayoutLMv2/XLM, not for v1)\n",
    "\n",
    "    hyperparameters=hyperparameters,\n",
    "    metric_definitions=metric_definitions,\n",
    "    environment={\n",
    "        # Required for our custom dataset loading code (which depends on tokenizer):\n",
    "        \"TOKENIZERS_PARALLELISM\": \"false\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the below cell will actually kick off the training job and stream logs from the running container.\n",
    "\n",
    "> ℹ️ You'll also be able to check the status of the job in the [Training jobs page of the SageMaker Console](https://console.aws.amazon.com/sagemaker/home?#/jobs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-22 12:28:39 Starting - Starting the training job...\n",
      "2023-05-22 12:28:54 Starting - Preparing the instances for training......\n",
      "2023-05-22 12:30:02 Downloading - Downloading input data...\n",
      "2023-05-22 12:30:37 Training - Downloading the training image...........................\n",
      "2023-05-22 12:35:19 Training - Training image download completed. Training in progress...\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-05-22 12:35:31,443 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-05-22 12:35:31,470 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-05-22 12:35:31,474 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-05-22 12:35:31,655 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip available: 22.2.2 -> 23.1.2\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2023-05-22 12:35:33,565 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-05-22 12:35:33,565 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-05-22 12:35:33,647 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"images\": \"/opt/ml/input/data/images\",\n",
      "        \"textract\": \"/opt/ml/input/data/textract\",\n",
      "        \"train\": \"/opt/ml/input/data/train\",\n",
      "        \"validation\": \"/opt/ml/input/data/validation\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"annotation_attr\": \"label\",\n",
      "        \"early_stopping_patience\": 15,\n",
      "        \"greater_is_better\": \"true\",\n",
      "        \"images_prefix\": \"textract-transformers/data/imgs-clean\",\n",
      "        \"metric_for_best_model\": \"eval_focus_else_acc_minus_one\",\n",
      "        \"model_name_or_path\": \"microsoft/layoutxlm-base\",\n",
      "        \"num_labels\": 20,\n",
      "        \"num_train_epochs\": 150,\n",
      "        \"per_device_eval_batch_size\": 4,\n",
      "        \"per_device_train_batch_size\": 2,\n",
      "        \"save_total_limit\": 10,\n",
      "        \"textract_prefix\": \"textract-transformers/data/textracted\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"images\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"textract\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"validation\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"job_name\": \"xlm-cfpb-hf-2023-05-22-12-28-38-729\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-015943506230/xlm-cfpb-hf-2023-05-22-12-28-38-729/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"annotation_attr\":\"label\",\"early_stopping_patience\":15,\"greater_is_better\":\"true\",\"images_prefix\":\"textract-transformers/data/imgs-clean\",\"metric_for_best_model\":\"eval_focus_else_acc_minus_one\",\"model_name_or_path\":\"microsoft/layoutxlm-base\",\"num_labels\":20,\"num_train_epochs\":150,\"per_device_eval_batch_size\":4,\"per_device_train_batch_size\":2,\"save_total_limit\":10,\"textract_prefix\":\"textract-transformers/data/textracted\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"images\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"textract\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"images\",\"textract\",\"train\",\"validation\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g4dn.xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-015943506230/xlm-cfpb-hf-2023-05-22-12-28-38-729/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"images\":\"/opt/ml/input/data/images\",\"textract\":\"/opt/ml/input/data/textract\",\"train\":\"/opt/ml/input/data/train\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g4dn.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"annotation_attr\":\"label\",\"early_stopping_patience\":15,\"greater_is_better\":\"true\",\"images_prefix\":\"textract-transformers/data/imgs-clean\",\"metric_for_best_model\":\"eval_focus_else_acc_minus_one\",\"model_name_or_path\":\"microsoft/layoutxlm-base\",\"num_labels\":20,\"num_train_epochs\":150,\"per_device_eval_batch_size\":4,\"per_device_train_batch_size\":2,\"save_total_limit\":10,\"textract_prefix\":\"textract-transformers/data/textracted\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"images\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"textract\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"job_name\":\"xlm-cfpb-hf-2023-05-22-12-28-38-729\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-015943506230/xlm-cfpb-hf-2023-05-22-12-28-38-729/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--annotation_attr\",\"label\",\"--early_stopping_patience\",\"15\",\"--greater_is_better\",\"true\",\"--images_prefix\",\"textract-transformers/data/imgs-clean\",\"--metric_for_best_model\",\"eval_focus_else_acc_minus_one\",\"--model_name_or_path\",\"microsoft/layoutxlm-base\",\"--num_labels\",\"20\",\"--num_train_epochs\",\"150\",\"--per_device_eval_batch_size\",\"4\",\"--per_device_train_batch_size\",\"2\",\"--save_total_limit\",\"10\",\"--textract_prefix\",\"textract-transformers/data/textracted\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_IMAGES=/opt/ml/input/data/images\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEXTRACT=/opt/ml/input/data/textract\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34mSM_HP_ANNOTATION_ATTR=label\u001b[0m\n",
      "\u001b[34mSM_HP_EARLY_STOPPING_PATIENCE=15\u001b[0m\n",
      "\u001b[34mSM_HP_GREATER_IS_BETTER=true\u001b[0m\n",
      "\u001b[34mSM_HP_IMAGES_PREFIX=textract-transformers/data/imgs-clean\u001b[0m\n",
      "\u001b[34mSM_HP_METRIC_FOR_BEST_MODEL=eval_focus_else_acc_minus_one\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME_OR_PATH=microsoft/layoutxlm-base\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_LABELS=20\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=150\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=2\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_TOTAL_LIMIT=10\u001b[0m\n",
      "\u001b[34mSM_HP_TEXTRACT_PREFIX=textract-transformers/data/textracted\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.22b20220929-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 train.py --annotation_attr label --early_stopping_patience 15 --greater_is_better true --images_prefix textract-transformers/data/imgs-clean --metric_for_best_model eval_focus_else_acc_minus_one --model_name_or_path microsoft/layoutxlm-base --num_labels 20 --num_train_epochs 150 --per_device_eval_batch_size 4 --per_device_train_batch_size 2 --save_total_limit 10 --textract_prefix textract-transformers/data/textracted\u001b[0m\n",
      "\u001b[34m[2023-05-22 12:35:35.145: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.26.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34mDefaulting CUBLAS_WORKSPACE_CONFIG=':4096:8' to enable deterministic ops as `seed` is set.\u001b[0m\n",
      "\u001b[34m2023-05-22 12:35:37,945 [main] INFO Loaded arguments:\u001b[0m\n",
      "\u001b[34mModelArguments(cache_dir='/tmp/transformers/cache', config_name=None, model_name_or_path='microsoft/layoutxlm-base', model_revision='main', tokenizer_name=None, use_auth_token=False)\u001b[0m\n",
      "\u001b[34mDataTrainingArguments(annotation_attr='label', dataproc_batch_size=16, max_seq_length=512, pad_to_multiple_of=8, max_train_samples=None, task_name='ner', textract='/opt/ml/input/data/textract', textract_prefix='textract-transformers/data/textracted', train='/opt/ml/input/data/train', validation='/opt/ml/input/data/validation', images='/opt/ml/input/data/images', images_prefix='textract-transformers/data/imgs-clean', num_labels=20, mlm_probability=0.15, tiam_probability=0.15, tim_probability=0.2)\u001b[0m\n",
      "\u001b[34mSageMakerTrainingArguments(\u001b[0m\n",
      "\u001b[34m_n_gpu=1,\u001b[0m\n",
      "\u001b[34madafactor=False,\u001b[0m\n",
      "\u001b[34madam_beta1=0.9,\u001b[0m\n",
      "\u001b[34madam_beta2=0.999,\u001b[0m\n",
      "\u001b[34madam_epsilon=1e-08,\u001b[0m\n",
      "\u001b[34mauto_find_batch_size=False,\u001b[0m\n",
      "\u001b[34mbf16=False,\u001b[0m\n",
      "\u001b[34mbf16_full_eval=False,\u001b[0m\n",
      "\u001b[34mdata_seed=None,\u001b[0m\n",
      "\u001b[34mdataloader_drop_last=False,\u001b[0m\n",
      "\u001b[34mdataloader_num_workers=2,\u001b[0m\n",
      "\u001b[34mdataloader_pin_memory=True,\u001b[0m\n",
      "\u001b[34mdataproc_num_workers=2,\u001b[0m\n",
      "\u001b[34mddp_bucket_cap_mb=None,\u001b[0m\n",
      "\u001b[34mddp_find_unused_parameters=True,\u001b[0m\n",
      "\u001b[34mddp_timeout=1800,\u001b[0m\n",
      "\u001b[34mdebug=[],\u001b[0m\n",
      "\u001b[34mdeepspeed=None,\u001b[0m\n",
      "\u001b[34mdisable_tqdm=True,\u001b[0m\n",
      "\u001b[34mdo_eval=True,\u001b[0m\n",
      "\u001b[34mdo_predict=False,\u001b[0m\n",
      "\u001b[34mdo_train=True,\u001b[0m\n",
      "\u001b[34mearly_stopping_patience=15,\u001b[0m\n",
      "\u001b[34mearly_stopping_threshold=0.0,\u001b[0m\n",
      "\u001b[34meval_accumulation_steps=None,\u001b[0m\n",
      "\u001b[34meval_delay=0,\u001b[0m\n",
      "\u001b[34meval_steps=None,\u001b[0m\n",
      "\u001b[34mevaluation_strategy=epoch,\u001b[0m\n",
      "\u001b[34mfp16=False,\u001b[0m\n",
      "\u001b[34mfp16_backend=auto,\u001b[0m\n",
      "\u001b[34mfp16_full_eval=False,\u001b[0m\n",
      "\u001b[34mfp16_opt_level=O1,\u001b[0m\n",
      "\u001b[34mfsdp=[],\u001b[0m\n",
      "\u001b[34mfsdp_min_num_params=0,\u001b[0m\n",
      "\u001b[34mfsdp_transformer_layer_cls_to_wrap=None,\u001b[0m\n",
      "\u001b[34mfull_determinism=True,\u001b[0m\n",
      "\u001b[34mgradient_accumulation_steps=1,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing=False,\u001b[0m\n",
      "\u001b[34mgreater_is_better=True,\u001b[0m\n",
      "\u001b[34mgroup_by_length=False,\u001b[0m\n",
      "\u001b[34mhalf_precision_backend=auto,\u001b[0m\n",
      "\u001b[34mhub_model_id=None,\u001b[0m\n",
      "\u001b[34mhub_private_repo=False,\u001b[0m\n",
      "\u001b[34mhub_strategy=every_save,\u001b[0m\n",
      "\u001b[34mhub_token=<HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mignore_data_skip=False,\u001b[0m\n",
      "\u001b[34minclude_inputs_for_metrics=False,\u001b[0m\n",
      "\u001b[34mjit_mode_eval=False,\u001b[0m\n",
      "\u001b[34mlabel_names=None,\u001b[0m\n",
      "\u001b[34mlabel_smoothing_factor=0.0,\u001b[0m\n",
      "\u001b[34mlearning_rate=5e-05,\u001b[0m\n",
      "\u001b[34mlength_column_name=length,\u001b[0m\n",
      "\u001b[34mload_best_model_at_end=True,\u001b[0m\n",
      "\u001b[34mlocal_rank=-1,\u001b[0m\n",
      "\u001b[34mlog_level=passive,\u001b[0m\n",
      "\u001b[34mlog_level_replica=passive,\u001b[0m\n",
      "\u001b[34mlog_on_each_node=True,\u001b[0m\n",
      "\u001b[34mlogging_dir=/tmp/transformers/checkpoints/runs/May22_12-35-37_algo-1,\u001b[0m\n",
      "\u001b[34mlogging_first_step=False,\u001b[0m\n",
      "\u001b[34mlogging_nan_inf_filter=True,\u001b[0m\n",
      "\u001b[34mlogging_steps=500,\u001b[0m\n",
      "\u001b[34mlogging_strategy=steps,\u001b[0m\n",
      "\u001b[34mlr_scheduler_type=linear,\u001b[0m\n",
      "\u001b[34mmax_grad_norm=1.0,\u001b[0m\n",
      "\u001b[34mmax_steps=-1,\u001b[0m\n",
      "\u001b[34mmetric_for_best_model=eval_focus_else_acc_minus_one,\u001b[0m\n",
      "\u001b[34mmodel_dir=/opt/ml/model,\u001b[0m\n",
      "\u001b[34mmp_parameters=,\u001b[0m\n",
      "\u001b[34mno_cuda=False,\u001b[0m\n",
      "\u001b[34mnum_train_epochs=150.0,\u001b[0m\n",
      "\u001b[34moptim=adamw_hf,\u001b[0m\n",
      "\u001b[34moptim_args=None,\u001b[0m\n",
      "\u001b[34moutput_dir=/tmp/transformers/checkpoints,\u001b[0m\n",
      "\u001b[34moverwrite_output_dir=True,\u001b[0m\n",
      "\u001b[34mpast_index=-1,\u001b[0m\n",
      "\u001b[34mper_device_eval_batch_size=4,\u001b[0m\n",
      "\u001b[34mper_device_train_batch_size=2,\u001b[0m\n",
      "\u001b[34mprediction_loss_only=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub_model_id=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_organization=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mray_scope=last,\u001b[0m\n",
      "\u001b[34mremove_unused_columns=False,\u001b[0m\n",
      "\u001b[34mreport_to=['tensorboard'],\u001b[0m\n",
      "\u001b[34mresume_from_checkpoint=None,\u001b[0m\n",
      "\u001b[34mrun_name=/tmp/transformers/checkpoints,\u001b[0m\n",
      "\u001b[34msave_on_each_node=False,\u001b[0m\n",
      "\u001b[34msave_steps=500,\u001b[0m\n",
      "\u001b[34msave_strategy=epoch,\u001b[0m\n",
      "\u001b[34msave_total_limit=10,\u001b[0m\n",
      "\u001b[34mseed=42,\u001b[0m\n",
      "\u001b[34msharded_ddp=[],\u001b[0m\n",
      "\u001b[34mskip_memory_metrics=True,\u001b[0m\n",
      "\u001b[34mtf32=None,\u001b[0m\n",
      "\u001b[34mtorch_compile=False,\u001b[0m\n",
      "\u001b[34mtorch_compile_backend=None,\u001b[0m\n",
      "\u001b[34mtorch_compile_mode=None,\u001b[0m\n",
      "\u001b[34mtorchdynamo=None,\u001b[0m\n",
      "\u001b[34mtpu_metrics_debug=False,\u001b[0m\n",
      "\u001b[34mtpu_num_cores=None,\u001b[0m\n",
      "\u001b[34muse_ipex=False,\u001b[0m\n",
      "\u001b[34muse_legacy_prediction_loop=False,\u001b[0m\n",
      "\u001b[34muse_mps_device=False,\u001b[0m\n",
      "\u001b[34mwarmup_ratio=0.0,\u001b[0m\n",
      "\u001b[34mwarmup_steps=0,\u001b[0m\n",
      "\u001b[34mweight_decay=0.0,\u001b[0m\n",
      "\u001b[34mxpu_backend=None,\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34m2023-05-22 12:35:37,945 [main] INFO Starting!\u001b[0m\n",
      "\u001b[34m2023-05-22 12:35:37,945 [main] INFO Started with local_rank -1\u001b[0m\n",
      "\u001b[34m2023-05-22 12:35:37,946 [main] INFO Creating config and model\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json:   0%|          | 0.00/1.02k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json: 100%|██████████| 1.02k/1.02k [00:00<00:00, 154kB/s]\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:660] 2023-05-22 12:35:38,002 >> loading configuration file config.json from cache at /tmp/transformers/cache/models--microsoft--layoutxlm-base/snapshots/8e04ebc4d3ba0013cf943b697c0aedf19b06472a/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:660] 2023-05-22 12:35:38,002 >> loading configuration file config.json from cache at /tmp/transformers/cache/models--microsoft--layoutxlm-base/snapshots/8e04ebc4d3ba0013cf943b697c0aedf19b06472a/config.json\u001b[0m\n",
      "\u001b[34mDownloading (…)n/config.4.13.0.json:   0%|          | 0.00/947 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)n/config.4.13.0.json: 100%|██████████| 947/947 [00:00<00:00, 152kB/s]\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:660] 2023-05-22 12:35:38,040 >> loading configuration file config.4.13.0.json from cache at /tmp/transformers/cache/models--microsoft--layoutxlm-base/snapshots/8e04ebc4d3ba0013cf943b697c0aedf19b06472a/config.4.13.0.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:660] 2023-05-22 12:35:38,040 >> loading configuration file config.4.13.0.json from cache at /tmp/transformers/cache/models--microsoft--layoutxlm-base/snapshots/8e04ebc4d3ba0013cf943b697c0aedf19b06472a/config.4.13.0.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:712] 2023-05-22 12:35:38,041 >> Model config LayoutLMv2Config {\n",
      "  \"_name_or_path\": \"microsoft/layoutxlm-base\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"convert_sync_batchnorm\": true,\n",
      "  \"coordinate_size\": 128,\n",
      "  \"detectron2_config_args\": {\n",
      "    \"MODEL.ANCHOR_GENERATOR.SIZES\": [\n",
      "      [\n",
      "        32\n",
      "      ],\n",
      "      [\n",
      "        64\n",
      "      ],\n",
      "      [\n",
      "        128\n",
      "      ],\n",
      "      [\n",
      "        256\n",
      "      ],\n",
      "      [\n",
      "        512\n",
      "      ]\n",
      "    ],\n",
      "    \"MODEL.BACKBONE.NAME\": \"build_resnet_fpn_backbone\",\n",
      "    \"MODEL.FPN.IN_FEATURES\": [\n",
      "      \"res2\",\n",
      "      \"res3\",\n",
      "      \"res4\",\n",
      "      \"res5\"\n",
      "    ],\n",
      "    \"MODEL.MASK_ON\": true,\n",
      "    \"MODEL.PIXEL_STD\": [\n",
      "      57.375,\n",
      "      57.12,\n",
      "      58.395\n",
      "    ],\n",
      "    \"MODEL.POST_NMS_TOPK_TEST\": 1000,\n",
      "    \"MODEL.RESNETS.ASPECT_RATIOS\": [\n",
      "      [\n",
      "        0.5,\n",
      "        1.0,\n",
      "        2.0\n",
      "      ]\n",
      "    ],\n",
      "    \"MODEL.RESNETS.DEPTH\": 101,\n",
      "    \"MODEL.RESNETS.NUM_GROUPS\": 32,\n",
      "    \"MODEL.RESNETS.OUT_FEATURES\": [\n",
      "      \"res2\",\n",
      "      \"res3\",\n",
      "      \"res4\",\n",
      "      \"res5\"\n",
      "    ],\n",
      "    \"MODEL.RESNETS.SIZES\": [\n",
      "      [\n",
      "        32\n",
      "      ],\n",
      "      [\n",
      "        64\n",
      "      ],\n",
      "      [\n",
      "        128\n",
      "      ],\n",
      "      [\n",
      "        256\n",
      "      ],\n",
      "      [\n",
      "        512\n",
      "      ]\n",
      "    ],\n",
      "    \"MODEL.RESNETS.STRIDE_IN_1X1\": false,\n",
      "    \"MODEL.RESNETS.WIDTH_PER_GROUP\": 8,\n",
      "    \"MODEL.ROI_BOX_HEAD.NAME\": \"FastRCNNConvFCHead\",\n",
      "    \"MODEL.ROI_BOX_HEAD.NUM_FC\": 2,\n",
      "    \"MODEL.ROI_BOX_HEAD.POOLER_RESOLUTION\": 14,\n",
      "    \"MODEL.ROI_HEADS.IN_FEATURES\": [\n",
      "      \"p2\",\n",
      "      \"p3\",\n",
      "      \"p4\",\n",
      "      \"p5\"\n",
      "    ],\n",
      "    \"MODEL.ROI_HEADS.NAME\": \"StandardROIHeads\",\n",
      "    \"MODEL.ROI_HEADS.NUM_CLASSES\": 5,\n",
      "    \"MODEL.ROI_MASK_HEAD.NAME\": \"MaskRCNNConvUpsampleHead\",\n",
      "    \"MODEL.ROI_MASK_HEAD.NUM_CONV\": 4,\n",
      "    \"MODEL.ROI_MASK_HEAD.POOLER_RESOLUTION\": 7,\n",
      "    \"MODEL.RPN.IN_FEATURES\": [\n",
      "      \"p2\",\n",
      "      \"p3\",\n",
      "      \"p4\",\n",
      "      \"p5\",\n",
      "      \"p6\"\n",
      "    ],\n",
      "    \"MODEL.RPN.POST_NMS_TOPK_TRAIN\": 1000,\n",
      "    \"MODEL.RPN.PRE_NMS_TOPK_TEST\": 1000,\n",
      "    \"MODEL.RPN.PRE_NMS_TOPK_TRAIN\": 2000\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"fast_qkv\": false,\n",
      "  \"finetuning_task\": \"ner\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"has_relative_attention_bias\": false,\n",
      "  \"has_spatial_attention_bias\": false,\n",
      "  \"has_visual_segment_embedding\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"0\",\n",
      "    \"1\": \"1\",\n",
      "    \"2\": \"2\",\n",
      "    \"3\": \"3\",\n",
      "    \"4\": \"4\",\n",
      "    \"5\": \"5\",\n",
      "    \"6\": \"6\",\n",
      "    \"7\": \"7\",\n",
      "    \"8\": \"8\",\n",
      "    \"9\": \"9\",\n",
      "    \"10\": \"10\",\n",
      "    \"11\": \"11\",\n",
      "    \"12\": \"12\",\n",
      "    \"13\": \"13\",\n",
      "    \"14\": \"14\",\n",
      "    \"15\": \"15\",\n",
      "    \"16\": \"16\",\n",
      "    \"17\": \"17\",\n",
      "    \"18\": \"18\",\n",
      "    \"19\": \"19\"\n",
      "  },\n",
      "  \"image_feature_pool_shape\": [\n",
      "    7,\n",
      "    7,\n",
      "    256\n",
      "  ],\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"0\": 0,\n",
      "    \"1\": 1,\n",
      "    \"10\": 10,\n",
      "    \"11\": 11,\n",
      "    \"12\": 12,\n",
      "    \"13\": 13,\n",
      "    \"14\": 14,\n",
      "    \"15\": 15,\n",
      "    \"16\": 16,\n",
      "    \"17\": 17,\n",
      "    \"18\": 18,\n",
      "    \"19\": 19,\n",
      "    \"2\": 2,\n",
      "    \"3\": 3,\n",
      "    \"4\": 4,\n",
      "    \"5\": 5,\n",
      "    \"6\": 6,\n",
      "    \"7\": 7,\n",
      "    \"8\": 8,\n",
      "    \"9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_2d_position_embeddings\": 1024,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"max_rel_2d_pos\": 256,\n",
      "  \"max_rel_pos\": 128,\n",
      "  \"model_type\": \"layoutlmv2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"rel_2d_pos_bins\": 64,\n",
      "  \"rel_pos_bins\": 32,\n",
      "  \"shape_size\": 128,\n",
      "  \"tokenizer_class\": \"LayoutXLMTokenizer\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 250002\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:712] 2023-05-22 12:35:38,041 >> Model config LayoutLMv2Config {\n",
      "  \"_name_or_path\": \"microsoft/layoutxlm-base\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"convert_sync_batchnorm\": true,\n",
      "  \"coordinate_size\": 128,\n",
      "  \"detectron2_config_args\": {\n",
      "    \"MODEL.ANCHOR_GENERATOR.SIZES\": [\n",
      "      [\n",
      "        32\n",
      "      ],\n",
      "      [\n",
      "        64\n",
      "      ],\n",
      "      [\n",
      "        128\n",
      "      ],\n",
      "      [\n",
      "        256\n",
      "      ],\n",
      "      [\n",
      "        512\n",
      "      ]\n",
      "    ],\n",
      "    \"MODEL.BACKBONE.NAME\": \"build_resnet_fpn_backbone\",\n",
      "    \"MODEL.FPN.IN_FEATURES\": [\n",
      "      \"res2\",\n",
      "      \"res3\",\n",
      "      \"res4\",\n",
      "      \"res5\"\n",
      "    ],\n",
      "    \"MODEL.MASK_ON\": true,\n",
      "    \"MODEL.PIXEL_STD\": [\n",
      "      57.375,\n",
      "      57.12,\n",
      "      58.395\n",
      "    ],\n",
      "    \"MODEL.POST_NMS_TOPK_TEST\": 1000,\n",
      "    \"MODEL.RESNETS.ASPECT_RATIOS\": [\n",
      "      [\n",
      "        0.5,\n",
      "        1.0,\n",
      "        2.0\n",
      "      ]\n",
      "    ],\n",
      "    \"MODEL.RESNETS.DEPTH\": 101,\n",
      "    \"MODEL.RESNETS.NUM_GROUPS\": 32,\n",
      "    \"MODEL.RESNETS.OUT_FEATURES\": [\n",
      "      \"res2\",\n",
      "      \"res3\",\n",
      "      \"res4\",\n",
      "      \"res5\"\n",
      "    ],\n",
      "    \"MODEL.RESNETS.SIZES\": [\n",
      "      [\n",
      "        32\n",
      "      ],\n",
      "      [\n",
      "        64\n",
      "      ],\n",
      "      [\n",
      "        128\n",
      "      ],\n",
      "      [\n",
      "        256\n",
      "      ],\n",
      "      [\n",
      "        512\n",
      "      ]\n",
      "    ],\n",
      "    \"MODEL.RESNETS.STRIDE_IN_1X1\": false,\n",
      "    \"MODEL.RESNETS.WIDTH_PER_GROUP\": 8,\n",
      "    \"MODEL.ROI_BOX_HEAD.NAME\": \"FastRCNNConvFCHead\",\n",
      "    \"MODEL.ROI_BOX_HEAD.NUM_FC\": 2,\n",
      "    \"MODEL.ROI_BOX_HEAD.POOLER_RESOLUTION\": 14,\n",
      "    \"MODEL.ROI_HEADS.IN_FEATURES\": [\n",
      "      \"p2\",\n",
      "      \"p3\",\n",
      "      \"p4\",\n",
      "      \"p5\"\n",
      "    ],\n",
      "    \"MODEL.ROI_HEADS.NAME\": \"StandardROIHeads\",\n",
      "    \"MODEL.ROI_HEADS.NUM_CLASSES\": 5,\n",
      "    \"MODEL.ROI_MASK_HEAD.NAME\": \"MaskRCNNConvUpsampleHead\",\n",
      "    \"MODEL.ROI_MASK_HEAD.NUM_CONV\": 4,\n",
      "    \"MODEL.ROI_MASK_HEAD.POOLER_RESOLUTION\": 7,\n",
      "    \"MODEL.RPN.IN_FEATURES\": [\n",
      "      \"p2\",\n",
      "      \"p3\",\n",
      "      \"p4\",\n",
      "      \"p5\",\n",
      "      \"p6\"\n",
      "    ],\n",
      "    \"MODEL.RPN.POST_NMS_TOPK_TRAIN\": 1000,\n",
      "    \"MODEL.RPN.PRE_NMS_TOPK_TEST\": 1000,\n",
      "    \"MODEL.RPN.PRE_NMS_TOPK_TRAIN\": 2000\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"fast_qkv\": false,\n",
      "  \"finetuning_task\": \"ner\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"has_relative_attention_bias\": false,\n",
      "  \"has_spatial_attention_bias\": false,\n",
      "  \"has_visual_segment_embedding\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"0\",\n",
      "    \"1\": \"1\",\n",
      "    \"2\": \"2\",\n",
      "    \"3\": \"3\",\n",
      "    \"4\": \"4\",\n",
      "    \"5\": \"5\",\n",
      "    \"6\": \"6\",\n",
      "    \"7\": \"7\",\n",
      "    \"8\": \"8\",\n",
      "    \"9\": \"9\",\n",
      "    \"10\": \"10\",\n",
      "    \"11\": \"11\",\n",
      "    \"12\": \"12\",\n",
      "    \"13\": \"13\",\n",
      "    \"14\": \"14\",\n",
      "    \"15\": \"15\",\n",
      "    \"16\": \"16\",\n",
      "    \"17\": \"17\",\n",
      "    \"18\": \"18\",\n",
      "    \"19\": \"19\"\n",
      "  },\n",
      "  \"image_feature_pool_shape\": [\n",
      "    7,\n",
      "    7,\n",
      "    256\n",
      "  ],\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"0\": 0,\n",
      "    \"1\": 1,\n",
      "    \"10\": 10,\n",
      "    \"11\": 11,\n",
      "    \"12\": 12,\n",
      "    \"13\": 13,\n",
      "    \"14\": 14,\n",
      "    \"15\": 15,\n",
      "    \"16\": 16,\n",
      "    \"17\": 17,\n",
      "    \"18\": 18,\n",
      "    \"19\": 19,\n",
      "    \"2\": 2,\n",
      "    \"3\": 3,\n",
      "    \"4\": 4,\n",
      "    \"5\": 5,\n",
      "    \"6\": 6,\n",
      "    \"7\": 7,\n",
      "    \"8\": 8,\n",
      "    \"9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_2d_position_embeddings\": 1024,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"max_rel_2d_pos\": 256,\n",
      "  \"max_rel_pos\": 128,\n",
      "  \"model_type\": \"layoutlmv2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"rel_2d_pos_bins\": 64,\n",
      "  \"rel_pos_bins\": 32,\n",
      "  \"shape_size\": 128,\n",
      "  \"tokenizer_class\": \"LayoutXLMTokenizer\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 250002\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mDownloading (…)rocessor_config.json:   0%|          | 0.00/135 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)rocessor_config.json: 100%|██████████| 135/135 [00:00<00:00, 76.9kB/s]\u001b[0m\n",
      "\u001b[34m[INFO|image_processing_utils.py:296] 2023-05-22 12:35:38,077 >> loading configuration file preprocessor_config.json from cache at /tmp/transformers/cache/models--microsoft--layoutxlm-base/snapshots/8e04ebc4d3ba0013cf943b697c0aedf19b06472a/preprocessor_config.json\u001b[0m\n",
      "\u001b[34m[INFO|image_processing_utils.py:296] 2023-05-22 12:35:38,077 >> loading configuration file preprocessor_config.json from cache at /tmp/transformers/cache/models--microsoft--layoutxlm-base/snapshots/8e04ebc4d3ba0013cf943b697c0aedf19b06472a/preprocessor_config.json\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/models/layoutlmv2/feature_extraction_layoutlmv2.py:30: FutureWarning: The class LayoutLMv2FeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use LayoutLMv2ImageProcessor instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[INFO|image_processing_utils.py:520] 2023-05-22 12:35:38,078 >> size should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'shortest_edge', 'longest_edge'}), got 224. Converted to {'height': 224, 'width': 224}.\u001b[0m\n",
      "\u001b[34m[INFO|image_processing_utils.py:520] 2023-05-22 12:35:38,078 >> size should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'shortest_edge', 'longest_edge'}), got 224. Converted to {'height': 224, 'width': 224}.\u001b[0m\n",
      "\u001b[34m[INFO|image_processing_utils.py:341] 2023-05-22 12:35:38,078 >> Image processor LayoutLMv2FeatureExtractor {\n",
      "  \"apply_ocr\": false,\n",
      "  \"do_resize\": false,\n",
      "  \"feature_extractor_type\": \"LayoutLMv2FeatureExtractor\",\n",
      "  \"image_processor_type\": \"LayoutLMv2FeatureExtractor\",\n",
      "  \"ocr_lang\": null,\n",
      "  \"resample\": 2,\n",
      "  \"size\": {\n",
      "    \"height\": 224,\n",
      "    \"width\": 224\n",
      "  },\n",
      "  \"tesseract_config\": \"\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|image_processing_utils.py:341] 2023-05-22 12:35:38,078 >> Image processor LayoutLMv2FeatureExtractor {\n",
      "  \"apply_ocr\": false,\n",
      "  \"do_resize\": false,\n",
      "  \"feature_extractor_type\": \"LayoutLMv2FeatureExtractor\",\n",
      "  \"image_processor_type\": \"LayoutLMv2FeatureExtractor\",\n",
      "  \"ocr_lang\": null,\n",
      "  \"resample\": 2,\n",
      "  \"size\": {\n",
      "    \"height\": 224,\n",
      "    \"width\": 224\n",
      "  },\n",
      "  \"tesseract_config\": \"\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mDownloading (…)tencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)tencepiece.bpe.model: 100%|██████████| 5.07M/5.07M [00:00<00:00, 42.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)tencepiece.bpe.model: 100%|██████████| 5.07M/5.07M [00:00<00:00, 40.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)/main/tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)/main/tokenizer.json: 100%|██████████| 9.10M/9.10M [00:00<00:00, 209MB/s]\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1802] 2023-05-22 12:35:39,075 >> loading file sentencepiece.bpe.model from cache at /tmp/transformers/cache/models--microsoft--layoutxlm-base/snapshots/8e04ebc4d3ba0013cf943b697c0aedf19b06472a/sentencepiece.bpe.model\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1802] 2023-05-22 12:35:39,075 >> loading file tokenizer.json from cache at /tmp/transformers/cache/models--microsoft--layoutxlm-base/snapshots/8e04ebc4d3ba0013cf943b697c0aedf19b06472a/tokenizer.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1802] 2023-05-22 12:35:39,075 >> loading file sentencepiece.bpe.model from cache at /tmp/transformers/cache/models--microsoft--layoutxlm-base/snapshots/8e04ebc4d3ba0013cf943b697c0aedf19b06472a/sentencepiece.bpe.model\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1802] 2023-05-22 12:35:39,075 >> loading file tokenizer.json from cache at /tmp/transformers/cache/models--microsoft--layoutxlm-base/snapshots/8e04ebc4d3ba0013cf943b697c0aedf19b06472a/tokenizer.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1802] 2023-05-22 12:35:39,075 >> loading file added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1802] 2023-05-22 12:35:39,075 >> loading file added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1802] 2023-05-22 12:35:39,075 >> loading file special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1802] 2023-05-22 12:35:39,075 >> loading file tokenizer_config.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1802] 2023-05-22 12:35:39,075 >> loading file special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1802] 2023-05-22 12:35:39,075 >> loading file tokenizer_config.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:660] 2023-05-22 12:35:39,076 >> loading configuration file config.json from cache at /tmp/transformers/cache/models--microsoft--layoutxlm-base/snapshots/8e04ebc4d3ba0013cf943b697c0aedf19b06472a/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:660] 2023-05-22 12:35:39,076 >> loading configuration file config.json from cache at /tmp/transformers/cache/models--microsoft--layoutxlm-base/snapshots/8e04ebc4d3ba0013cf943b697c0aedf19b06472a/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:660] 2023-05-22 12:35:39,076 >> loading configuration file config.4.13.0.json from cache at /tmp/transformers/cache/models--microsoft--layoutxlm-base/snapshots/8e04ebc4d3ba0013cf943b697c0aedf19b06472a/config.4.13.0.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:660] 2023-05-22 12:35:39,076 >> loading configuration file config.4.13.0.json from cache at /tmp/transformers/cache/models--microsoft--layoutxlm-base/snapshots/8e04ebc4d3ba0013cf943b697c0aedf19b06472a/config.4.13.0.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:712] 2023-05-22 12:35:39,077 >> Model config LayoutLMv2Config {\n",
      "  \"_name_or_path\": \"microsoft/layoutxlm-base\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"convert_sync_batchnorm\": true,\n",
      "  \"coordinate_size\": 128,\n",
      "  \"detectron2_config_args\": {\n",
      "    \"MODEL.ANCHOR_GENERATOR.SIZES\": [\n",
      "      [\n",
      "        32\n",
      "      ],\n",
      "      [\n",
      "        64\n",
      "      ],\n",
      "      [\n",
      "        128\n",
      "      ],\n",
      "      [\n",
      "        256\n",
      "      ],\n",
      "      [\n",
      "        512\n",
      "      ]\n",
      "    ],\n",
      "    \"MODEL.BACKBONE.NAME\": \"build_resnet_fpn_backbone\",\n",
      "    \"MODEL.FPN.IN_FEATURES\": [\n",
      "      \"res2\",\n",
      "      \"res3\",\n",
      "      \"res4\",\n",
      "      \"res5\"\n",
      "    ],\n",
      "    \"MODEL.MASK_ON\": true,\n",
      "    \"MODEL.PIXEL_STD\": [\n",
      "      57.375,\n",
      "      57.12,\n",
      "      58.395\n",
      "    ],\n",
      "    \"MODEL.POST_NMS_TOPK_TEST\": 1000,\n",
      "    \"MODEL.RESNETS.ASPECT_RATIOS\": [\n",
      "      [\n",
      "        0.5,\n",
      "        1.0,\n",
      "        2.0\n",
      "      ]\n",
      "    ],\n",
      "    \"MODEL.RESNETS.DEPTH\": 101,\n",
      "    \"MODEL.RESNETS.NUM_GROUPS\": 32,\n",
      "    \"MODEL.RESNETS.OUT_FEATURES\": [\n",
      "      \"res2\",\n",
      "      \"res3\",\n",
      "      \"res4\",\n",
      "      \"res5\"\n",
      "    ],\n",
      "    \"MODEL.RESNETS.SIZES\": [\n",
      "      [\n",
      "        32\n",
      "      ],\n",
      "      [\n",
      "        64\n",
      "      ],\n",
      "      [\n",
      "        128\n",
      "      ],\n",
      "      [\n",
      "        256\n",
      "      ],\n",
      "      [\n",
      "        512\n",
      "      ]\n",
      "    ],\n",
      "    \"MODEL.RESNETS.STRIDE_IN_1X1\": false,\n",
      "    \"MODEL.RESNETS.WIDTH_PER_GROUP\": 8,\n",
      "    \"MODEL.ROI_BOX_HEAD.NAME\": \"FastRCNNConvFCHead\",\n",
      "    \"MODEL.ROI_BOX_HEAD.NUM_FC\": 2,\n",
      "    \"MODEL.ROI_BOX_HEAD.POOLER_RESOLUTION\": 14,\n",
      "    \"MODEL.ROI_HEADS.IN_FEATURES\": [\n",
      "      \"p2\",\n",
      "      \"p3\",\n",
      "      \"p4\",\n",
      "      \"p5\"\n",
      "    ],\n",
      "    \"MODEL.ROI_HEADS.NAME\": \"StandardROIHeads\",\n",
      "    \"MODEL.ROI_HEADS.NUM_CLASSES\": 5,\n",
      "    \"MODEL.ROI_MASK_HEAD.NAME\": \"MaskRCNNConvUpsampleHead\",\n",
      "    \"MODEL.ROI_MASK_HEAD.NUM_CONV\": 4,\n",
      "    \"MODEL.ROI_MASK_HEAD.POOLER_RESOLUTION\": 7,\n",
      "    \"MODEL.RPN.IN_FEATURES\": [\n",
      "      \"p2\",\n",
      "      \"p3\",\n",
      "      \"p4\",\n",
      "      \"p5\",\n",
      "      \"p6\"\n",
      "    ],\n",
      "    \"MODEL.RPN.POST_NMS_TOPK_TRAIN\": 1000,\n",
      "    \"MODEL.RPN.PRE_NMS_TOPK_TEST\": 1000,\n",
      "    \"MODEL.RPN.PRE_NMS_TOPK_TRAIN\": 2000\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"fast_qkv\": false,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"has_relative_attention_bias\": false,\n",
      "  \"has_spatial_attention_bias\": false,\n",
      "  \"has_visual_segment_embedding\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_feature_pool_shape\": [\n",
      "    7,\n",
      "    7,\n",
      "    256\n",
      "  ],\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_2d_position_embeddings\": 1024,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"max_rel_2d_pos\": 256,\n",
      "  \"max_rel_pos\": 128,\n",
      "  \"model_type\": \"layoutlmv2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"rel_2d_pos_bins\": 64,\n",
      "  \"rel_pos_bins\": 32,\n",
      "  \"shape_size\": 128,\n",
      "  \"tokenizer_class\": \"LayoutXLMTokenizer\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 250002\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:712] 2023-05-22 12:35:39,077 >> Model config LayoutLMv2Config {\n",
      "  \"_name_or_path\": \"microsoft/layoutxlm-base\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"convert_sync_batchnorm\": true,\n",
      "  \"coordinate_size\": 128,\n",
      "  \"detectron2_config_args\": {\n",
      "    \"MODEL.ANCHOR_GENERATOR.SIZES\": [\n",
      "      [\n",
      "        32\n",
      "      ],\n",
      "      [\n",
      "        64\n",
      "      ],\n",
      "      [\n",
      "        128\n",
      "      ],\n",
      "      [\n",
      "        256\n",
      "      ],\n",
      "      [\n",
      "        512\n",
      "      ]\n",
      "    ],\n",
      "    \"MODEL.BACKBONE.NAME\": \"build_resnet_fpn_backbone\",\n",
      "    \"MODEL.FPN.IN_FEATURES\": [\n",
      "      \"res2\",\n",
      "      \"res3\",\n",
      "      \"res4\",\n",
      "      \"res5\"\n",
      "    ],\n",
      "    \"MODEL.MASK_ON\": true,\n",
      "    \"MODEL.PIXEL_STD\": [\n",
      "      57.375,\n",
      "      57.12,\n",
      "      58.395\n",
      "    ],\n",
      "    \"MODEL.POST_NMS_TOPK_TEST\": 1000,\n",
      "    \"MODEL.RESNETS.ASPECT_RATIOS\": [\n",
      "      [\n",
      "        0.5,\n",
      "        1.0,\n",
      "        2.0\n",
      "      ]\n",
      "    ],\n",
      "    \"MODEL.RESNETS.DEPTH\": 101,\n",
      "    \"MODEL.RESNETS.NUM_GROUPS\": 32,\n",
      "    \"MODEL.RESNETS.OUT_FEATURES\": [\n",
      "      \"res2\",\n",
      "      \"res3\",\n",
      "      \"res4\",\n",
      "      \"res5\"\n",
      "    ],\n",
      "    \"MODEL.RESNETS.SIZES\": [\n",
      "      [\n",
      "        32\n",
      "      ],\n",
      "      [\n",
      "        64\n",
      "      ],\n",
      "      [\n",
      "        128\n",
      "      ],\n",
      "      [\n",
      "        256\n",
      "      ],\n",
      "      [\n",
      "        512\n",
      "      ]\n",
      "    ],\n",
      "    \"MODEL.RESNETS.STRIDE_IN_1X1\": false,\n",
      "    \"MODEL.RESNETS.WIDTH_PER_GROUP\": 8,\n",
      "    \"MODEL.ROI_BOX_HEAD.NAME\": \"FastRCNNConvFCHead\",\n",
      "    \"MODEL.ROI_BOX_HEAD.NUM_FC\": 2,\n",
      "    \"MODEL.ROI_BOX_HEAD.POOLER_RESOLUTION\": 14,\n",
      "    \"MODEL.ROI_HEADS.IN_FEATURES\": [\n",
      "      \"p2\",\n",
      "      \"p3\",\n",
      "      \"p4\",\n",
      "      \"p5\"\n",
      "    ],\n",
      "    \"MODEL.ROI_HEADS.NAME\": \"StandardROIHeads\",\n",
      "    \"MODEL.ROI_HEADS.NUM_CLASSES\": 5,\n",
      "    \"MODEL.ROI_MASK_HEAD.NAME\": \"MaskRCNNConvUpsampleHead\",\n",
      "    \"MODEL.ROI_MASK_HEAD.NUM_CONV\": 4,\n",
      "    \"MODEL.ROI_MASK_HEAD.POOLER_RESOLUTION\": 7,\n",
      "    \"MODEL.RPN.IN_FEATURES\": [\n",
      "      \"p2\",\n",
      "      \"p3\",\n",
      "      \"p4\",\n",
      "      \"p5\",\n",
      "      \"p6\"\n",
      "    ],\n",
      "    \"MODEL.RPN.POST_NMS_TOPK_TRAIN\": 1000,\n",
      "    \"MODEL.RPN.PRE_NMS_TOPK_TEST\": 1000,\n",
      "    \"MODEL.RPN.PRE_NMS_TOPK_TRAIN\": 2000\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"fast_qkv\": false,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"has_relative_attention_bias\": false,\n",
      "  \"has_spatial_attention_bias\": false,\n",
      "  \"has_visual_segment_embedding\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_feature_pool_shape\": [\n",
      "    7,\n",
      "    7,\n",
      "    256\n",
      "  ],\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_2d_position_embeddings\": 1024,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"max_rel_2d_pos\": 256,\n",
      "  \"max_rel_pos\": 128,\n",
      "  \"model_type\": \"layoutlmv2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"rel_2d_pos_bins\": 64,\n",
      "  \"rel_pos_bins\": 32,\n",
      "  \"shape_size\": 128,\n",
      "  \"tokenizer_class\": \"LayoutXLMTokenizer\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 250002\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:   0%|          | 0.00/1.48G [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:   2%|▏         | 31.5M/1.48G [00:00<00:05, 243MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:   4%|▍         | 62.9M/1.48G [00:00<00:05, 262MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:   6%|▋         | 94.4M/1.48G [00:00<00:04, 279MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:   9%|▊         | 126M/1.48G [00:00<00:04, 283MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  11%|█         | 157M/1.48G [00:00<00:04, 290MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  13%|█▎        | 189M/1.48G [00:00<00:04, 296MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  15%|█▍        | 220M/1.48G [00:00<00:04, 300MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  17%|█▋        | 252M/1.48G [00:00<00:04, 300MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  19%|█▉        | 283M/1.48G [00:00<00:04, 285MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  21%|██▏       | 315M/1.48G [00:01<00:04, 264MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  23%|██▎       | 346M/1.48G [00:01<00:04, 259MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  26%|██▌       | 377M/1.48G [00:01<00:04, 244MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  28%|██▊       | 409M/1.48G [00:01<00:04, 245MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  30%|██▉       | 440M/1.48G [00:01<00:04, 248MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  32%|███▏      | 472M/1.48G [00:01<00:03, 255MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  34%|███▍      | 503M/1.48G [00:01<00:03, 259MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  36%|███▌      | 535M/1.48G [00:02<00:03, 261MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  38%|███▊      | 566M/1.48G [00:02<00:03, 266MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  40%|████      | 598M/1.48G [00:02<00:03, 273MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  43%|████▎     | 629M/1.48G [00:02<00:03, 276MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  45%|████▍     | 661M/1.48G [00:02<00:02, 278MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  47%|████▋     | 692M/1.48G [00:02<00:02, 276MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  49%|████▉     | 724M/1.48G [00:02<00:02, 277MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  51%|█████     | 755M/1.48G [00:02<00:02, 275MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  53%|█████▎    | 786M/1.48G [00:02<00:02, 277MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  55%|█████▌    | 818M/1.48G [00:03<00:02, 247MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  58%|█████▊    | 849M/1.48G [00:03<00:02, 255MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  60%|█████▉    | 881M/1.48G [00:03<00:02, 260MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  62%|██████▏   | 912M/1.48G [00:03<00:02, 264MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  64%|██████▍   | 944M/1.48G [00:03<00:01, 268MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  66%|██████▌   | 975M/1.48G [00:03<00:01, 270MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  68%|██████▊   | 1.01G/1.48G [00:03<00:01, 273MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  70%|███████   | 1.04G/1.48G [00:03<00:01, 275MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  72%|███████▏  | 1.07G/1.48G [00:03<00:01, 275MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  75%|███████▍  | 1.10G/1.48G [00:04<00:01, 278MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  77%|███████▋  | 1.13G/1.48G [00:04<00:01, 279MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  79%|███████▉  | 1.16G/1.48G [00:04<00:01, 279MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  81%|████████  | 1.20G/1.48G [00:04<00:01, 280MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  83%|████████▎ | 1.23G/1.48G [00:04<00:00, 277MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  85%|████████▌ | 1.26G/1.48G [00:04<00:00, 278MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  87%|████████▋ | 1.29G/1.48G [00:04<00:00, 279MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  89%|████████▉ | 1.32G/1.48G [00:04<00:00, 279MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  92%|█████████▏| 1.35G/1.48G [00:04<00:00, 280MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  94%|█████████▎| 1.38G/1.48G [00:05<00:00, 276MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  96%|█████████▌| 1.42G/1.48G [00:05<00:00, 276MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  98%|█████████▊| 1.45G/1.48G [00:05<00:00, 276MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin: 100%|██████████| 1.48G/1.48G [00:05<00:00, 274MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin: 100%|██████████| 1.48G/1.48G [00:05<00:00, 271MB/s]\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2275] 2023-05-22 12:35:45,238 >> loading weights file pytorch_model.bin from cache at /tmp/transformers/cache/models--microsoft--layoutxlm-base/snapshots/8e04ebc4d3ba0013cf943b697c0aedf19b06472a/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2275] 2023-05-22 12:35:45,238 >> loading weights file pytorch_model.bin from cache at /tmp/transformers/cache/models--microsoft--layoutxlm-base/snapshots/8e04ebc4d3ba0013cf943b697c0aedf19b06472a/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_layoutlmv2.py:574] 2023-05-22 12:35:49,840 >> using `AvgPool2d` instead of `AdaptiveAvgPool2d`\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_layoutlmv2.py:574] 2023-05-22 12:35:49,840 >> using `AvgPool2d` instead of `AdaptiveAvgPool2d`\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:2847] 2023-05-22 12:35:50,786 >> Some weights of the model checkpoint at microsoft/layoutxlm-base were not used when initializing LayoutLMv2ForTokenClassification: ['layoutlmv2.visual.backbone.bottom_up.res4.20.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.stem.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv2.norm.num_batches_tracked']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing LayoutLMv2ForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing LayoutLMv2ForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:2859] 2023-05-22 12:35:50,786 >> Some weights of LayoutLMv2ForTokenClassification were not initialized from the model checkpoint at microsoft/layoutxlm-base and are newly initialized: ['classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:2847] 2023-05-22 12:35:50,786 >> Some weights of the model checkpoint at microsoft/layoutxlm-base were not used when initializing LayoutLMv2ForTokenClassification: ['layoutlmv2.visual.backbone.bottom_up.res4.20.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.stem.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv2.norm.num_batches_tracked']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing LayoutLMv2ForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing LayoutLMv2ForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:2859] 2023-05-22 12:35:50,786 >> Some weights of LayoutLMv2ForTokenClassification were not initialized from the model checkpoint at microsoft/layoutxlm-base and are newly initialized: ['classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m2023-05-22 12:35:50,831 [main] INFO Loading datasets\u001b[0m\n",
      "\u001b[34m2023-05-22 12:35:50,831 [data.ner] INFO Getting NER datasets\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset json/default to /tmp/transformers/cache/json/default-90ded1daf3ed06d0/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\u001b[0m\n",
      "\u001b[34mDataset json downloaded and prepared to /tmp/transformers/cache/json/default-90ded1daf3ed06d0/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m2023-05-22 12:35:50,896 [data.base] INFO Loading text and images... (progress bar disabled)\u001b[0m\n",
      "\u001b[34m2023-05-22 12:36:16,499 [data.base] INFO Pre-warming tokenizer before split .map()\u001b[0m\n",
      "\u001b[34m2023-05-22 12:36:16,734 [data.base] INFO Tokenizer pre-warm done\u001b[0m\n",
      "\u001b[34m2023-05-22 12:36:16,734 [data.base] INFO Splitting long pages... (progress bar disabled)\u001b[0m\n",
      "\u001b[34m2023-05-22 12:36:30,218 [data.ner] INFO Train dataset ready: Dataset({\n",
      "    features: ['source-ref', 'textract-ref', 'page-num', 'label', 'label-metadata', 'text', 'boxes', 'images', 'word_labels'],\n",
      "    num_rows: 226\u001b[0m\n",
      "\u001b[34m})\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset json/default to /tmp/transformers/cache/json/default-c34e816d08aca264/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\u001b[0m\n",
      "\u001b[34mDataset json downloaded and prepared to /tmp/transformers/cache/json/default-c34e816d08aca264/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m2023-05-22 12:36:30,268 [data.base] INFO Loading text and images... (progress bar disabled)\u001b[0m\n",
      "\u001b[34m2023-05-22 12:36:34,126 [data.base] INFO Pre-warming tokenizer before split .map()\u001b[0m\n",
      "\u001b[34m2023-05-22 12:36:34,372 [data.base] INFO Tokenizer pre-warm done\u001b[0m\n",
      "\u001b[34m2023-05-22 12:36:34,372 [data.base] INFO Splitting long pages... (progress bar disabled)\u001b[0m\n",
      "\u001b[34m2023-05-22 12:36:37,080 [data.ner] INFO Validation dataset ready: Dataset({\n",
      "    features: ['source-ref', 'textract-ref', 'page-num', 'label', 'label-metadata', 'text', 'boxes', 'images', 'word_labels'],\n",
      "    num_rows: 24\u001b[0m\n",
      "\u001b[34m})\u001b[0m\n",
      "\u001b[34m2023-05-22 12:36:37,081 [main] INFO train dataset has 226 samples\u001b[0m\n",
      "\u001b[34m2023-05-22 12:36:37,081 [main] INFO validation dataset has 24 samples\u001b[0m\n",
      "\u001b[34m2023-05-22 12:36:37,081 [main] INFO Setting up trainer\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1650] 2023-05-22 12:36:43,076 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1651] 2023-05-22 12:36:43,077 >>   Num examples = 226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1650] 2023-05-22 12:36:43,076 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1651] 2023-05-22 12:36:43,077 >>   Num examples = 226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1652] 2023-05-22 12:36:43,077 >>   Num Epochs = 150\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1653] 2023-05-22 12:36:43,077 >>   Instantaneous batch size per device = 2\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1654] 2023-05-22 12:36:43,077 >>   Total train batch size (w. parallel, distributed & accumulation) = 2\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1652] 2023-05-22 12:36:43,077 >>   Num Epochs = 150\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1653] 2023-05-22 12:36:43,077 >>   Instantaneous batch size per device = 2\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1654] 2023-05-22 12:36:43,077 >>   Total train batch size (w. parallel, distributed & accumulation) = 2\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1655] 2023-05-22 12:36:43,077 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1656] 2023-05-22 12:36:43,077 >>   Total optimization steps = 16950\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1655] 2023-05-22 12:36:43,077 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1656] 2023-05-22 12:36:43,077 >>   Total optimization steps = 16950\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1657] 2023-05-22 12:36:43,078 >>   Number of trainable parameters = 368241684\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1657] 2023-05-22 12:36:43,078 >>   Number of trainable parameters = 368241684\u001b[0m\n",
      "\u001b[34m[2023-05-22 12:36:43.174: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.26.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-05-22 12:36:43.226 algo-1:44 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-05-22 12:36:43.365 algo-1:44 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 12:38:10,484 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 12:38:10,484 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 12:38:10,484 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 12:38:10,484 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 12:38:10,484 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 12:38:10,484 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m2023-05-22 12:38:14,882 [data.ner] INFO Evaluation class prediction ratios: {0: 0.0013922728854855553, 1: 8.70170553428472e-05, 4: 8.70170553428472e-05, 13: 0.026540201879568397, 14: 0.0028715628263139576, 15: 0.005482074486599374, 16: 0.017664462234597982, 17: 0.0029585798816568047, 19: 0.9429168116950922}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.35402151942253113, 'eval_n_examples': 24, 'eval_acc': 0.9250658947244693, 'eval_n_focus_examples': 19, 'eval_focus_acc': 0.3629913636011728, 'eval_focus_else_acc_minus_one': 0.3629913636011728, 'eval_runtime': 4.3987, 'eval_samples_per_second': 5.456, 'eval_steps_per_second': 1.364, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 12:38:14,884 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-113\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 12:38:14,884 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-113\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 12:38:14,885 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-113/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 12:38:14,885 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-113/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 12:38:18,201 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-113/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 12:38:18,201 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-113/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 12:40:01,344 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 12:40:01,344 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 12:40:01,344 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 12:40:01,344 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 12:40:01,344 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 12:40:01,344 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m2023-05-22 12:40:05,892 [data.ner] INFO Evaluation class prediction ratios: {0: 0.0024364775495997215, 1: 0.0018273581621997912, 4: 0.004437869822485207, 8: 0.000870170553428472, 10: 0.0012182387747998607, 11: 0.0002610511660285416, 12: 0.005656108597285068, 13: 0.018447615732683605, 14: 0.004611903933170901, 15: 0.004176818656456666, 16: 0.009223807866341803, 17: 0.0001740341106856944, 18: 0.03419770274973895, 19: 0.9124608423250957}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.1839122772216797, 'eval_n_examples': 24, 'eval_acc': 0.9371769680698403, 'eval_n_focus_examples': 20, 'eval_focus_acc': 0.5351880245411538, 'eval_focus_else_acc_minus_one': 0.5351880245411538, 'eval_runtime': 4.5495, 'eval_samples_per_second': 5.275, 'eval_steps_per_second': 1.319, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 12:40:05,895 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-226\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 12:40:05,895 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-226\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 12:40:05,897 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-226/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 12:40:05,897 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-226/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 12:40:08,922 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-226/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 12:40:08,922 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-226/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 12:41:52,130 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 12:41:52,130 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 12:41:52,131 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 12:41:52,131 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 12:41:52,131 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 12:41:52,131 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m2023-05-22 12:41:57,596 [data.ner] INFO Evaluation class prediction ratios: {0: 0.0015663069961712496, 2: 0.000870170553428472, 4: 0.001740341106856944, 8: 0.0005221023320570832, 10: 0.0012182387747998607, 12: 0.002523494604942569, 13: 0.015053950574312564, 14: 0.004089801601113818, 15: 0.0040027845457709715, 16: 0.009832927253741734, 17: 8.70170553428472e-05, 18: 0.027584406543682563, 19: 0.9309084580577793}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.20347726345062256, 'eval_n_examples': 24, 'eval_acc': 0.9476694542908789, 'eval_n_focus_examples': 20, 'eval_focus_acc': 0.5681574749648942, 'eval_focus_else_acc_minus_one': 0.5681574749648942, 'eval_runtime': 5.4662, 'eval_samples_per_second': 4.391, 'eval_steps_per_second': 1.098, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 12:41:57,598 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-339\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 12:41:57,598 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-339\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 12:41:57,600 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-339/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 12:41:57,600 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-339/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 12:42:00,599 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-339/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 12:42:00,599 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-339/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 12:43:44,403 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 12:43:44,403 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 12:43:44,404 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 12:43:44,404 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 12:43:44,404 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 12:43:44,404 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m2023-05-22 12:43:49,265 [data.ner] INFO Evaluation class prediction ratios: {0: 0.0032196310476853463, 1: 0.005743125652627915, 2: 0.0001740341106856944, 3: 8.70170553428472e-05, 4: 0.0030455969369996517, 6: 0.000870170553428472, 8: 0.000870170553428472, 10: 0.0023494604942568744, 12: 0.005656108597285068, 13: 0.011747302471284372, 14: 0.0024364775495997215, 15: 0.004089801601113818, 16: 0.009223807866341803, 17: 0.0010442046641141664, 18: 0.008875739644970414, 19: 0.9405673512008353}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.26724278926849365, 'eval_n_examples': 24, 'eval_acc': 0.9474119257055235, 'eval_n_focus_examples': 20, 'eval_focus_acc': 0.53462624769506, 'eval_focus_else_acc_minus_one': 0.53462624769506, 'eval_runtime': 4.8622, 'eval_samples_per_second': 4.936, 'eval_steps_per_second': 1.234, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 12:43:49,267 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-452\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 12:43:49,267 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-452\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 12:43:49,269 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-452/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 12:43:49,269 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-452/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 12:43:52,297 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-452/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 12:43:52,297 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-452/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m{'loss': 0.2316, 'learning_rate': 4.8525073746312687e-05, 'epoch': 4.42}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 12:45:36,492 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 12:45:36,492 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 12:45:36,493 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 12:45:36,493 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 12:45:36,493 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 12:45:36,493 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m2023-05-22 12:45:41,526 [data.ner] INFO Evaluation class prediction ratios: {0: 0.0027845457709711106, 1: 0.005830142707970762, 4: 0.007222415593456317, 6: 0.0001740341106856944, 8: 0.000870170553428472, 10: 0.002088409328228333, 11: 0.000435085276714236, 12: 0.005656108597285068, 13: 0.010268012530455969, 14: 0.0064392620953706925, 15: 0.0040027845457709715, 16: 0.009832927253741734, 17: 0.0013922728854855553, 18: 0.03680821441002437, 19: 0.9061956143404107}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.17835648357868195, 'eval_n_examples': 24, 'eval_acc': 0.9453375121938336, 'eval_n_focus_examples': 20, 'eval_focus_acc': 0.5786629215898917, 'eval_focus_else_acc_minus_one': 0.5786629215898917, 'eval_runtime': 5.0337, 'eval_samples_per_second': 4.768, 'eval_steps_per_second': 1.192, 'epoch': 5.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 12:45:41,527 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-565\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 12:45:41,527 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-565\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 12:45:41,529 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-565/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 12:45:41,529 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-565/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 12:45:44,500 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-565/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 12:45:44,500 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-565/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 12:47:25,672 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 12:47:25,672 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 12:47:25,672 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 12:47:25,672 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 12:47:25,672 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 12:47:25,672 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m2023-05-22 12:47:30,364 [data.ner] INFO Evaluation class prediction ratios: {0: 0.0027845457709711106, 1: 0.005221023320570832, 2: 0.001740341106856944, 4: 0.0022624434389140274, 6: 0.0003480682213713888, 8: 0.000870170553428472, 10: 0.0014792899408284023, 11: 0.0016533240515140967, 12: 0.005656108597285068, 13: 0.012965541246084232, 14: 0.004437869822485207, 15: 0.0040027845457709715, 16: 0.0060911938739993034, 17: 0.0002610511660285416, 18: 0.011660285415941524, 19: 0.9385659589279499}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.3035804033279419, 'eval_n_examples': 24, 'eval_acc': 0.9419282335515152, 'eval_n_focus_examples': 20, 'eval_focus_acc': 0.5103930860269652, 'eval_focus_else_acc_minus_one': 0.5103930860269652, 'eval_runtime': 4.6928, 'eval_samples_per_second': 5.114, 'eval_steps_per_second': 1.279, 'epoch': 6.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 12:47:30,366 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-678\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 12:47:30,366 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-678\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 12:47:30,368 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-678/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 12:47:30,368 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-678/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 12:47:33,370 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-678/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 12:47:33,370 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-678/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 12:49:13,124 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 12:49:13,124 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 12:49:13,124 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 12:49:13,124 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 12:49:13,124 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 12:49:13,124 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m2023-05-22 12:49:17,267 [data.ner] INFO Evaluation class prediction ratios: {0: 0.0033066481030281933, 1: 0.005395057431256526, 2: 8.70170553428472e-05, 3: 0.0001740341106856944, 4: 0.0037417333797424295, 6: 0.0006961364427427776, 8: 0.000870170553428472, 10: 0.0019143752175426383, 11: 0.0006961364427427776, 12: 0.005656108597285068, 13: 0.014357814131569788, 14: 0.005221023320570832, 15: 0.003915767490428124, 16: 0.00965889314305604, 18: 0.008005569091541943, 19: 0.9363035154890359}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2464808225631714, 'eval_n_examples': 24, 'eval_acc': 0.9504285122981605, 'eval_n_focus_examples': 20, 'eval_focus_acc': 0.5636883651060967, 'eval_focus_else_acc_minus_one': 0.5636883651060967, 'eval_runtime': 4.1444, 'eval_samples_per_second': 5.791, 'eval_steps_per_second': 1.448, 'epoch': 7.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 12:49:17,269 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-791\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 12:49:17,269 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-791\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 12:49:17,271 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-791/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 12:49:17,271 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-791/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 12:49:20,268 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-791/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 12:49:20,268 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-791/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 12:51:01,220 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 12:51:01,220 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 12:51:01,220 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 12:51:01,220 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 12:51:01,220 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 12:51:01,220 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m2023-05-22 12:51:05,752 [data.ner] INFO Evaluation class prediction ratios: {0: 0.0036547163243995824, 1: 0.005656108597285068, 2: 0.003393665158371041, 4: 0.0024364775495997215, 6: 0.0003480682213713888, 8: 0.000870170553428472, 10: 0.002697528715628263, 11: 0.0006961364427427776, 12: 0.005656108597285068, 13: 0.01427079707622694, 14: 0.006961364427427776, 15: 0.00435085276714236, 16: 0.010093978419770276, 17: 0.0016533240515140967, 18: 0.008440654368256178, 19: 0.928820048729551}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2891736328601837, 'eval_n_examples': 24, 'eval_acc': 0.9463353980271298, 'eval_n_focus_examples': 22, 'eval_focus_acc': 0.5566406171354692, 'eval_focus_else_acc_minus_one': 0.5566406171354692, 'eval_runtime': 4.5334, 'eval_samples_per_second': 5.294, 'eval_steps_per_second': 1.324, 'epoch': 8.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 12:51:05,754 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-904\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 12:51:05,754 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-904\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 12:51:05,756 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-904/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 12:51:05,756 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-904/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 12:51:08,815 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-904/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 12:51:08,815 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-904/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m{'loss': 0.0336, 'learning_rate': 4.705014749262537e-05, 'epoch': 8.85}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 12:52:50,621 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 12:52:50,621 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 12:52:50,621 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 12:52:50,621 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 12:52:50,621 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 12:52:50,621 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m2023-05-22 12:52:55,775 [data.ner] INFO Evaluation class prediction ratios: {0: 0.0033066481030281933, 1: 0.006178210929342151, 2: 0.0015663069961712496, 3: 0.0003480682213713888, 4: 0.0024364775495997215, 8: 0.0009571876087713191, 10: 0.0019143752175426383, 11: 0.0020013922728854858, 12: 0.005656108597285068, 13: 0.012182387747998607, 14: 0.0051340062652279845, 15: 0.0040027845457709715, 16: 0.009745910198398886, 17: 0.0001740341106856944, 18: 0.005743125652627915, 19: 0.9386529759832927}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.3678862750530243, 'eval_n_examples': 24, 'eval_acc': 0.9484744151755088, 'eval_n_focus_examples': 21, 'eval_focus_acc': 0.5338497289671407, 'eval_focus_else_acc_minus_one': 0.5338497289671407, 'eval_runtime': 5.155, 'eval_samples_per_second': 4.656, 'eval_steps_per_second': 1.164, 'epoch': 9.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 12:52:55,777 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-1017\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 12:52:55,777 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-1017\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 12:52:55,779 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-1017/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 12:52:55,779 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-1017/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 12:52:58,770 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-1017/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 12:52:58,770 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-1017/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 12:54:42,201 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 12:54:42,201 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 12:54:42,201 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 12:54:42,201 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 12:54:42,201 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 12:54:42,201 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m2023-05-22 12:54:46,920 [data.ner] INFO Evaluation class prediction ratios: {0: 0.0037417333797424295, 1: 0.005656108597285068, 3: 0.0001740341106856944, 4: 0.004524886877828055, 6: 0.0002610511660285416, 8: 0.0009571876087713191, 10: 0.00217542638357118, 11: 0.0015663069961712496, 12: 0.0060911938739993034, 13: 0.011225200139227289, 14: 0.0077445179255134005, 15: 0.0040027845457709715, 16: 0.009832927253741734, 18: 0.010442046641141664, 19: 0.9316045945005221}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2641279995441437, 'eval_n_examples': 24, 'eval_acc': 0.9477434040107733, 'eval_n_focus_examples': 20, 'eval_focus_acc': 0.6306739401177073, 'eval_focus_else_acc_minus_one': 0.6306739401177073, 'eval_runtime': 4.7192, 'eval_samples_per_second': 5.086, 'eval_steps_per_second': 1.271, 'epoch': 10.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 12:54:46,922 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-1130\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 12:54:46,922 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-1130\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 12:54:46,923 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-1130/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 12:54:46,923 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-1130/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 12:54:49,905 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-1130/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 12:54:49,905 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-1130/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 12:56:33,287 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 12:56:33,287 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 12:56:33,287 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 12:56:33,287 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 12:56:33,287 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 12:56:33,287 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m2023-05-22 12:56:38,409 [data.ner] INFO Evaluation class prediction ratios: {0: 0.0033066481030281933, 1: 0.005743125652627915, 3: 0.0001740341106856944, 4: 0.004176818656456666, 6: 0.0001740341106856944, 8: 0.0005221023320570832, 9: 0.0003480682213713888, 10: 0.0022624434389140274, 11: 0.0015663069961712496, 12: 0.005656108597285068, 13: 0.012791507135398538, 14: 0.006700313261399234, 15: 0.004089801601113818, 16: 0.010180995475113122, 18: 0.007657500870170553, 19: 0.9346501914375217}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.34395989775657654, 'eval_n_examples': 24, 'eval_acc': 0.9543456743073211, 'eval_n_focus_examples': 19, 'eval_focus_acc': 0.6292366406048142, 'eval_focus_else_acc_minus_one': 0.6292366406048142, 'eval_runtime': 5.1229, 'eval_samples_per_second': 4.685, 'eval_steps_per_second': 1.171, 'epoch': 11.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 12:56:38,411 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-1243\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 12:56:38,411 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-1243\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 12:56:38,413 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-1243/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 12:56:38,413 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-1243/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 12:56:41,407 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-1243/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 12:56:41,407 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-1243/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 12:56:55,338 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-113] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 12:56:55,338 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-113] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 12:58:25,937 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 12:58:25,937 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 12:58:25,938 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 12:58:25,938 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 12:58:25,938 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 12:58:25,938 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m2023-05-22 12:58:30,495 [data.ner] INFO Evaluation class prediction ratios: {0: 0.003393665158371041, 1: 0.005395057431256526, 3: 0.0006091193873999304, 4: 0.0032196310476853463, 6: 0.0009571876087713191, 8: 0.0006961364427427776, 9: 0.0001740341106856944, 10: 0.0019143752175426383, 11: 0.0014792899408284023, 12: 0.005656108597285068, 13: 0.012269404803341455, 14: 0.006004176818656457, 15: 0.0040027845457709715, 16: 0.008962756700313261, 18: 0.022711451444483118, 19: 0.9225548207448659}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2271786481142044, 'eval_n_examples': 24, 'eval_acc': 0.9545597869523809, 'eval_n_focus_examples': 20, 'eval_focus_acc': 0.6279103933242742, 'eval_focus_else_acc_minus_one': 0.6279103933242742, 'eval_runtime': 4.5582, 'eval_samples_per_second': 5.265, 'eval_steps_per_second': 1.316, 'epoch': 12.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 12:58:30,497 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-1356\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 12:58:30,497 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-1356\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 12:58:30,499 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-1356/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 12:58:30,499 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-1356/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 12:58:33,493 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-1356/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 12:58:33,493 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-1356/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 12:58:47,104 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-226] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 12:58:47,104 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-226] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:00:17,793 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:00:17,793 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:00:17,793 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:00:17,793 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:00:17,793 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:00:17,793 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m2023-05-22 13:00:21,922 [data.ner] INFO Evaluation class prediction ratios: {0: 0.003480682213713888, 1: 0.005656108597285068, 3: 0.0006091193873999304, 4: 0.004698920988513749, 6: 0.0003480682213713888, 8: 0.0006961364427427776, 9: 0.0001740341106856944, 10: 0.0020013922728854858, 11: 0.0015663069961712496, 12: 0.005656108597285068, 13: 0.012182387747998607, 14: 0.006178210929342151, 15: 0.0040027845457709715, 16: 0.010180995475113122, 18: 0.019230769230769232, 19: 0.9233379742429516}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.29713574051856995, 'eval_n_examples': 24, 'eval_acc': 0.9559952556306625, 'eval_n_focus_examples': 20, 'eval_focus_acc': 0.6465663985484602, 'eval_focus_else_acc_minus_one': 0.6465663985484602, 'eval_runtime': 4.1302, 'eval_samples_per_second': 5.811, 'eval_steps_per_second': 1.453, 'epoch': 13.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:00:21,924 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-1469\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:00:21,924 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-1469\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:00:21,926 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-1469/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:00:21,926 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-1469/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:00:24,940 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-1469/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:00:24,940 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-1469/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:00:38,624 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-339] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:00:38,624 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-339] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m{'loss': 0.0147, 'learning_rate': 4.5575221238938055e-05, 'epoch': 13.27}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:02:05,376 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:02:05,376 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:02:05,376 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:02:05,376 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:02:05,377 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:02:05,377 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m2023-05-22 13:02:09,516 [data.ner] INFO Evaluation class prediction ratios: {0: 0.0032196310476853463, 1: 0.005743125652627915, 2: 0.0005221023320570832, 3: 0.0006091193873999304, 4: 0.002610511660285416, 6: 0.0001740341106856944, 8: 0.0006961364427427776, 9: 0.0003480682213713888, 10: 0.0030455969369996517, 11: 0.000870170553428472, 12: 0.005656108597285068, 13: 0.011399234249912982, 14: 0.005830142707970762, 15: 0.0040027845457709715, 16: 0.009832927253741734, 18: 0.01618517229376958, 19: 0.9292551340062652}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.358638197183609, 'eval_n_examples': 24, 'eval_acc': 0.9517008935288663, 'eval_n_focus_examples': 20, 'eval_focus_acc': 0.5927511888618395, 'eval_focus_else_acc_minus_one': 0.5927511888618395, 'eval_runtime': 4.14, 'eval_samples_per_second': 5.797, 'eval_steps_per_second': 1.449, 'epoch': 14.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:02:09,518 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-1582\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:02:09,518 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-1582\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:02:09,519 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-1582/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:02:09,519 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-1582/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:02:12,575 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-1582/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:02:12,575 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-1582/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:02:26,234 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-452] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:02:26,234 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-452] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:03:53,453 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:03:53,453 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:03:53,454 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:03:53,454 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:03:53,454 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:03:53,454 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m2023-05-22 13:03:58,225 [data.ner] INFO Evaluation class prediction ratios: {0: 0.003132613992342499, 1: 0.005569091541942221, 2: 0.0005221023320570832, 3: 0.0006091193873999304, 4: 0.0027845457709711106, 6: 0.0001740341106856944, 8: 0.0009571876087713191, 9: 0.0010442046641141664, 10: 0.0019143752175426383, 11: 0.0018273581621997912, 12: 0.005656108597285068, 13: 0.009136790810998956, 14: 0.004089801601113818, 15: 0.0040027845457709715, 16: 0.009745910198398886, 17: 0.0007831534980856248, 18: 0.00652627915071354, 19: 0.9415245388096067}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.43496081233024597, 'eval_n_examples': 24, 'eval_acc': 0.947362702202479, 'eval_n_focus_examples': 19, 'eval_focus_acc': 0.6154763265852408, 'eval_focus_else_acc_minus_one': 0.6154763265852408, 'eval_runtime': 4.7719, 'eval_samples_per_second': 5.029, 'eval_steps_per_second': 1.257, 'epoch': 15.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:03:58,227 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-1695\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:03:58,227 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-1695\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:03:58,228 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-1695/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:03:58,228 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-1695/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:04:01,266 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-1695/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:04:01,266 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-1695/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:04:14,859 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-565] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:04:14,859 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-565] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:05:42,908 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:05:42,908 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:05:42,909 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:05:42,909 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:05:42,909 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:05:42,909 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m2023-05-22 13:05:47,178 [data.ner] INFO Evaluation class prediction ratios: {0: 0.003567699269056735, 1: 0.005656108597285068, 2: 0.0002610511660285416, 3: 0.0006091193873999304, 4: 0.0028715628263139576, 6: 0.0001740341106856944, 8: 0.0005221023320570832, 9: 0.0001740341106856944, 10: 0.0018273581621997912, 11: 0.0019143752175426383, 12: 0.005656108597285068, 13: 0.010180995475113122, 14: 0.006961364427427776, 15: 0.004524886877828055, 16: 0.009484859032370344, 17: 0.001740341106856944, 18: 0.01644622345979812, 19: 0.9274277758440654}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.3600122034549713, 'eval_n_examples': 24, 'eval_acc': 0.9465333729747164, 'eval_n_focus_examples': 20, 'eval_focus_acc': 0.5896639917536539, 'eval_focus_else_acc_minus_one': 0.5896639917536539, 'eval_runtime': 4.2705, 'eval_samples_per_second': 5.62, 'eval_steps_per_second': 1.405, 'epoch': 16.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:05:47,180 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-1808\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:05:47,180 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-1808\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:05:47,182 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-1808/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:05:47,182 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-1808/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:05:50,203 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-1808/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:05:50,203 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-1808/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:06:03,148 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-678] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:06:03,148 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-678] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:07:33,005 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:07:33,005 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:07:33,005 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:07:33,006 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:07:33,005 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:07:33,006 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m2023-05-22 13:07:37,739 [data.ner] INFO Evaluation class prediction ratios: {0: 0.0028715628263139576, 1: 0.005743125652627915, 2: 0.0010442046641141664, 3: 0.0006091193873999304, 4: 0.0022624434389140274, 5: 0.0001740341106856944, 6: 0.0001740341106856944, 8: 0.0005221023320570832, 9: 0.0003480682213713888, 10: 0.0023494604942568744, 11: 0.0018273581621997912, 12: 0.005656108597285068, 13: 0.011225200139227289, 14: 0.006004176818656457, 15: 0.0040027845457709715, 16: 0.009571876087713193, 18: 0.01809954751131222, 19: 0.9275147928994083}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.27797672152519226, 'eval_n_examples': 24, 'eval_acc': 0.9539457914361954, 'eval_n_focus_examples': 20, 'eval_focus_acc': 0.6053923105452357, 'eval_focus_else_acc_minus_one': 0.6053923105452357, 'eval_runtime': 4.7347, 'eval_samples_per_second': 5.069, 'eval_steps_per_second': 1.267, 'epoch': 17.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:07:37,741 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-1921\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:07:37,741 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-1921\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:07:37,743 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-1921/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:07:37,743 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-1921/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:07:40,755 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-1921/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:07:40,755 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-1921/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:07:54,424 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-791] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:07:54,424 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-791] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m{'loss': 0.0082, 'learning_rate': 4.410029498525074e-05, 'epoch': 17.7}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:09:25,258 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:09:25,258 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:09:25,258 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:09:25,258 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:09:25,258 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:09:25,258 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m2023-05-22 13:09:29,517 [data.ner] INFO Evaluation class prediction ratios: {0: 0.0024364775495997215, 1: 0.005395057431256526, 2: 0.0010442046641141664, 3: 0.000435085276714236, 4: 0.00217542638357118, 5: 0.0001740341106856944, 6: 0.000435085276714236, 8: 0.0006091193873999304, 9: 0.0003480682213713888, 10: 0.0016533240515140967, 11: 0.0018273581621997912, 12: 0.005656108597285068, 13: 0.013139575356769927, 14: 0.006787330316742082, 15: 0.0040027845457709715, 16: 0.009136790810998956, 18: 0.01522798468499826, 19: 0.9295161851722937}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.3905765116214752, 'eval_n_examples': 24, 'eval_acc': 0.9491741807317631, 'eval_n_focus_examples': 20, 'eval_focus_acc': 0.5942181664903728, 'eval_focus_else_acc_minus_one': 0.5942181664903728, 'eval_runtime': 4.2602, 'eval_samples_per_second': 5.634, 'eval_steps_per_second': 1.408, 'epoch': 18.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:09:29,519 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-2034\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:09:29,519 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-2034\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:09:29,521 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-2034/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:09:29,521 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-2034/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:09:32,543 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-2034/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:09:32,543 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-2034/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:09:46,202 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-904] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:09:46,202 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-904] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:11:16,874 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:11:16,874 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:11:16,875 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:11:16,875 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:11:16,875 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:11:16,875 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m2023-05-22 13:11:21,961 [data.ner] INFO Evaluation class prediction ratios: {0: 0.0032196310476853463, 1: 0.005656108597285068, 2: 0.0006961364427427776, 3: 0.0006091193873999304, 4: 0.0033066481030281933, 5: 0.000435085276714236, 6: 0.000435085276714236, 8: 0.0007831534980856248, 9: 0.0003480682213713888, 10: 0.00217542638357118, 11: 0.0018273581621997912, 12: 0.0064392620953706925, 13: 0.013313609467455622, 14: 0.006787330316742082, 15: 0.004524886877828055, 16: 0.010006961364427427, 17: 0.0006961364427427776, 18: 0.0263661677688827, 19: 0.9123738252697529}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.319676011800766, 'eval_n_examples': 24, 'eval_acc': 0.9513710997443375, 'eval_n_focus_examples': 20, 'eval_focus_acc': 0.6565291238107076, 'eval_focus_else_acc_minus_one': 0.6565291238107076, 'eval_runtime': 5.0874, 'eval_samples_per_second': 4.718, 'eval_steps_per_second': 1.179, 'epoch': 19.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:11:21,963 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-2147\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:11:21,963 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-2147\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:11:21,965 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-2147/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:11:21,965 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-2147/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:11:25,119 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-2147/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:11:25,119 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-2147/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:11:38,768 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-1017] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:11:38,768 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-1017] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:13:09,110 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:13:09,110 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:13:09,110 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:13:09,110 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:13:09,110 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:13:09,110 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m2023-05-22 13:13:14,235 [data.ner] INFO Evaluation class prediction ratios: {0: 0.0027845457709711106, 1: 0.005743125652627915, 3: 0.0006091193873999304, 4: 0.0033066481030281933, 6: 0.000435085276714236, 8: 0.0006961364427427776, 9: 0.0003480682213713888, 10: 0.0022624434389140274, 11: 0.0020013922728854858, 12: 0.006178210929342151, 13: 0.012791507135398538, 14: 0.007048381482770623, 15: 0.004698920988513749, 16: 0.009571876087713193, 17: 0.0023494604942568744, 18: 0.0201009397841977, 19: 0.919074138531152}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.26426127552986145, 'eval_n_examples': 24, 'eval_acc': 0.953945153266031, 'eval_n_focus_examples': 23, 'eval_focus_acc': 0.5491056523347532, 'eval_focus_else_acc_minus_one': 0.5491056523347532, 'eval_runtime': 5.1261, 'eval_samples_per_second': 4.682, 'eval_steps_per_second': 1.17, 'epoch': 20.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:13:14,237 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-2260\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:13:14,237 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-2260\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:13:14,239 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-2260/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:13:14,239 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-2260/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:13:17,250 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-2260/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:13:17,250 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-2260/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:13:29,943 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-1130] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:13:29,943 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-1130] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:15:00,272 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:15:00,272 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:15:00,273 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:15:00,273 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:15:00,273 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:15:00,273 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m2023-05-22 13:15:04,794 [data.ner] INFO Evaluation class prediction ratios: {0: 0.004785938043856596, 1: 0.005221023320570832, 2: 0.0005221023320570832, 3: 0.0006091193873999304, 4: 0.0024364775495997215, 6: 0.0006091193873999304, 8: 0.0005221023320570832, 9: 0.0003480682213713888, 10: 0.0015663069961712496, 11: 0.0016533240515140967, 12: 0.0064392620953706925, 13: 0.011138183083884442, 14: 0.006961364427427776, 15: 0.004524886877828055, 16: 0.010355029585798817, 18: 0.03437173686042464, 19: 0.9079359554472677}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.22443978488445282, 'eval_n_examples': 24, 'eval_acc': 0.9534752611654814, 'eval_n_focus_examples': 21, 'eval_focus_acc': 0.5702349183864779, 'eval_focus_else_acc_minus_one': 0.5702349183864779, 'eval_runtime': 4.5224, 'eval_samples_per_second': 5.307, 'eval_steps_per_second': 1.327, 'epoch': 21.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:15:04,796 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-2373\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:15:04,796 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-2373\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:15:04,798 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-2373/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:15:04,798 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-2373/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:15:07,845 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-2373/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:15:07,845 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-2373/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:15:22,049 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-1243] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:15:22,049 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-1243] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:16:52,337 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:16:52,337 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:16:52,337 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:16:52,337 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:16:52,337 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:16:52,337 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m2023-05-22 13:16:56,817 [data.ner] INFO Evaluation class prediction ratios: {0: 0.003393665158371041, 1: 0.005656108597285068, 2: 0.0005221023320570832, 3: 0.0005221023320570832, 4: 0.002610511660285416, 6: 0.0003480682213713888, 8: 0.0005221023320570832, 9: 0.0003480682213713888, 10: 0.0012182387747998607, 11: 0.0018273581621997912, 12: 0.005830142707970762, 13: 0.01305255830142708, 14: 0.0051340062652279845, 15: 0.00495997215454229, 16: 0.010268012530455969, 17: 0.000435085276714236, 18: 0.00991994430908458, 19: 0.9334319526627219}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.37988853454589844, 'eval_n_examples': 24, 'eval_acc': 0.9466101933161477, 'eval_n_focus_examples': 20, 'eval_focus_acc': 0.5564399772294398, 'eval_focus_else_acc_minus_one': 0.5564399772294398, 'eval_runtime': 4.4811, 'eval_samples_per_second': 5.356, 'eval_steps_per_second': 1.339, 'epoch': 22.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:16:56,819 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-2486\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:16:56,819 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-2486\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:16:56,821 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-2486/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:16:56,821 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-2486/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:16:59,870 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-2486/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:16:59,870 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-2486/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:17:13,495 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-1356] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:17:13,495 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-1356] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m{'loss': 0.008, 'learning_rate': 4.262536873156342e-05, 'epoch': 22.12}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:18:44,028 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:18:44,028 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:18:44,028 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:18:44,028 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:18:44,028 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:18:44,028 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m2023-05-22 13:18:48,511 [data.ner] INFO Evaluation class prediction ratios: {0: 0.0027845457709711106, 1: 0.005656108597285068, 2: 0.0009571876087713191, 3: 0.0006091193873999304, 4: 0.00217542638357118, 6: 0.000435085276714236, 8: 0.0006091193873999304, 9: 0.0003480682213713888, 10: 0.0013922728854855553, 11: 0.0018273581621997912, 12: 0.005656108597285068, 13: 0.01209537069265576, 14: 0.0060911938739993034, 15: 0.0040027845457709715, 16: 0.009571876087713193, 18: 0.007918552036199095, 19: 0.9378698224852071}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.3990955650806427, 'eval_n_examples': 24, 'eval_acc': 0.9480180529112108, 'eval_n_focus_examples': 20, 'eval_focus_acc': 0.5742371706856695, 'eval_focus_else_acc_minus_one': 0.5742371706856695, 'eval_runtime': 4.4832, 'eval_samples_per_second': 5.353, 'eval_steps_per_second': 1.338, 'epoch': 23.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:18:48,512 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-2599\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:18:48,512 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-2599\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:18:48,514 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-2599/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:18:48,514 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-2599/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:18:51,510 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-2599/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:18:51,510 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-2599/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:19:05,545 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-1469] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:19:05,545 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-1469] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:20:35,804 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:20:35,804 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:20:35,804 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:20:35,804 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:20:35,804 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:20:35,804 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m2023-05-22 13:20:40,117 [data.ner] INFO Evaluation class prediction ratios: {0: 0.003567699269056735, 1: 0.005656108597285068, 2: 0.0006091193873999304, 3: 0.0006091193873999304, 4: 0.002088409328228333, 6: 0.0006961364427427776, 8: 0.0006091193873999304, 9: 0.0003480682213713888, 10: 0.0018273581621997912, 11: 0.001740341106856944, 12: 0.005656108597285068, 13: 0.011834319526627219, 14: 0.00495997215454229, 15: 0.004437869822485207, 16: 0.010355029585798817, 18: 0.015489035851026801, 19: 0.9295161851722937}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.3684145212173462, 'eval_n_examples': 24, 'eval_acc': 0.9522623881973846, 'eval_n_focus_examples': 20, 'eval_focus_acc': 0.610609590094049, 'eval_focus_else_acc_minus_one': 0.610609590094049, 'eval_runtime': 4.3134, 'eval_samples_per_second': 5.564, 'eval_steps_per_second': 1.391, 'epoch': 24.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:20:40,119 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-2712\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:20:40,119 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-2712\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:20:40,120 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-2712/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:20:40,120 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-2712/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:20:43,144 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-2712/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:20:43,144 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-2712/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:20:56,335 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-1582] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:20:56,335 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-1582] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:22:26,872 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:22:26,872 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:22:26,872 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:22:26,872 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:22:26,872 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:22:26,872 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m2023-05-22 13:22:31,634 [data.ner] INFO Evaluation class prediction ratios: {0: 0.004437869822485207, 1: 0.005917159763313609, 2: 0.0003480682213713888, 3: 0.0007831534980856248, 4: 0.003132613992342499, 6: 0.0012182387747998607, 8: 0.0010442046641141664, 10: 0.0023494604942568744, 11: 0.001740341106856944, 12: 0.005656108597285068, 13: 0.011747302471284372, 14: 0.006352245040027845, 15: 0.004176818656456666, 16: 0.008788722589627568, 18: 0.014966933518969718, 19: 0.9273407587887226}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.3099804222583771, 'eval_n_examples': 24, 'eval_acc': 0.9555104871334009, 'eval_n_focus_examples': 20, 'eval_focus_acc': 0.6336343295654541, 'eval_focus_else_acc_minus_one': 0.6336343295654541, 'eval_runtime': 4.763, 'eval_samples_per_second': 5.039, 'eval_steps_per_second': 1.26, 'epoch': 25.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:22:31,636 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-2825\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:22:31,636 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-2825\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:22:31,638 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-2825/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:22:31,638 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-2825/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:22:34,659 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-2825/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:22:34,659 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-2825/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:22:48,274 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-1695] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:22:48,274 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-1695] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:24:16,909 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:24:16,909 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:24:16,909 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:24:16,909 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:24:16,909 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:24:16,909 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m2023-05-22 13:24:20,956 [data.ner] INFO Evaluation class prediction ratios: {0: 0.003480682213713888, 1: 0.005395057431256526, 2: 0.0009571876087713191, 3: 0.0006091193873999304, 4: 0.002088409328228333, 6: 0.0010442046641141664, 8: 0.0007831534980856248, 9: 0.0002610511660285416, 10: 0.002523494604942569, 11: 0.0009571876087713191, 12: 0.005656108597285068, 13: 0.012530455969369997, 14: 0.006352245040027845, 15: 0.0040027845457709715, 16: 0.007309432648799165, 17: 8.70170553428472e-05, 18: 0.01957883745214062, 19: 0.9263835711799513}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2916954457759857, 'eval_n_examples': 24, 'eval_acc': 0.9505585400187867, 'eval_n_focus_examples': 20, 'eval_focus_acc': 0.5865828567728657, 'eval_focus_else_acc_minus_one': 0.5865828567728657, 'eval_runtime': 4.0479, 'eval_samples_per_second': 5.929, 'eval_steps_per_second': 1.482, 'epoch': 26.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:24:20,958 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-2938\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:24:20,958 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-2938\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:24:20,960 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-2938/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:24:20,960 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-2938/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:24:23,994 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-2938/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:24:23,994 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-2938/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:24:37,637 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-1808] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:24:37,637 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-1808] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m{'loss': 0.0041, 'learning_rate': 4.115044247787611e-05, 'epoch': 26.55}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:26:04,454 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:26:04,454 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:26:04,455 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:26:04,455 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:26:04,455 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:26:04,455 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m2023-05-22 13:26:08,563 [data.ner] INFO Evaluation class prediction ratios: {0: 0.003567699269056735, 1: 0.005656108597285068, 2: 0.0022624434389140274, 3: 0.001305255830142708, 4: 0.0019143752175426383, 5: 0.0003480682213713888, 6: 0.002088409328228333, 8: 0.0006961364427427776, 9: 0.0003480682213713888, 10: 0.005482074486599374, 11: 0.000870170553428472, 12: 0.005656108597285068, 13: 0.009310824921684651, 14: 0.007048381482770623, 15: 0.00435085276714236, 16: 0.010268012530455969, 17: 0.004611903933170901, 18: 0.02392969021928298, 19: 0.9102854159415246}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.3621314465999603, 'eval_n_examples': 24, 'eval_acc': 0.9451692750879334, 'eval_n_focus_examples': 23, 'eval_focus_acc': 0.5217878838793555, 'eval_focus_else_acc_minus_one': 0.5217878838793555, 'eval_runtime': 4.11, 'eval_samples_per_second': 5.839, 'eval_steps_per_second': 1.46, 'epoch': 27.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:26:08,566 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-3051\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:26:08,566 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-3051\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:26:08,567 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-3051/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:26:08,567 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-3051/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:26:11,579 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-3051/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:26:11,579 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-3051/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:26:25,222 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-1921] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:26:25,222 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-1921] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:27:52,035 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:27:52,035 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:27:52,035 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:27:52,036 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:27:52,035 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:27:52,036 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m2023-05-22 13:27:56,129 [data.ner] INFO Evaluation class prediction ratios: {0: 0.0030455969369996517, 1: 0.005656108597285068, 2: 0.0023494604942568744, 3: 0.0013922728854855553, 4: 0.002088409328228333, 5: 8.70170553428472e-05, 6: 0.0006091193873999304, 8: 0.000870170553428472, 10: 0.0010442046641141664, 11: 0.001305255830142708, 12: 0.00652627915071354, 13: 0.014357814131569788, 14: 0.005656108597285068, 15: 0.004176818656456666, 16: 0.008788722589627568, 17: 0.0001740341106856944, 18: 0.017490428123912286, 19: 0.9243821789070658}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.38260307908058167, 'eval_n_examples': 24, 'eval_acc': 0.9462708381405691, 'eval_n_focus_examples': 21, 'eval_focus_acc': 0.5640617624428876, 'eval_focus_else_acc_minus_one': 0.5640617624428876, 'eval_runtime': 4.0941, 'eval_samples_per_second': 5.862, 'eval_steps_per_second': 1.466, 'epoch': 28.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:27:56,131 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-3164\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:27:56,131 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-3164\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:27:56,132 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-3164/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:27:56,132 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-3164/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:27:59,141 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-3164/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:27:59,141 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-3164/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:28:12,816 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-2034] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:28:12,816 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-2034] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:29:41,071 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:29:41,071 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:29:41,071 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:29:41,071 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:29:41,071 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:29:41,071 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m2023-05-22 13:29:45,871 [data.ner] INFO Evaluation class prediction ratios: {0: 0.0030455969369996517, 1: 0.005221023320570832, 2: 0.0012182387747998607, 3: 0.000870170553428472, 4: 0.002610511660285416, 6: 0.000435085276714236, 8: 0.0005221023320570832, 9: 0.0003480682213713888, 10: 0.0013922728854855553, 11: 0.0015663069961712496, 12: 0.005656108597285068, 13: 0.012965541246084232, 14: 0.0037417333797424295, 15: 0.0032196310476853463, 16: 0.008788722589627568, 18: 0.037243299686738604, 19: 0.911155586494953}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.31981536746025085, 'eval_n_examples': 24, 'eval_acc': 0.953410967625875, 'eval_n_focus_examples': 20, 'eval_focus_acc': 0.6330864835568065, 'eval_focus_else_acc_minus_one': 0.6330864835568065, 'eval_runtime': 4.8001, 'eval_samples_per_second': 5.0, 'eval_steps_per_second': 1.25, 'epoch': 29.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:29:45,872 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-3277\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:29:45,872 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-3277\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:29:45,874 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-3277/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:29:45,874 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-3277/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:29:48,887 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-3277/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:29:48,887 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-3277/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:30:02,537 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-2260] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:30:02,537 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-2260] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:31:32,115 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:31:32,115 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:31:32,115 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:31:32,115 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:31:32,115 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:31:32,115 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m2023-05-22 13:31:36,787 [data.ner] INFO Evaluation class prediction ratios: {0: 0.0032196310476853463, 1: 0.005221023320570832, 2: 0.000435085276714236, 3: 0.0001740341106856944, 4: 0.002523494604942569, 6: 0.0003480682213713888, 8: 0.0005221023320570832, 9: 0.0003480682213713888, 10: 0.0013922728854855553, 11: 0.0016533240515140967, 12: 0.005656108597285068, 13: 0.012182387747998607, 14: 0.004698920988513749, 15: 0.004524886877828055, 16: 0.007570483814827706, 18: 0.0033066481030281933, 19: 0.9462234597981204}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.532485842704773, 'eval_n_examples': 24, 'eval_acc': 0.9478376676584691, 'eval_n_focus_examples': 19, 'eval_focus_acc': 0.5458100957045741, 'eval_focus_else_acc_minus_one': 0.5458100957045741, 'eval_runtime': 4.6725, 'eval_samples_per_second': 5.136, 'eval_steps_per_second': 1.284, 'epoch': 30.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:31:36,789 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-3390\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:31:36,789 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-3390\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:31:36,790 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-3390/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:31:36,790 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-3390/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:31:39,821 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-3390/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:31:39,821 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-3390/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:31:53,453 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-2373] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:31:53,453 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-2373] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m{'loss': 0.0044, 'learning_rate': 3.967551622418879e-05, 'epoch': 30.97}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:33:23,464 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:33:23,464 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:33:23,464 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:33:23,464 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:33:23,464 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:33:23,464 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m2023-05-22 13:33:28,090 [data.ner] INFO Evaluation class prediction ratios: {0: 0.0033066481030281933, 1: 0.007657500870170553, 2: 0.0022624434389140274, 3: 0.001740341106856944, 4: 0.002088409328228333, 5: 0.0002610511660285416, 6: 0.0007831534980856248, 8: 0.0006961364427427776, 9: 0.0003480682213713888, 10: 0.0024364775495997215, 11: 0.0013922728854855553, 12: 0.005656108597285068, 13: 0.014357814131569788, 14: 0.007570483814827706, 15: 0.004524886877828055, 16: 0.008962756700313261, 17: 0.0002610511660285416, 18: 0.01896971806474069, 19: 0.9167246780368953}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.29448890686035156, 'eval_n_examples': 24, 'eval_acc': 0.9566520356943992, 'eval_n_focus_examples': 21, 'eval_focus_acc': 0.59866029696436, 'eval_focus_else_acc_minus_one': 0.59866029696436, 'eval_runtime': 4.6274, 'eval_samples_per_second': 5.186, 'eval_steps_per_second': 1.297, 'epoch': 31.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:33:28,092 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-3503\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:33:28,092 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-3503\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:33:28,094 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-3503/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:33:28,094 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-3503/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:33:31,136 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-3503/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:33:31,136 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-3503/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:33:44,892 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-2486] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:33:44,892 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-2486] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:35:14,819 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:35:14,819 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:35:14,820 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:35:14,820 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:35:14,820 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:35:14,820 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m2023-05-22 13:35:19,196 [data.ner] INFO Evaluation class prediction ratios: {0: 0.0033066481030281933, 1: 0.005743125652627915, 2: 0.0010442046641141664, 3: 0.0010442046641141664, 4: 0.0029585798816568047, 8: 0.0006091193873999304, 9: 0.0003480682213713888, 10: 0.0020013922728854858, 11: 0.0012182387747998607, 12: 0.005743125652627915, 13: 0.014096762965541246, 14: 0.007831534980856248, 15: 0.004524886877828055, 16: 0.009571876087713193, 18: 0.013922728854855551, 19: 0.9260355029585798}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2566688358783722, 'eval_n_examples': 24, 'eval_acc': 0.9599915444103946, 'eval_n_focus_examples': 19, 'eval_focus_acc': 0.6633403720412484, 'eval_focus_else_acc_minus_one': 0.6633403720412484, 'eval_runtime': 4.3776, 'eval_samples_per_second': 5.482, 'eval_steps_per_second': 1.371, 'epoch': 32.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:35:19,198 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-3616\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:35:19,198 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-3616\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:35:19,200 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-3616/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:35:19,200 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-3616/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:35:22,195 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-3616/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:35:22,195 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-3616/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:35:35,811 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-2147] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:35:35,811 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-2147] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:37:06,107 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:37:06,107 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:37:06,107 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:37:06,107 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:37:06,107 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:37:06,107 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m2023-05-22 13:37:10,625 [data.ner] INFO Evaluation class prediction ratios: {0: 0.003393665158371041, 1: 0.005656108597285068, 2: 0.0006961364427427776, 3: 0.0001740341106856944, 4: 0.0032196310476853463, 6: 0.0003480682213713888, 8: 0.0006961364427427776, 9: 0.0003480682213713888, 10: 0.0020013922728854858, 11: 0.0011312217194570137, 12: 0.005743125652627915, 13: 0.014183780020884093, 14: 0.006700313261399234, 15: 0.004524886877828055, 16: 0.009397841977027498, 18: 0.01461886529759833, 19: 0.9271667246780368}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.37068721652030945, 'eval_n_examples': 24, 'eval_acc': 0.9564838816121171, 'eval_n_focus_examples': 20, 'eval_focus_acc': 0.6036564292800481, 'eval_focus_else_acc_minus_one': 0.6036564292800481, 'eval_runtime': 4.5185, 'eval_samples_per_second': 5.312, 'eval_steps_per_second': 1.328, 'epoch': 33.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:37:10,627 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-3729\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:37:10,627 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-3729\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:37:10,628 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-3729/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:37:10,628 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-3729/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:37:13,643 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-3729/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:37:13,643 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-3729/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:37:27,271 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-2599] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:37:27,271 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-2599] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:38:57,449 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:38:57,449 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:38:57,449 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:38:57,449 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:38:57,449 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:38:57,449 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m2023-05-22 13:39:01,793 [data.ner] INFO Evaluation class prediction ratios: {0: 0.0028715628263139576, 1: 0.005482074486599374, 3: 0.0005221023320570832, 4: 0.0037417333797424295, 6: 0.0001740341106856944, 8: 0.0005221023320570832, 9: 0.0003480682213713888, 10: 0.0015663069961712496, 11: 0.001305255830142708, 12: 0.005656108597285068, 13: 0.012443438914027148, 14: 0.005743125652627915, 15: 0.004524886877828055, 16: 0.009397841977027498, 17: 0.0053080403759136795, 18: 0.018621649843369302, 19: 0.9217716672467804}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.3541835844516754, 'eval_n_examples': 24, 'eval_acc': 0.9480822043988374, 'eval_n_focus_examples': 21, 'eval_focus_acc': 0.56644701305999, 'eval_focus_else_acc_minus_one': 0.56644701305999, 'eval_runtime': 4.3451, 'eval_samples_per_second': 5.523, 'eval_steps_per_second': 1.381, 'epoch': 34.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:39:01,795 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-3842\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:39:01,795 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-3842\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:39:01,797 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-3842/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:39:01,797 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-3842/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:39:04,777 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-3842/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:39:04,777 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-3842/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:39:18,412 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-2712] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:39:18,412 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-2712] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:40:48,369 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:40:48,369 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:40:48,369 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:40:48,369 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:40:48,369 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:40:48,369 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m2023-05-22 13:40:52,648 [data.ner] INFO Evaluation class prediction ratios: {0: 0.0023494604942568744, 1: 0.0053080403759136795, 2: 0.0014792899408284023, 3: 0.0001740341106856944, 4: 0.0022624434389140274, 6: 0.0001740341106856944, 8: 0.0006091193873999304, 9: 0.0003480682213713888, 10: 0.0013922728854855553, 11: 0.0011312217194570137, 12: 0.005656108597285068, 13: 0.013400626522798468, 14: 0.0064392620953706925, 15: 0.004524886877828055, 16: 0.009484859032370344, 18: 0.014183780020884093, 19: 0.9310824921684651}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.4644593298435211, 'eval_n_examples': 24, 'eval_acc': 0.9482393665173636, 'eval_n_focus_examples': 20, 'eval_focus_acc': 0.583681865905695, 'eval_focus_else_acc_minus_one': 0.583681865905695, 'eval_runtime': 4.2797, 'eval_samples_per_second': 5.608, 'eval_steps_per_second': 1.402, 'epoch': 35.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:40:52,650 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-3955\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:40:52,650 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-3955\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:40:52,652 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-3955/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:40:52,652 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-3955/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:40:55,638 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-3955/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:40:55,638 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-3955/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:41:09,472 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-2825] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:41:09,472 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-2825] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m{'loss': 0.0024, 'learning_rate': 3.8200589970501475e-05, 'epoch': 35.4}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:42:39,327 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:42:39,327 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:42:39,328 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:42:39,328 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:42:39,328 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:42:39,328 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m2023-05-22 13:42:44,378 [data.ner] INFO Evaluation class prediction ratios: {0: 0.0033066481030281933, 1: 0.005656108597285068, 2: 0.000870170553428472, 3: 0.000870170553428472, 4: 0.0027845457709711106, 6: 0.0002610511660285416, 8: 0.0006091193873999304, 9: 0.0003480682213713888, 10: 0.002610511660285416, 11: 0.001305255830142708, 12: 0.005656108597285068, 13: 0.012008353637312914, 14: 0.005917159763313609, 15: 0.004524886877828055, 16: 0.009223807866341803, 17: 0.0003480682213713888, 18: 0.011225200139227289, 19: 0.9324747650539505}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.4521774351596832, 'eval_n_examples': 24, 'eval_acc': 0.9472485639067322, 'eval_n_focus_examples': 20, 'eval_focus_acc': 0.5724807007930625, 'eval_focus_else_acc_minus_one': 0.5724807007930625, 'eval_runtime': 5.0515, 'eval_samples_per_second': 4.751, 'eval_steps_per_second': 1.188, 'epoch': 36.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:42:44,381 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-4068\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:42:44,381 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-4068\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:42:44,382 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-4068/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:42:44,382 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-4068/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:42:47,376 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-4068/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:42:47,376 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-4068/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:43:01,029 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-2938] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:43:01,029 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-2938] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:44:31,028 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:44:31,028 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:44:31,028 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:44:31,028 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:44:31,028 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:44:31,028 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m2023-05-22 13:44:35,152 [data.ner] INFO Evaluation class prediction ratios: {0: 0.0019143752175426383, 1: 0.005221023320570832, 2: 0.000435085276714236, 3: 0.0009571876087713191, 4: 0.0019143752175426383, 6: 0.0003480682213713888, 8: 0.0005221023320570832, 9: 0.0003480682213713888, 10: 0.0003480682213713888, 11: 0.0011312217194570137, 12: 0.005656108597285068, 13: 0.01305255830142708, 14: 0.0053080403759136795, 15: 0.004176818656456666, 16: 0.009571876087713193, 17: 0.0029585798816568047, 18: 0.0033066481030281933, 19: 0.9428297946397494}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.5016303658485413, 'eval_n_examples': 24, 'eval_acc': 0.9471312581244327, 'eval_n_focus_examples': 20, 'eval_focus_acc': 0.5445716949791694, 'eval_focus_else_acc_minus_one': 0.5445716949791694, 'eval_runtime': 4.1251, 'eval_samples_per_second': 5.818, 'eval_steps_per_second': 1.454, 'epoch': 37.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:44:35,154 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-4181\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:44:35,154 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-4181\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:44:35,156 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-4181/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:44:35,156 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-4181/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:44:38,222 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-4181/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:44:38,222 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-4181/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:44:51,860 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-3051] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:44:51,860 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-3051] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:46:21,799 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:46:21,799 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:46:21,799 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:46:21,799 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:46:21,799 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:46:21,799 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m2023-05-22 13:46:26,871 [data.ner] INFO Evaluation class prediction ratios: {0: 0.0033066481030281933, 1: 0.005221023320570832, 2: 0.0002610511660285416, 3: 0.0016533240515140967, 4: 0.0028715628263139576, 6: 0.0001740341106856944, 8: 0.0005221023320570832, 9: 0.0001740341106856944, 10: 0.0030455969369996517, 11: 0.001305255830142708, 12: 0.005656108597285068, 13: 0.010268012530455969, 14: 0.004872955099199443, 15: 0.004524886877828055, 16: 0.009745910198398886, 17: 0.0011312217194570137, 18: 0.016881308736512356, 19: 0.9283849634528367}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.3461017608642578, 'eval_n_examples': 24, 'eval_acc': 0.9517455840481442, 'eval_n_focus_examples': 20, 'eval_focus_acc': 0.6594673569215885, 'eval_focus_else_acc_minus_one': 0.6594673569215885, 'eval_runtime': 5.0733, 'eval_samples_per_second': 4.731, 'eval_steps_per_second': 1.183, 'epoch': 38.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:46:26,873 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-4294\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:46:26,873 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-4294\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:46:26,875 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-4294/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:46:26,875 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-4294/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:46:29,865 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-4294/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:46:29,865 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-4294/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:46:43,915 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-3164] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:46:43,915 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-3164] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:48:14,438 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:48:14,438 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:48:14,438 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:48:14,438 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:48:14,438 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:48:14,438 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m2023-05-22 13:48:19,864 [data.ner] INFO Evaluation class prediction ratios: {0: 0.0016533240515140967, 1: 0.005656108597285068, 2: 0.0006091193873999304, 3: 0.000870170553428472, 4: 0.003132613992342499, 6: 0.0001740341106856944, 8: 0.0005221023320570832, 9: 0.0003480682213713888, 10: 0.0015663069961712496, 11: 0.0019143752175426383, 12: 0.005656108597285068, 13: 0.011051166028541594, 14: 0.0073964497041420114, 15: 0.004437869822485207, 16: 0.00652627915071354, 17: 0.00435085276714236, 18: 0.019143752175426385, 19: 0.9249912982944657}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.33140644431114197, 'eval_n_examples': 24, 'eval_acc': 0.9455261693347783, 'eval_n_focus_examples': 21, 'eval_focus_acc': 0.5468446237462024, 'eval_focus_else_acc_minus_one': 0.5468446237462024, 'eval_runtime': 5.4268, 'eval_samples_per_second': 4.423, 'eval_steps_per_second': 1.106, 'epoch': 39.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:48:19,866 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-4407\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:48:19,866 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-4407\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:48:19,868 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-4407/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:48:19,868 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-4407/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:48:23,097 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-4407/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:48:23,097 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-4407/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:48:35,942 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-3277] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:48:35,942 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-3277] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m{'loss': 0.0023, 'learning_rate': 3.672566371681416e-05, 'epoch': 39.82}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:50:06,295 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:50:06,295 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:50:06,295 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:50:06,295 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:50:06,295 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:50:06,295 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m2023-05-22 13:50:11,380 [data.ner] INFO Evaluation class prediction ratios: {0: 0.0022624434389140274, 1: 0.005395057431256526, 3: 0.0011312217194570137, 4: 0.0036547163243995824, 6: 8.70170553428472e-05, 8: 0.0006091193873999304, 9: 0.0003480682213713888, 10: 0.0020013922728854858, 11: 0.001305255830142708, 12: 0.005656108597285068, 13: 0.01148625130525583, 14: 0.0060911938739993034, 15: 0.004263835711799513, 16: 0.0064392620953706925, 17: 0.0002610511660285416, 18: 0.00965889314305604, 19: 0.9393491124260355}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.38070836663246155, 'eval_n_examples': 24, 'eval_acc': 0.9484503752305619, 'eval_n_focus_examples': 19, 'eval_focus_acc': 0.5590960840719985, 'eval_focus_else_acc_minus_one': 0.5590960840719985, 'eval_runtime': 5.0857, 'eval_samples_per_second': 4.719, 'eval_steps_per_second': 1.18, 'epoch': 40.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:50:11,382 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-4520\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:50:11,382 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-4520\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:50:11,384 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-4520/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:50:11,384 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-4520/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:50:14,406 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-4520/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:50:14,406 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-4520/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:50:27,399 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-3390] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:50:27,399 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-3390] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:51:54,079 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:51:54,079 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:51:54,079 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:51:54,079 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:51:54,079 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:51:54,079 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m2023-05-22 13:51:58,448 [data.ner] INFO Evaluation class prediction ratios: {0: 0.003132613992342499, 1: 0.005656108597285068, 2: 0.000435085276714236, 3: 0.0009571876087713191, 4: 0.0020013922728854858, 6: 0.0001740341106856944, 8: 0.0005221023320570832, 9: 0.0003480682213713888, 10: 0.0027845457709711106, 11: 0.001305255830142708, 12: 0.005656108597285068, 13: 0.01305255830142708, 14: 0.006874347372084928, 15: 0.004263835711799513, 16: 0.006352245040027845, 17: 0.000435085276714236, 18: 0.02018795683954055, 19: 0.9258614688478942}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.3117140233516693, 'eval_n_examples': 24, 'eval_acc': 0.9545936782429303, 'eval_n_focus_examples': 21, 'eval_focus_acc': 0.5686994962498289, 'eval_focus_else_acc_minus_one': 0.5686994962498289, 'eval_runtime': 4.3703, 'eval_samples_per_second': 5.492, 'eval_steps_per_second': 1.373, 'epoch': 41.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:51:58,450 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-4633\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:51:58,450 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-4633\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:51:58,452 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-4633/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:51:58,452 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-4633/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:52:01,461 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-4633/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:52:01,461 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-4633/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:52:15,182 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-3503] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:52:15,182 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-3503] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:53:41,679 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:53:41,679 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:53:41,679 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:53:41,679 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:53:41,679 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:53:41,679 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m2023-05-22 13:53:45,932 [data.ner] INFO Evaluation class prediction ratios: {0: 0.003393665158371041, 1: 0.005917159763313609, 4: 0.004437869822485207, 5: 0.0015663069961712496, 6: 0.0002610511660285416, 8: 0.0006961364427427776, 9: 0.000435085276714236, 10: 0.00217542638357118, 11: 0.0014792899408284023, 12: 0.005656108597285068, 13: 0.010180995475113122, 14: 0.00652627915071354, 15: 0.004524886877828055, 16: 0.006178210929342151, 17: 0.006004176818656457, 18: 0.016359206404455272, 19: 0.9242081447963801}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.33516278862953186, 'eval_n_examples': 24, 'eval_acc': 0.9498254103916713, 'eval_n_focus_examples': 22, 'eval_focus_acc': 0.5711809724957685, 'eval_focus_else_acc_minus_one': 0.5711809724957685, 'eval_runtime': 4.2538, 'eval_samples_per_second': 5.642, 'eval_steps_per_second': 1.411, 'epoch': 42.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:53:45,934 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-4746\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:53:45,934 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-4746\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:53:45,936 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-4746/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:53:45,936 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-4746/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:53:48,927 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-4746/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:53:48,927 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-4746/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:54:01,820 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-3729] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:54:01,820 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-3729] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:55:29,718 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:55:29,718 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:55:29,718 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:55:29,718 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:55:29,718 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:55:29,718 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m2023-05-22 13:55:34,343 [data.ner] INFO Evaluation class prediction ratios: {0: 0.0030455969369996517, 1: 0.005395057431256526, 2: 0.0013922728854855553, 3: 0.0009571876087713191, 4: 0.0023494604942568744, 6: 0.0001740341106856944, 8: 0.0005221023320570832, 9: 0.000435085276714236, 10: 0.0006961364427427776, 11: 0.0012182387747998607, 12: 0.005830142707970762, 13: 0.014792899408284023, 14: 0.005830142707970762, 15: 0.004524886877828055, 16: 0.006787330316742082, 17: 0.001740341106856944, 18: 0.014009745910198398, 19: 0.9302993386703794}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.381779283285141, 'eval_n_examples': 24, 'eval_acc': 0.9510344314654983, 'eval_n_focus_examples': 22, 'eval_focus_acc': 0.5291515845185049, 'eval_focus_else_acc_minus_one': 0.5291515845185049, 'eval_runtime': 4.6261, 'eval_samples_per_second': 5.188, 'eval_steps_per_second': 1.297, 'epoch': 43.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:55:34,345 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-4859\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:55:34,345 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-4859\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:55:34,347 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-4859/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:55:34,347 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-4859/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:55:37,350 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-4859/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:55:37,350 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-4859/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:55:51,032 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-3842] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:55:51,032 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-3842] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:57:20,406 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:57:20,406 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:57:20,406 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:57:20,406 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:57:20,406 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:57:20,406 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m2023-05-22 13:57:25,212 [data.ner] INFO Evaluation class prediction ratios: {0: 0.0024364775495997215, 1: 0.005656108597285068, 2: 0.0003480682213713888, 3: 0.0010442046641141664, 4: 0.0027845457709711106, 8: 0.0006091193873999304, 9: 0.0006091193873999304, 10: 0.0010442046641141664, 11: 0.0015663069961712496, 12: 0.005656108597285068, 13: 0.01427079707622694, 14: 0.005482074486599374, 15: 0.004698920988513749, 16: 0.00652627915071354, 17: 0.0009571876087713191, 18: 0.021232161503654718, 19: 0.9250783153498086}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2869884967803955, 'eval_n_examples': 24, 'eval_acc': 0.9552703939657513, 'eval_n_focus_examples': 22, 'eval_focus_acc': 0.5341256662189002, 'eval_focus_else_acc_minus_one': 0.5341256662189002, 'eval_runtime': 4.8073, 'eval_samples_per_second': 4.992, 'eval_steps_per_second': 1.248, 'epoch': 44.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:57:25,214 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-4972\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:57:25,214 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-4972\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:57:25,216 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-4972/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:57:25,216 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-4972/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:57:28,230 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-4972/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:57:28,230 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-4972/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:57:41,852 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-3955] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:57:41,852 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-3955] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m{'loss': 0.0029, 'learning_rate': 3.5250737463126844e-05, 'epoch': 44.25}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:59:12,388 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 13:59:12,388 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:59:12,389 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:59:12,389 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 13:59:12,389 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 13:59:12,389 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m2023-05-22 13:59:17,270 [data.ner] INFO Evaluation class prediction ratios: {0: 0.0029585798816568047, 1: 0.005656108597285068, 4: 0.00435085276714236, 6: 0.0006961364427427776, 8: 0.0005221023320570832, 9: 0.0003480682213713888, 10: 0.0015663069961712496, 11: 0.0023494604942568744, 12: 0.005656108597285068, 13: 0.01148625130525583, 14: 0.005743125652627915, 15: 0.00435085276714236, 16: 0.00713539853811347, 17: 0.0006091193873999304, 18: 0.014183780020884093, 19: 0.9323877479986077}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.3958381712436676, 'eval_n_examples': 24, 'eval_acc': 0.9485586740879454, 'eval_n_focus_examples': 21, 'eval_focus_acc': 0.5157890654516206, 'eval_focus_else_acc_minus_one': 0.5157890654516206, 'eval_runtime': 4.8826, 'eval_samples_per_second': 4.915, 'eval_steps_per_second': 1.229, 'epoch': 45.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:59:17,272 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-5085\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 13:59:17,272 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-5085\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:59:17,274 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-5085/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 13:59:17,274 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-5085/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:59:20,280 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-5085/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 13:59:20,280 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-5085/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:59:33,903 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-4068] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 13:59:33,903 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-4068] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 14:01:04,282 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 14:01:04,282 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 14:01:04,283 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 14:01:04,283 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 14:01:04,283 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 14:01:04,283 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m2023-05-22 14:01:09,766 [data.ner] INFO Evaluation class prediction ratios: {0: 0.0030455969369996517, 1: 0.005569091541942221, 3: 0.000435085276714236, 4: 0.0030455969369996517, 6: 0.000435085276714236, 8: 0.0005221023320570832, 9: 0.0003480682213713888, 10: 0.0016533240515140967, 11: 0.0012182387747998607, 12: 0.005656108597285068, 13: 0.011921336581970065, 14: 0.006352245040027845, 15: 0.004524886877828055, 16: 0.006178210929342151, 18: 0.0108771319178559, 19: 0.9382178907065785}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.4283958673477173, 'eval_n_examples': 24, 'eval_acc': 0.9470721329887833, 'eval_n_focus_examples': 20, 'eval_focus_acc': 0.5516342512092749, 'eval_focus_else_acc_minus_one': 0.5516342512092749, 'eval_runtime': 5.4846, 'eval_samples_per_second': 4.376, 'eval_steps_per_second': 1.094, 'epoch': 46.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 14:01:09,768 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-5198\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 14:01:09,768 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-5198\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 14:01:09,770 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-5198/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 14:01:09,770 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-5198/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 14:01:12,836 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-5198/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 14:01:12,836 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-5198/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 14:01:26,548 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-4181] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 14:01:26,548 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-4181] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 14:02:57,379 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-05-22 14:02:57,379 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 14:02:57,380 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 14:02:57,380 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-05-22 14:02:57,380 >>   Num examples = 24\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-05-22 14:02:57,380 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m2023-05-22 14:03:02,129 [data.ner] INFO Evaluation class prediction ratios: {0: 0.0030455969369996517, 1: 0.005656108597285068, 2: 8.70170553428472e-05, 3: 0.000435085276714236, 4: 0.0033066481030281933, 6: 0.0001740341106856944, 8: 0.0005221023320570832, 9: 0.0003480682213713888, 10: 0.0007831534980856248, 11: 0.0018273581621997912, 12: 0.005656108597285068, 13: 0.012008353637312914, 14: 0.005046989209885138, 15: 0.0028715628263139576, 16: 0.007048381482770623, 18: 0.014444831186912635, 19: 0.9367386007657501}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.37803950905799866, 'eval_n_examples': 24, 'eval_acc': 0.9500847588615295, 'eval_n_focus_examples': 20, 'eval_focus_acc': 0.563048127224845, 'eval_focus_else_acc_minus_one': 0.563048127224845, 'eval_runtime': 4.7502, 'eval_samples_per_second': 5.052, 'eval_steps_per_second': 1.263, 'epoch': 47.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 14:03:02,131 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-5311\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 14:03:02,131 >> Saving model checkpoint to /tmp/transformers/checkpoints/checkpoint-5311\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 14:03:02,133 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-5311/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 14:03:02,133 >> Configuration saved in /tmp/transformers/checkpoints/checkpoint-5311/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 14:03:05,171 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-5311/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 14:03:05,171 >> Model weights saved in /tmp/transformers/checkpoints/checkpoint-5311/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 14:03:18,976 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-4294] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2787] 2023-05-22 14:03:18,976 >> Deleting older checkpoint [/tmp/transformers/checkpoints/checkpoint-4294] due to args.save_total_limit\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1901] 2023-05-22 14:03:19,062 >> \u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2025] 2023-05-22 14:03:19,062 >> Loading best model from /tmp/transformers/checkpoints/checkpoint-3616 (score: 0.6633403720412484).\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1901] 2023-05-22 14:03:19,062 >> \u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2025] 2023-05-22 14:03:19,062 >> Loading best model from /tmp/transformers/checkpoints/checkpoint-3616 (score: 0.6633403720412484).\u001b[0m\n",
      "\u001b[34m{'train_runtime': 5199.5111, 'train_samples_per_second': 6.52, 'train_steps_per_second': 3.26, 'train_loss': 0.029525295946472806, 'epoch': 47.0}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 14:03:22,591 >> Saving model checkpoint to /tmp/transformers/checkpoints\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 14:03:22,591 >> Saving model checkpoint to /tmp/transformers/checkpoints\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 14:03:22,596 >> Configuration saved in /tmp/transformers/checkpoints/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 14:03:22,596 >> Configuration saved in /tmp/transformers/checkpoints/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 14:03:28,149 >> Model weights saved in /tmp/transformers/checkpoints/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 14:03:28,149 >> Model weights saved in /tmp/transformers/checkpoints/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m***** train metrics *****\u001b[0m\n",
      "\u001b[34mepoch                    =       47.0\n",
      "  train_loss               =     0.0295\n",
      "  train_runtime            = 1:26:39.51\n",
      "  train_samples            =        226\n",
      "  train_samples_per_second =       6.52\n",
      "  train_steps_per_second   =       3.26\u001b[0m\n",
      "\u001b[34m2023-05-22 14:03:28,154 [main] INFO Saving model to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 14:03:28,154 >> Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-05-22 14:03:28,154 >> Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 14:03:28,163 >> Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-05-22 14:03:28,163 >> Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 14:03:37,043 >> Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-05-22 14:03:37,043 >> Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|image_processing_utils.py:200] 2023-05-22 14:03:37,045 >> Image processor saved in /opt/ml/model/preprocessor_config.json\u001b[0m\n",
      "\u001b[34m[INFO|image_processing_utils.py:200] 2023-05-22 14:03:37,045 >> Image processor saved in /opt/ml/model/preprocessor_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2160] 2023-05-22 14:03:37,046 >> tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2160] 2023-05-22 14:03:37,046 >> tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2167] 2023-05-22 14:03:37,046 >> Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2167] 2023-05-22 14:03:37,046 >> Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m2023-05-22 14:03:37,575 [main] INFO Copying code to /opt/ml/model/code for inference\u001b[0m\n",
      "\u001b[34m2023-05-22 14:03:37,575 [main] INFO Copying ./ddp_launcher.py to /opt/ml/model/code/ddp_launcher.py\u001b[0m\n",
      "\u001b[34m2023-05-22 14:03:37,576 [main] INFO Copying ./requirements.txt to /opt/ml/model/code/requirements.txt\u001b[0m\n",
      "\u001b[34m2023-05-22 14:03:37,577 [main] INFO Copying ./inference.py to /opt/ml/model/code/inference.py\u001b[0m\n",
      "\u001b[34m2023-05-22 14:03:37,578 [main] INFO Copying ./inference_seq2seq.py to /opt/ml/model/code/inference_seq2seq.py\u001b[0m\n",
      "\u001b[34m2023-05-22 14:03:37,578 [main] INFO Copying ./train.py to /opt/ml/model/code/train.py\u001b[0m\n",
      "\u001b[34m2023-05-22 14:03:37,579 [main] INFO Copying ./__init__.py to /opt/ml/model/code/__init__.py\u001b[0m\n",
      "\u001b[34m2023-05-22 14:03:37,580 [main] INFO Copying ./smtc_launcher.py to /opt/ml/model/code/smtc_launcher.py\u001b[0m\n",
      "\u001b[34m2023-05-22 14:03:37,581 [main] INFO Copying ./code/__init__.py to /opt/ml/model/code/code/__init__.py\u001b[0m\n",
      "\u001b[34m2023-05-22 14:03:37,581 [main] INFO Copying ./code/config.py to /opt/ml/model/code/code/config.py\u001b[0m\n",
      "\u001b[34m2023-05-22 14:03:37,582 [main] INFO Copying ./code/inference.py to /opt/ml/model/code/code/inference.py\u001b[0m\n",
      "\u001b[34m2023-05-22 14:03:37,583 [main] INFO Copying ./code/inference_seq2seq.py to /opt/ml/model/code/code/inference_seq2seq.py\u001b[0m\n",
      "\u001b[34m2023-05-22 14:03:37,584 [main] INFO Copying ./code/logging_utils.py to /opt/ml/model/code/code/logging_utils.py\u001b[0m\n",
      "\u001b[34m2023-05-22 14:03:37,585 [main] INFO Copying ./code/smddpfix.py to /opt/ml/model/code/code/smddpfix.py\u001b[0m\n",
      "\u001b[34m2023-05-22 14:03:37,585 [main] INFO Copying ./code/train.py to /opt/ml/model/code/code/train.py\u001b[0m\n",
      "\u001b[34m2023-05-22 14:03:37,587 [main] INFO Copying ./code/data/__init__.py to /opt/ml/model/code/code/data/__init__.py\u001b[0m\n",
      "\u001b[34m2023-05-22 14:03:37,588 [main] INFO Copying ./code/data/base.py to /opt/ml/model/code/code/data/base.py\u001b[0m\n",
      "\u001b[34m2023-05-22 14:03:37,589 [main] INFO Copying ./code/data/geometry.py to /opt/ml/model/code/code/data/geometry.py\u001b[0m\n",
      "\u001b[34m2023-05-22 14:03:37,589 [main] INFO Copying ./code/data/mlm.py to /opt/ml/model/code/code/data/mlm.py\u001b[0m\n",
      "\u001b[34m2023-05-22 14:03:37,590 [main] INFO Copying ./code/data/ner.py to /opt/ml/model/code/code/data/ner.py\u001b[0m\n",
      "\u001b[34m2023-05-22 14:03:37,591 [main] INFO Copying ./code/data/smgt.py to /opt/ml/model/code/code/data/smgt.py\u001b[0m\n",
      "\u001b[34m2023-05-22 14:03:37,591 [main] INFO Copying ./code/data/splitting.py to /opt/ml/model/code/code/data/splitting.py\u001b[0m\n",
      "\u001b[34m2023-05-22 14:03:37,592 [main] INFO Copying ./code/data/seq2seq/__init__.py to /opt/ml/model/code/code/data/seq2seq/__init__.py\u001b[0m\n",
      "\u001b[34m2023-05-22 14:03:37,592 [main] INFO Copying ./code/data/seq2seq/date_normalization.py to /opt/ml/model/code/code/data/seq2seq/date_normalization.py\u001b[0m\n",
      "\u001b[34m2023-05-22 14:03:37,593 [main] INFO Copying ./code/data/seq2seq/metrics.py to /opt/ml/model/code/code/data/seq2seq/metrics.py\u001b[0m\n",
      "\u001b[34m2023-05-22 14:03:37,593 [main] INFO Copying ./code/data/seq2seq/task_builder.py to /opt/ml/model/code/code/data/seq2seq/task_builder.py\u001b[0m\n",
      "\u001b[34m2023-05-22 14:03:37,594 [main] INFO Copying ./code/models/__init__.py to /opt/ml/model/code/code/models/__init__.py\u001b[0m\n",
      "\u001b[34m2023-05-22 14:03:37,595 [main] INFO Copying ./code/models/layoutlmv2.py to /opt/ml/model/code/code/models/layoutlmv2.py\u001b[0m\n",
      "\u001b[34mRunning smdistributed.dataparallel v1.4.3\u001b[0m\n",
      "\u001b[34mError in atexit._run_exitfuncs:\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/dataparallel/torch/torch_smddp/__init__.py\", line 51, in at_exit_smddp\u001b[0m\n",
      "\u001b[34mhm.shutdown()\u001b[0m\n",
      "\u001b[34mRuntimeError: Was this script started with smddprun? For more info on using smddprun, run smddprun -h\u001b[0m\n",
      "\u001b[34m2023-05-22 14:03:39,287 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-05-22 14:03:39,287 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-05-22 14:03:39,287 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-05-22 14:03:45 Uploading - Uploading generated training model\n",
      "2023-05-22 14:06:51 Completed - Training job completed\n",
      "Training seconds: 5809\n",
      "Billable seconds: 5809\n"
     ]
    }
   ],
   "source": [
    "inputs = {\n",
    "    \"images\": thumbs_s3uri,    # (Can omit this channel with LayoutLMv1 for performance)\n",
    "    \"train\": train_manifest_s3uri,\n",
    "    \"textract\": textract_s3uri + \"/\",\n",
    "    \"validation\": test_manifest_s3uri,\n",
    "}\n",
    "if pretrained_s3_uri:\n",
    "    print(f\"Using custom pre-trained model {pretrained_s3_uri}\")\n",
    "    inputs[\"model_name_or_path\"] = pretrained_s3_uri\n",
    "\n",
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## (Optional) Hyperparameter tuning\n",
    "\n",
    "Particularly when applying novel techniques or working in new domains, we'll often need to find good values for a range of different *hyperparameters* of our proposed algorithms.\n",
    "\n",
    "Rather than spending time manually adjusting these parameters, we can use [SageMaker Automatic Model Tuning](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html) which uses an intelligent [Bayesian optimization approach](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html) to efficiently and automatically search for high-performing combinations over several training jobs.\n",
    "\n",
    "You can optionally run the cell below to kick off an HPO job for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = sagemaker.tuner.HyperparameterTuner(\n",
    "    estimator,\n",
    "    \"validation:target\",\n",
    "    base_tuning_job_name=\"xlm-cfpb-hpo\",\n",
    "    hyperparameter_ranges={\n",
    "        \"learning_rate\": sagemaker.parameter.ContinuousParameter(\n",
    "            1e-8,\n",
    "            1e-3,\n",
    "            scaling_type=\"Logarithmic\",\n",
    "        ),\n",
    "        \"per_device_train_batch_size\": sagemaker.parameter.CategoricalParameter([2, 4, 6, 8]),\n",
    "        \"label_smoothing_factor\": sagemaker.parameter.CategoricalParameter([0.0, 1e-12, 1e-9, 1e-6]),\n",
    "    },\n",
    "    metric_definitions=metric_definitions,\n",
    "    strategy=\"Bayesian\",\n",
    "    objective_type=\"Maximize\",\n",
    "    max_jobs=21,\n",
    "    max_parallel_jobs=2,\n",
    "    # early_stopping_type=\"Auto\",  # Off by default - could consider turning it on\n",
    "#     warm_start_config=sagemaker.tuner.WarmStartConfig(\n",
    "#         warm_start_type=sagemaker.tuner.WarmStartTypes.IDENTICAL_DATA_AND_ALGORITHM,\n",
    "#         parents={ \"xlm-cfpb-hpo-210723-1625\" },\n",
    "#     ),\n",
    ")\n",
    "\n",
    "tuner.fit(\n",
    "    inputs={\n",
    "        \"images\": thumbs_s3uri,    # (Can omit this channel with LayoutLMv1 for performance)\n",
    "        \"train\": train_manifest_s3uri,\n",
    "        \"textract\": textract_s3uri + \"/\",\n",
    "        \"validation\": test_manifest_s3uri,\n",
    "    },\n",
    "    wait=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This job will run asynchronously so won't block the notebook, but you can check on the status from the [Hyperparameter tuning jobs list](https://console.aws.amazon.com/sagemaker/home?#/hyper-tuning-jobs) of the SageMaker Console."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Deploy the model\n",
    "\n",
    "Once our model is trained (or maybe even automatically hyperparameter-tuned over several training jobs), we can prepare to use it for inference.\n",
    "\n",
    "Note that if, for some reason, you need to recover the state of a previous training or tuning job after a notebook restart or similar, you can `attach()` to training or tuning jobs by name - as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed, you can attach to a previous training job by name like this:\n",
    "#estimator = HuggingFaceEstimator.attach(\"xlm-cfpb-hf-2023-05-21-18-18-40-295\")\n",
    "# tuner = sagemaker.tuner.HyperparameterTuner.attach(\"llmv2-cfpb-hpo-210603-0542\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker supports a [range of different deployment types](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html) for inference: You may already be familiar with the [real-time](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html) and [batch transform](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html) options from the [main SageMaker Example Notebook repository](https://github.com/aws/amazon-sagemaker-examples).\n",
    "\n",
    "For document processing use-cases like this one though, [SageMaker asynchronous inference](https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference.html) may be a better fit:\n",
    "\n",
    "1. Unlike real-time endpoints (at the time of writing), asynchronous endpoints can [auto-scale down to zero instances](https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference-autoscale.html). This can offer substantial cost savings if your business process is low-volume and often idle: With the trade-off that overall process latency may increase, especially for cold-start requests.\n",
    "1. Asynchronous inference can support longer timeouts and larger request/response payload sizes than real-time: Which can be useful in cases where an individual document may be long and take a significant time to process with a model.\n",
    "    - While you can work around the payload size restriction in real-time endpoints by accepting and returning JSON pointers to S3 objects, instead of passing large payloads inline, the inference time-out could still become an issue for particularly heavy requests\n",
    "\n",
    "To deploy our model to an asynchronous endpoint ready to integrate to the OCR pipeline stack:\n",
    "\n",
    "- As detailed in the [SDK docs](https://sagemaker.readthedocs.io/en/stable/overview.html#sagemaker-asynchronous-inference), the optional `async_inference_config` parameter tells SageMaker that the endpoint will be asynchronous rather than real-time.\n",
    "- For permissions integration, our async endpoint will need to store its outputs in the proper S3 location the pipeline is expecting (`output_path`). We can look that up from here in the notebook via the same SSM-based `config` we've seen before.\n",
    "- To resume the pipeline when the model processes a document, our endpoint will need to notify the pipeline's SNS topic. Again, this is given on `config`.\n",
    "- While the *SageMaker* limits on request/response size and response timeouts are higher for asynchronous endpoints than real-time, we need to also make sure the serving stack *within the container* is configured to support very large payloads. Setting the `MMS_*` environment variables below prevents errors related to this. For more information see the [AWSLabs Multi-Model Server configuration doc](https://github.com/awslabs/multi-model-server/blob/master/docs/configuration.md) and corresponding page [for TorchServe](https://github.com/pytorch/serve/blob/master/docs/configuration.md#other-properties)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure async endpoint settings for the pipeline stack:\n",
    "async_inference_config = sagemaker.async_inference.AsyncInferenceConfig(\n",
    "    output_path=f\"s3://{config.model_results_bucket}\",\n",
    "    max_concurrent_invocations_per_instance=2,  # (Can tune this for performance)\n",
    "    notification_config={\n",
    "        \"SuccessTopic\": config.model_callback_topic_arn,\n",
    "        \"ErrorTopic\": config.model_callback_topic_arn,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Extra environment variables to enable large payloads in async\n",
    "async_extra_env_vars = {\n",
    "    \"MMS_DEFAULT_RESPONSE_TIMEOUT\": str(60*3),  # 3min instead of default (maybe 60sec?)\n",
    "    \"MMS_MAX_REQUEST_SIZE\": str(100*1024*1024),  # 100MiB instead of default ~6.2MiB\n",
    "    \"MMS_MAX_RESPONSE_SIZE\": str(100*1024*1024),  # 100MiB instead of default ~6.2MiB\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Easy one-click deployment\n",
    "\n",
    "For straightforward deployment, you can just call `estimator.deploy()` (or equivalently, `tuner.deploy()`) - specifying the extra `async_inference_config` and environment variables for our target async deployment:\n",
    "\n",
    "> ⚠️ **Warning:** If you change inference code (e.g. [src/code/inference.py](src/code/inference.py)) and re-deploy by this one-click method, your change will likely not be picked up. See the deep-dive section below instead, for making updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!"
     ]
    }
   ],
   "source": [
    "training_job_name = estimator.latest_training_job.describe()[\"TrainingJobName\"]\n",
    "# Or:\n",
    "# training_job_name = tuner.best_training_job()\n",
    "\n",
    "predictor = estimator.deploy(\n",
    "    # Avoid us accidentally deploying the same model twice by setting name per training job:\n",
    "    endpoint_name=training_job_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g4dn.xlarge\",\n",
    "    image_uri=inf_image_uri,\n",
    "    serializer=sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer=sagemaker.deserializers.JSONDeserializer(),\n",
    "    env={\n",
    "        \"PYTHONUNBUFFERED\": \"1\",  # TODO: Disable once debugging is done\n",
    "        **async_extra_env_vars,\n",
    "    },\n",
    "    async_inference_config=async_inference_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xlm-cfpb-hf-2023-05-22-12-28-38-729\n"
     ]
    }
   ],
   "source": [
    "training_job_name = estimator.latest_training_job.describe()[\"TrainingJobName\"]\n",
    "print (training_job_name)\n",
    "#xlm-cfpb-hf-2023-05-22-12-28-38-729"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Digging deeper into the model\n",
    "\n",
    "Alternatively, you may instead want to explore the artifacts saved by the training job, or edit the `code` script bundle before deploying the endpoint - especially for debugging any problems with inference. Let's see how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Look up job name and artifact location from previous training job as before:\n",
    "training_job_desc = estimator.latest_training_job.describe()\n",
    "model_s3uri = training_job_desc[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
    "model_name = training_job_desc[\"TrainingJobName\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "s3://sagemaker-us-east-1-015943506230/textract-transformers/trainjobs/xlm-cfpb-hf-2023-05-22-12-28-38-729/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "training_job_desc = estimator.latest_training_job.describe()\n",
    "#print (training_job_desc)\n",
    "# xlm-cfpb-hf-2023-05-22-12-28-38-729\n",
    "print()\n",
    "print (model_s3uri)\n",
    "#s3://sagemaker-us-east-1-015943506230/textract-transformers/trainjobs/xlm-cfpb-hf-2023-05-22-12-28-38-729/output/model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# *Optionally* download and extract the contents of the model.tar.gz locally:\n",
    "# (Deleting old data/model folder if it exists)\n",
    "!rm -rf ./data/model\n",
    "!aws s3 cp $model_s3uri ./data/model/model.tar.gz\n",
    "!cd data/model && tar -xzvf model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-015943506230/textract-transformers/trainjobs/xlm-cfpb-hf-2023-05-22-12-28-38-729/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "print (model_s3uri)\n",
    "# s3://sagemaker-us-east-1-015943506230/textract-transformers/trainjobs/xlm-cfpb-hf-2023-05-22-12-28-38-729/output/model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "try:\n",
    "    # Make sure we don't accidentally re-use same model:\n",
    "    smclient.delete_model(ModelName=model_name)\n",
    "    print(f\"Deleted existing model {model_name}\")\n",
    "except smclient.exceptions.ClientError as e:\n",
    "    if not (\n",
    "        e.response[\"Error\"][\"Code\"] in (404, \"404\")\n",
    "        or e.response[\"Error\"].get(\"Message\", \"\").startswith(\"Could not find model\")\n",
    "    ):\n",
    "        raise e\n",
    "\n",
    "model = HuggingFaceModel(\n",
    "    name=model_name,\n",
    "    model_data=model_s3uri,\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    source_dir=\"src/\",\n",
    "    entry_point=\"inference.py\",\n",
    "    py_version=py_version,\n",
    "    pytorch_version=pt_version,\n",
    "    transformers_version=hf_version,\n",
    "    image_uri=inf_image_uri,\n",
    "    env={\n",
    "        \"PYTHONUNBUFFERED\": \"1\",  # TODO: Disable once debugging is done\n",
    "        **async_extra_env_vars,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting previous endpoint...\n",
      "Deploying model...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[149], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeploying model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m predictor \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mdeploy(\n\u001b[1;32m     14\u001b[0m     endpoint_name\u001b[38;5;241m=\u001b[39mtraining_job_desc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrainingJobName\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     15\u001b[0m     initial_instance_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     16\u001b[0m     instance_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mml.g4dn.xlarge\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m     serializer\u001b[38;5;241m=\u001b[39msagemaker\u001b[38;5;241m.\u001b[39mserializers\u001b[38;5;241m.\u001b[39mJSONSerializer(),\n\u001b[1;32m     18\u001b[0m     deserializer\u001b[38;5;241m=\u001b[39msagemaker\u001b[38;5;241m.\u001b[39mdeserializers\u001b[38;5;241m.\u001b[39mJSONDeserializer(),\n\u001b[1;32m     19\u001b[0m     async_inference_config\u001b[38;5;241m=\u001b[39masync_inference_config,\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mDone!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# INTERM TrainingJobName TO TEST PIPELINE:   xlm-cfpb-hf-2023-05-22-12-28-38-729\n",
    "\n",
    "try:\n",
    "    # Delete previous endpoint, if already in use:\n",
    "    predictor.delete_endpoint(delete_endpoint_config=True)\n",
    "    print(\"Deleting previous endpoint...\")\n",
    "    time.sleep(8)\n",
    "except (NameError, smclient.exceptions.ResourceNotFound):\n",
    "    pass  # No existing endpoint to delete\n",
    "except smclient.exceptions.ClientError as e:\n",
    "    if \"Could not find\" not in e.response[\"Error\"].get(\"Message\", \"\"):\n",
    "        raise e\n",
    "\n",
    "print(\"Deploying model...\")\n",
    "predictor = model.deploy(\n",
    "    endpoint_name=training_job_desc[\"TrainingJobName\"],\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g4dn.xlarge\",\n",
    "    serializer=sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer=sagemaker.deserializers.JSONDeserializer(),\n",
    "    async_inference_config=async_inference_config,\n",
    ")\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Extract clean input images on-demand\n",
    "\n",
    "> ▶️ If you're using LayoutLMv1, you can skip this section\n",
    "\n",
    "Some models (like LayoutLMv2/XLM, but **not** LayoutLMv1) consume **page images** in addition to text and layout data.\n",
    "\n",
    "The same code we used in notebook 1 to extract clean page images from raw source documents, can be deployed as an (asynchronous) *inference endpoint* for *on-demand* page thumbnail image generation whenever a new document comes in. If you deployed the pipeline CDK stack with the default options, this endpoint should be **already deployed for you** and can be located as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config.thumbnails_callback_topic_arn == \"undefined\":\n",
    "    logger.warning(\n",
    "        \"This pipeline CDK stack was deployed with thumbnailing disabled (by setting parameter \"\n",
    "        \"use_thumbnails=False). Even if you manually deploy a thumbnailing endpoint from the \"\n",
    "        \"notebook, it will not be used in online processing.\"\n",
    "    )\n",
    "\n",
    "preproc_endpoint_name = ssm.get_parameter(\n",
    "    Name=config.thumbnail_endpoint_name_param,\n",
    ")[\"Parameter\"][\"Value\"]\n",
    "print(f\"Pre-created thumbnailer endpoint name:\\n  {preproc_endpoint_name}\")\n",
    "\n",
    "if preproc_endpoint_name == \"undefined\":\n",
    "    raise ValueError(\n",
    "        \"The thumbnailing endpoint was not automatically created by this pipeline's CDK stack \"\n",
    "        \"deployment. See the 'Optional Extras.ipynb' notebook for instructions to manually deploy \"\n",
    "        \"the thumbnailer before continuing.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ℹ️ If the thumbnailer endpoint is **missing or mis-configured** in your environment:\n",
    "> \n",
    "> - **Check** whether your pipeline is deployed with online thumbnailing support enabled\n",
    ">     - This is not mandatory for experimenting with alternative models in notebook, but if you later connect a model that consumes thumbnails to a pipeline that doesn't generate them, model accuracy will be degraded.\n",
    ">     - To confirm, find your *pipeline* state machine in [AWS Step Functions](https://console.aws.amazon.com/states/home?#/statemachines) (the one containing NLP and post-processing steps), and check it runs a Thumbnail Generation step in parallel to OCR.\n",
    ">     - To update, configure the `USE_THUMBNAILS` environment variable referenced by [/cdk_app.py](../cdk_app.py) and re-deploy your CDK app. Check whether your version of the CDK code supports auto-deploying the endpoint.\n",
    "> - **If necessary, manually set up** a thumbnailing endpoint:\n",
    ">     - See the thumbnailing deployment instructions in [Optional Extras.ipynb](Optional%20Extras.ipynb)\n",
    ">     - You can re-run the above cell if you connected the custom thumbnailer to your pipeline, or just set `preproc_endpoint_name` here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    desc = smclient.describe_endpoint(EndpointName=preproc_endpoint_name)\n",
    "except smclient.exceptions.ClientError as e:\n",
    "    if e.response.get(\"Error\", {}).get(\"Message\", \"\").startswith(\"Could not find\"):\n",
    "        desc = None  # Endpoint does not exist\n",
    "    else:\n",
    "        raise e  # Some other unknown issue\n",
    "\n",
    "if desc is None:\n",
    "    raise ValueError(\n",
    "        \"The configured thumbnailing endpoint does not exist in SageMaker. See the 'Optional \"\n",
    "        \"Extras.ipynb' notebook for instructions to manually deploy the thumbnailer before \"\n",
    "        \"continuing. Missing endpoint: %s\" % preproc_endpoint_name\n",
    "    )\n",
    "\n",
    "preproc_predictor = sagemaker.predictor_async.AsyncPredictor(\n",
    "    sagemaker.Predictor(\n",
    "        preproc_endpoint_name,\n",
    "        serializer=util.deployment.FileSerializer.from_filename(\"any.pdf\"),\n",
    "        deserializer=util.deployment.CompressedNumpyDeserializer(),\n",
    "    ),\n",
    "    name=preproc_endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This endpoint accepts images or documents and outputs resized page thumbnail images.\n",
    "\n",
    "For multi-page documents the main output format is `application/x-npz`, which produces a [compressed numpy archive](https://numpy.org/doc/stable/reference/generated/numpy.savez_compressed.html#numpy.savez_compressed) in which `images` is an **array of images** each represented by **PNG bytes**. These formats require customizing the client (predictor) *serializer* and *deserializer* from the default for PyTorch. Since `Predictor` de/serializers set the `Content-Type` and `Accept` headers, we'll also need to re-configure the serializer whenever switching between input document types (for example PDF vs PNG).\n",
    "\n",
    "To support potentially large documents, the preprocessor is deployed to an **asynchronous** endpoint which enables larger request and response payload sizes.\n",
    "\n",
    "So how would it look to test the endpoint from Python? Let's see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Choose an input (document or image):\n",
    "input_file = \"data/raw/121 Financial Credit Union/Visa Credit Card Agreement.pdf\"\n",
    "#input_file = \"data/imgs-clean/121 Financial Credit Union/Visa Credit Card Agreement-0001-1.png\"\n",
    "\n",
    "# Ensure de/serializers are correctly set up:\n",
    "preproc_predictor.serializer = util.deployment.FileSerializer.from_filename(input_file)\n",
    "preproc_predictor.deserializer = util.deployment.CompressedNumpyDeserializer()\n",
    "# Duplication because of https://github.com/aws/sagemaker-python-sdk/issues/3100\n",
    "preproc_predictor.predictor.serializer = preproc_predictor.serializer\n",
    "preproc_predictor.predictor.deserializer = preproc_predictor.deserializer\n",
    "\n",
    "# Run prediction:\n",
    "print(\"Calling endpoint...\")\n",
    "resp = preproc_predictor.predict(input_file)\n",
    "print(f\"Got response of type {type(resp)}\")\n",
    "\n",
    "# Render result:\n",
    "util.viz.draw_thumbnails_response(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Model\n",
    "\n",
    "Once the deployment is complete (and, if our model takes image inputs, the page thumbnail generator endpoint is ready), we're ready to try it out with some requests!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As with estimators, you can also attach the notebook to previously deployed endpoints like this:\n",
    "\n",
    "# preproc_endpoint_name=\"ocr-thumbnail-1-2022-05-23-15-52-35-703\"\n",
    "# preproc_predictor = sagemaker.predictor_async.AsyncPredictor(\n",
    "#     sagemaker.Predictor(\n",
    "#         preproc_endpoint_name,\n",
    "#         serializer=util.deployment.FileSerializer.from_filename(\"any.pdf\"),\n",
    "#         deserializer=util.deployment.CompressedNumpyDeserializer(),\n",
    "#     ),\n",
    "#     name=preproc_endpoint_name,\n",
    "# )\n",
    "\n",
    "#endpoint_name=\"xlm-cfpb-hf-2022-05-23-14-10-19-602\"\n",
    "# predictor = sagemaker.predictor_async.AsyncPredictor(\n",
    "#     sagemaker.Predictor(\n",
    "#         endpoint_name,\n",
    "#         serializer=sagemaker.serializers.JSONSerializer(),\n",
    "#         deserializer=sagemaker.deserializers.JSONDeserializer(),\n",
    "#     ),\n",
    "#     name=endpoint_name,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making requests and rendering results\n",
    "\n",
    "At a high level, the layout+language model accepts Textract-like JSON (e.g. as returned by [AnalyzeDocument](https://docs.aws.amazon.com/textract/latest/dg/API_AnalyzeDocument.html#API_AnalyzeDocument_ResponseSyntax) or [DetectDocumentText](https://docs.aws.amazon.com/textract/latest/dg/API_DetectDocumentText.html#API_DetectDocumentText_ResponseSyntax) APIs) and classifies each `WORD` [block](https://docs.aws.amazon.com/textract/latest/dg/API_Block.html) according to the entity classes we defined earlier: Returning the same JSON with additional fields added to indicate the predictions.\n",
    "\n",
    "In addition (per the logic in [src/code/inference.py](src/code/inference.py)):\n",
    "\n",
    "- To incorporate image features (for models that support them), requests can also include an `S3Thumbnails: { Bucket, Key }` object pointing to a thumbnailer endpoint response on S3.\n",
    "- Instead of passing the (typically large and already-S3-resident) Amazon Textract JSON inline, an `S3Input: { Bucket, Key }` reference can be passed instead (and this is actually how the standard pipeline integration works).\n",
    "- Output could also be redirected by passing an `S3Output: { Bucket, Key }` field in the request, but this is ignored and not needed on async endpoint deployments.\n",
    "- `TargetPageNum` and `TargetPageOnly` fields can be specified to limit processing to a single page of the input document.\n",
    "\n",
    "We can use utility functions to render these predictions as we did the manual annotations previously:\n",
    "\n",
    "> ⏰ **Inference may take time in some cases:**\n",
    ">\n",
    "> - Although enabling thumbnails can increase demo inference time below by several seconds, the end-to-end pipeline generates these images in parallel with running Amazon Textract - so there's usually no significant impact in practice.\n",
    "> - If you enabled **auto-scale-to-zero** on your your thumbnailer and/or model endpoint, you may see a cold-start of several minutes.\n",
    "\n",
    "> ⚠️ **Check:** Because of the way the SageMaker Python SDK's [AsyncPredictor](https://sagemaker.readthedocs.io/en/stable/api/inference/predictor_async.html) emulates a synchronous `predict()` interface for async endpoints, you may find the notebook waits indefinitely instead of raising an error when something goes wrong. If an inference takes more than ~30s to complete, check the endpoint logs from your [SageMaker Console Endpoints page](https://console.aws.amazon.com/sagemaker/home?#/endpoints) to see if your request resulted in an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "608b961e899847d599b616bbf7588c8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Example:', max=9), Output()), _dom_classes=('widget-inte…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(ix)>"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "import trp\n",
    "\n",
    "# Enabling thumbnails can significantly increase inference time here, but can improve results for\n",
    "# models that consume image features (like LayoutLMv2, XLM):\n",
    "include_thumbnails = False\n",
    "\n",
    "def predict_from_manifest_item(\n",
    "    item,\n",
    "    predictor,\n",
    "    imgs_s3key_prefix=imgs_s3uri[len(\"s3://\"):].partition(\"/\")[2],\n",
    "    raw_s3uri_prefix=raw_s3uri,\n",
    "    textract_s3key_prefix=textract_s3uri[len(\"s3://\"):].partition(\"/\")[2],\n",
    "    imgs_local_prefix=\"data/imgs-clean\",\n",
    "    textract_local_prefix=\"data/textracted\",\n",
    "    draw=True,\n",
    "):\n",
    "    paths = util.viz.local_paths_from_manifest_item(\n",
    "        item,\n",
    "        imgs_s3key_prefix,\n",
    "        textract_s3key_prefix=textract_s3key_prefix,\n",
    "        imgs_local_prefix=imgs_local_prefix,\n",
    "        textract_local_prefix=textract_local_prefix,\n",
    "    )\n",
    "\n",
    "    if include_thumbnails:\n",
    "        doc_textract_s3key = item[\"textract-ref\"][len(\"s3://\"):].partition(\"/\")[2]\n",
    "        doc_raw_s3uri = raw_s3uri_prefix + doc_textract_s3key[len(textract_s3key_prefix):].rpartition(\"/\")[0]\n",
    "        print(f\"Fetching thumbnails for {doc_raw_s3uri}\")\n",
    "        thumbs_async = preproc_predictor.predict_async(input_path=doc_raw_s3uri)\n",
    "        thumbs_bucket, _, thumbs_key = thumbs_async.output_path[len(\"s3://\"):].partition(\"/\")\n",
    "        # Wait for the request to complete:\n",
    "        thumbs_async.get_result(sagemaker.async_inference.WaiterConfig())\n",
    "        req_extras = {\"S3Thumbnails\": {\"Bucket\": thumbs_bucket, \"Key\": thumbs_key}}\n",
    "        print(\"Got thumbnails result\")\n",
    "    else:\n",
    "        req_extras = {}\n",
    "    \n",
    "    Texttrack_Json_File = '3://sagemaker-us-east-1-015943506230/textract-transformers/data/textracted/Comenity Capital Bank/Overstock Store Credit Card Agreement.pdf/consolidated.json'\n",
    "        \n",
    "\n",
    "    result_json = predictor.predict({\n",
    "        #\"S3Input\": {\"S3Uri\": item[\"textract-ref\"]},\n",
    "        \"S3Input\": {\"S3Uri\": Texttrack_Json_File },\n",
    "        \"TargetPageNum\": item[\"page-num\"],\n",
    "        \"TargetPageOnly\": True,\n",
    "        **req_extras,\n",
    "    })\n",
    "\n",
    "    if \"Warnings\" in result_json:\n",
    "        for warning in result_json[\"Warnings\"]:\n",
    "            logger.warning(warning)\n",
    "            \n",
    "            \n",
    "            \n",
    "    result_trp = trp.Document(result_json)\n",
    "\n",
    "    if draw:\n",
    "        util.viz.draw_smgt_annotated_page(\n",
    "            paths[\"image\"],\n",
    "            entity_classes,\n",
    "            annotations=[],\n",
    "            textract_result=result_trp,\n",
    "            # Note that page_num should be item[\"page-num\"] if we requested the full set of pages\n",
    "            # from the model above:\n",
    "            page_num=1,\n",
    "        )\n",
    "    return result_trp\n",
    "\n",
    "\n",
    "widgets.interact(\n",
    "    lambda ix: predict_from_manifest_item(test_examples[ix], predictor),\n",
    "    ix=widgets.IntSlider(\n",
    "        min=0,\n",
    "        max=len(test_examples) - 1,\n",
    "        step=1,\n",
    "        value=0,\n",
    "        description=\"Example:\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From token classification to entity detection\n",
    "\n",
    "You may have noticed a slight mismatch: We're talking about extracting 'fields' or 'entities' from the document, but our model just classifies individual words. Going from words to entities assumes we're able to understand which words go \"together\" and what order they should be read in.\n",
    "\n",
    "Fortunately, Textract helps us out with this too as the word blocks are already collected into `LINE`s.\n",
    "\n",
    "For many straightforward applications, we can simply loop through the lines on a page and define an \"entity detection\" as a contiguous group of the same class - as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-22 15:18:26,272 root [WARNING] SageMaker model's preprocessor (<class 'transformers.models.layoutxlm.processing_layoutxlm.LayoutXLMProcessor'>) expects page images (as .S3Thumbnails.{Bucket, Key} numpy array pointer in the request) but none were given. Generating default blank images - accuracy may be degraded.\n"
     ]
    }
   ],
   "source": [
    "res = predict_from_manifest_item(\n",
    "    test_examples[3],\n",
    "    predictor,\n",
    "    draw=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source-ref': 's3://sagemaker-us-east-1-015943506230/textract-transformers/data/imgs-clean/Comenity Capital Bank/Overstock Store Credit Card Agreement-0001-19.png', 'textract-ref': 's3://sagemaker-us-east-1-015943506230/textract-transformers/data/textracted/Comenity Capital Bank/Overstock Store Credit Card Agreement.pdf/consolidated.json', 'page-num': 19, 'label': {'image_size': [{'width': 3301, 'height': 2550, 'depth': 3}], 'annotations': [{'class_id': 0, 'top': 59, 'left': 107, 'height': 41, 'width': 155}, {'class_id': 16, 'top': 1032, 'left': 281, 'height': 48, 'width': 286}, {'class_id': 16, 'top': 1111, 'left': 281, 'height': 48, 'width': 439}]}, 'label-metadata': {'objects': [{'confidence': 0}, {'confidence': 0}, {'confidence': 0}], 'class-map': {'0': 'Agreement Effective Date', '16': 'Provider Name'}, 'type': 'groundtruth/object-detection', 'human-annotated': 'yes', 'creation-date': '2021-07-05T05:42:47.514427', 'job-name': 'labeling-job/DUMMY-JOB-NAME', 'adjustment-status': 'unadjusted'}}\n"
     ]
    }
   ],
   "source": [
    "print ( test_examples[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Agreement Effective Date:\n",
      "12/3/2020\n",
      "----------\n",
      "Provider Name:\n",
      "Comenity Bank\n",
      "----------\n",
      "Provider Name:\n",
      "Comenity Capital Bank\n"
     ]
    }
   ],
   "source": [
    "other_cls = len(entity_classes)\n",
    "prev_cls = other_cls\n",
    "current_entity = \"\"\n",
    "\n",
    "for page in res.pages:\n",
    "    for line in page.lines:\n",
    "        for word in line.words:\n",
    "            pred_cls = word._block[\"PredictedClass\"]\n",
    "            if pred_cls != prev_cls:\n",
    "                if prev_cls != other_cls:\n",
    "                    print(f\"----------\\n{entity_classes[prev_cls]}:\\n{current_entity}\")\n",
    "                prev_cls = pred_cls\n",
    "                if pred_cls != other_cls:\n",
    "                    current_entity = word.text\n",
    "                else:\n",
    "                    current_entity = \"\"\n",
    "                continue\n",
    "            current_entity = \" \".join((current_entity, word.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course there may be some instances where this heuristic breaks down, but we still have access to all the position (and text) information from each `LINE` and `WORD` to write additional rules for reading order and separation if wanted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating the model with the OCR Pipeline\n",
    "\n",
    "If you've deployed the **OCR pipeline stack** in your AWS Account, you can now configure it to use this endpoint as follows:\n",
    "\n",
    "- First, identify the **endpoint name** of your deployed model. Assuming you created the predictor as above, you can simply run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(predictor.endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next, identify the **AWS Systems Manager Parameter** that configures the SageMaker endpoint for the OCR pipeline stack.\n",
    "\n",
    "The below code should pull it through for you, but alternatively you can refer to your stack's **Outputs** in the [AWS CloudFormation Console](https://console.aws.amazon.com/cloudformation/home?#/stacks). The Output name should include `SageMakerEndpoint`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(config.sagemaker_endpoint_name_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finally, we'll update this SSM parameter to point to the deployed SageMaker endpoint.\n",
    "\n",
    "The below code should do this for you automatically:\n",
    "\n",
    "> ⚠️ **Note:** The [Lambda function](../pipeline/enrichment/fn-call-sagemaker/main.py) that calls your model from the OCR pipeline caches the endpoint name for a few minutes (`CACHE_TTL_SECONDS`) to reduce unnecessary ssm:GetParameter calls - so it may take a little time for an update here to take effect if you already processed a document recently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_endpoint_name = predictor.endpoint_name\n",
    "\n",
    "print(f\"Configuring pipeline with model: {pipeline_endpoint_name}\")\n",
    "\n",
    "ssm.put_parameter(\n",
    "    Name=config.sagemaker_endpoint_name_param,\n",
    "    Overwrite=True,\n",
    "    Value=pipeline_endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you could open the [AWS Systems Manager Parameter Store console](https://console.aws.amazon.com/systems-manager/parameters/?tab=Table) and click on the *name* of the parameter to open its detail page, then the **Edit** button in the top right corner as shown below:\n",
    "\n",
    "![](img/ssm-param-detail-screenshot.png \"Screenshot of SSM parameter detail page showing Edit button\")\n",
    "\n",
    "From this screen you can manually set the **Value** of the parameter and save the changes.\n",
    "\n",
    "Whether you updated the SSM parameters via code or the console, your the pre-processing and enrichment stages of your stack should now be configured to use your endpoints!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the pipeline entity definitions\n",
    "\n",
    "As well as configuring the *enrichment* stage of the pipeline to reference the deployed version of the model, we need to configure the *post-processing* stage to match the model's **definition of entity/field types**.\n",
    "\n",
    "The entity configuration is as we saved in the previous notebook, but the `annotation_guidance` attributes are not needed:\n",
    "\n",
    "> ℹ️ **Note:** As well as the mapping from ID numbers (returned by the model) to human-readable class names, this configuration controls how the pipeline consolidates entity matches into \"fields\" of the document: E.g. choosing the \"most likely\" or \"first\" value between multiple detections, or setting up a multi-value field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_entity_config = json.dumps([f.to_dict(omit=[\"annotation_guidance\"]) for f in fields], indent=2)\n",
    "print(pipeline_entity_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As above, you *could* set this value manually in the SSM console for the parameter named as `EntityConfig`.\n",
    "\n",
    "...But we can make the same update via code through the APIs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Setting pipeline entity configuration\")\n",
    "ssm.put_parameter(\n",
    "    Name=config.entity_config_param,\n",
    "    Overwrite=True,\n",
    "    Value=pipeline_entity_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying out the pipeline\n",
    "\n",
    "To see the pipeline in action:\n",
    "\n",
    "▶️ **Open** the [AWS Step Functions Console](https://console.aws.amazon.com/states/home?#/statemachines) and click on the name of your *State Machine* from the list to see its details.\n",
    "\n",
    "(If you can't find it in the list, the code below should look it up for you or you can check the *Outputs* tab of your pipeline stack in the [AWS CloudFormation Console](https://console.aws.amazon.com/cloudformation/home?#/stacks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Your pipeline state machine is:\")\n",
    "print(config.pipeline_sfn_arn.rpartition(\":\")[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "▶️ **Locate** your pipeline's `InputBucket` in [Amazon S3](https://s3.console.aws.amazon.com/s3/home?)\n",
    "\n",
    "(Likewise you can look this up from CloudFormation or using the below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Your pipeline's input S3 bucket:\")\n",
    "print(config.pipeline_input_bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "▶️ **Upload** a sample document (PDF) from our dataset to the S3 bucket\n",
    "\n",
    "You can do this by dragging and dropping the file to the S3 console - or running the cells below to upload a test document through the AWS CLI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdfpaths = []\n",
    "for currpath, dirs, files in os.walk(\"data/raw\"):\n",
    "    if \"/.\" in currpath or \"__\" in currpath:\n",
    "        continue\n",
    "    pdfpaths += [\n",
    "        os.path.join(currpath, f) for f in files\n",
    "        if f.lower().endswith(\".pdf\")\n",
    "    ]\n",
    "pdfpaths = sorted(pdfpaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_filepath = pdfpaths[0]\n",
    "test_s3uri = f\"s3://{config.pipeline_input_bucket_name}/{test_filepath}\"\n",
    "\n",
    "!aws s3 cp '{test_filepath}' '{test_s3uri}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that a new *execution* (run) of the state machine is triggered automatically:\n",
    "\n",
    "> ℹ️ This may take a few seconds after the upload is complete. If you're not seeing it:\n",
    ">\n",
    "> - Check you're in the correct \"pipeline\" state machine, as this solution's stack creates more than one state machine\n",
    "> - Try refreshing the page or the execution list\n",
    "\n",
    "![](img/sfn-statemachine-screenshot.png \"Screenshot of AWS Step Functions state machine detail page showing execution list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clicking through to the execution, you'll be able to see the progress through the workflow and output/error information.\n",
    "\n",
    "Depending on your configuration, your view may look a little different to the below and you may have **either a successful execution or a failure at the review step**:\n",
    "\n",
    "Don't worry if your human review stage is still failing, as we'll configure that in the next notebook.\n",
    "\n",
    "![](img/sfn-execution-status-screenshot.png \"Screenshot of Step Functions execution detail view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "You should now have been able to train and deploy the enrichment model, and demonstrate its integration to the pipeline.\n",
    "\n",
    "However, the final human review stage is not fully set up yet, so may have triggered an error.\n",
    "\n",
    "In the final notebook, we'll configure the human review functionality to finish up the flow: **Open up notebook [3. Human Review.ipynb](3.%20Human%20Review.ipynb)** to follow along.\n",
    "\n",
    "You may also like to check out the **Auto-scaling** section of **[Optional Extras.ipynb](Optional%20Extras.ipynb)**, to optimise your resource consumption and cost by scaling our model endpoint depending on current load.\n",
    "\n",
    "\n",
    "### A note on clean-up\n",
    "\n",
    "Note that while training, processing and transform jobs in SageMaker start and stop compute resources for the specific job being executed, deployed **endpoints** stay active (and therefore accumulating charges) until you turn them off.\n",
    "\n",
    "When you're finished using an endpoint, you should delete it either through the [Amazon SageMaker Console](https://console.aws.amazon.com/sagemaker/home?#/endpoints) or via commands like the below.\n",
    "\n",
    "(Of course, your OCR pipeline stack will throw an error if you try to run it configured with an Endpoint Name that no longer exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preproc_predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
